<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Interactive Discovery and Inspection of Subpopulations | Intelligent Assistance for Expert-Driven Subpopulation Discovery in High-Dimensional Timestamped Medical Data</title>
<meta name="author" content="Uli Niemann">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.6/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9002/tabs.js"></script><script src="libs/bs3compat-0.2.2.9002/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="font-awesome.min.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Intelligent Assistance for Expert-Driven Subpopulation Discovery in High-Dimensional Timestamped Medical Data</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome!</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="background.html"><span class="header-section-number">2</span> Medical Background and Datasets</a></li>
<li class="book-part">Subpopulation Discovery in High-Dimensional Data</li>
<li><a class="active" href="imm.html"><span class="header-section-number">3</span> Interactive Discovery and Inspection of Subpopulations</a></li>
<li><a class="" href="sdclu.html"><span class="header-section-number">4</span> Identifying Distinct Subpopulations</a></li>
<li><a class="" href="phenotypes.html"><span class="header-section-number">5</span> Visual Identification of Informative Features</a></li>
<li class="book-part">EXPLOITING DYNAMICS</li>
<li><a class="" href="evo.html"><span class="header-section-number">6</span> Constructing Evolution Features to Capture Change over Time</a></li>
<li><a class="" href="diabfoot.html"><span class="header-section-number">7</span> Feature Extraction from Short Temporal Sequences for Clustering</a></li>
<li class="book-part">POST-MINING FOR INTERPRETATION</li>
<li><a class="" href="iml.html"><span class="header-section-number">8</span> Post-Hoc Interpretation of Classification Models</a></li>
<li><a class="" href="gender.html"><span class="header-section-number">9</span> Subpopulation-Specific Learning and Post-Hoc Model Interpretation</a></li>
<li class="book-part">CONCLUSION</li>
<li><a class="" href="conclusion.html"><span class="header-section-number">10</span> Summary and Future Work</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="references.html">References</a></li>
<li><a class="" href="appx-pheno.html"><span class="header-section-number">A</span> Overview of Variables Selected for Phenotyping</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="imm" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Interactive Discovery and Inspection of Subpopulations<a class="anchor" aria-label="anchor" href="#imm"><i class="fas fa-link"></i></a>
</h1>
<div id="brief-chapter-summary" class="section level4 unnumbered infobox chapter-summary">
<h4>Brief Chapter Summary<a class="anchor" aria-label="anchor" href="#brief-chapter-summary"><i class="fas fa-link"></i></a>
</h4>
<p>Analysis of population-based cohort data has been mostly hypothesis-driven.
We present a workflow and an interactive application for data-driven analysis of population-based cohort data using hepatic steatosis as an example.
Our mining workflow includes steps</p>
<ol style="list-style-type: lower-roman">
<li>to discover subpopulations that have different distributions with respect to the target variable,</li>
<li>to classify each subpopulation taking class imbalance into account, and</li>
<li>to identify variables associated with the target variable.</li>
</ol>
<p>We show that our workflow is suited (a) to build subpopulations before classification to reduce class imbalance and (b) to drill-down on the derived models to identify predictive variables and subpopulations worthy of further investigation.</p>
</div>
<div class="lit chapter-literature">
<p>This chapter is partly based on:</p>
<ul>
<li>Uli Niemann, Henry Völzke, Jens-Peter Kühn, and Myra Spiliopoulou. “Learning and inspecting classification rules from longitudinal epidemiological data to identify predictive features on hepatic steatosis.” In: <em>Expert Systems with Applications</em> 41.11 (2014), pp. 5405-5415. DOI: <a href="https://doi.org/10.1016%2Fj.eswa.2014.02.040">10.1016/j.eswa.2014.02.040</a>.</li>
<li>Uli Niemann, Myra Spiliopoulou, Henry Völzke, and Jens-Peter Kühn. “Interactive Medical Miner: Interactively exploring subpopulations in epidemiological datasets.” In: <em>ECML PKDD 2014, Part III, LNCS 8726</em>. Springer, 2014, pp. 460-463.
DOI: <a href="https://doi.org/10.1007%2F978-3-662-44845-8_35">10.1007/978-3-662-44845-8_35</a>.</li>
</ul>
</div>
<!-- **TODO: Kurze Überleitung** -->
<p>This chapter is organized as follows.
In Section <a href="imm.html#imm-intro">3.1</a>, we motivate for classification and interactive subpopulation discovery in epidemiological cohort studies and review related work.
We present our workflow and the interactive assistant in Section <a href="imm.html#imm-workflow">3.2</a>.
In Section <a href="imm.html#imm-experiments">3.3</a>, we report our results and main findings.
The chapter closes with a summary and a discussion of the main contributions in Section <a href="imm.html#imm-conclusion">3.4</a>.</p>
<div id="imm-intro" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Motivation and Comparison to Related Work<a class="anchor" aria-label="anchor" href="#imm-intro"><i class="fas fa-link"></i></a>
</h2>
<p>Medical decisions about the diagnosis and treatment of multifactorial conditions such as diseases and disorders are based on clinical and epidemiological studies <span class="citation"><a href="references.html#ref-elmore2020jekel" role="doc-biblioref">[80]</a></span>.
The latter contain information about participants with and without a disease and allow learning of discriminatory models and, in longitudinal designs, understanding disease progression.
For example, several studies identified risk factors (such as obesity and alcohol consumption) and comorbidities (such as cardiovascular disease) associated with hepatic steatosis <span class="citation"><a href="references.html#ref-IttermannEtAl:Thyroid2012" role="doc-biblioref">[81]</a>–<a href="references.html#ref-Markus:2013" role="doc-biblioref">[85]</a></span>.
However, these studies identified risk factors and associated outcomes that relate to the <em>entire</em> population.
Our work arose from the need to identify such factors for subpopulations to promote personalized diagnosis and treatment, as expected in personalized medicine <span class="citation"><a href="references.html#ref-Hingorani:2013" role="doc-biblioref">[86]</a>, <a href="references.html#ref-Voelzke:Cardiol2013" role="doc-biblioref">[87]</a></span>.</p>
<div id="role-of-subpopulations-in-classifier-learning-for-cohorts" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Role of Subpopulations in Classifier Learning for Cohorts<a class="anchor" aria-label="anchor" href="#role-of-subpopulations-in-classifier-learning-for-cohorts"><i class="fas fa-link"></i></a>
</h3>
<p>Classification on subpopulations was studied by Zhanga and Kodell <span class="citation"><a href="references.html#ref-AIM13" role="doc-biblioref">[88]</a></span>, who pointed out that classifier performance on the whole dataset may be low if the entire population is very heterogeneous.
Therefore, they first trained an ensemble of classifiers and then used each ensemble member’s predictions to create a new feature space.
They performed hierarchical clustering to partition the instances into three subpopulations: one where the prediction accuracy is high, one where it is in the intermediate range, and one where it is low.
Using this approach, Zhanga and Kodell partition the original data set into subpopulations that are easy or hard to classify.
While the method seems appealing in general, it appears inappropriate for the three-class problem of the SHIP data, which has a highly skewed distribution, so it is clear that the low classification accuracy is caused (in part) by the class imbalance.
Therefore, we exploratively examined the data set <em>before</em> classification to identify less skewed subpopulations and <em>after</em> classification to determine – within each subpopulation – variables strongly associated with the target.</p>
<!-- **Classification Rule Mining.**  -->
<p>Pinheiro et al. performed association rule discovery in patients with liver cancer <span class="citation"><a href="references.html#ref-PinheiroEtAl:ICCABS13" role="doc-biblioref">[89]</a></span>.
The authors pointed out that early detection of liver cancer reduces the mortality rate.
Early detection is still difficult because patients often do not show symptoms in the early stages of liver cancer <span class="citation"><a href="references.html#ref-PinheiroEtAl:ICCABS13" role="doc-biblioref">[89]</a></span>.
Pinheiro et al. used the association rule algorithm FP-growth <span class="citation"><a href="references.html#ref-Han:FPGrowth00" role="doc-biblioref">[90]</a></span> to discover high-confidence association rules and high-confidence classification rules related to liver cancer mortality.
We also considered association rules promising for medical data analysis because they are easy to compute and produce results that are understandable to humans.
Therefore, we used association rules as the primary method, but for epidemiological data and classification rather than for mortality prediction.
To use association rules for classification, we specified that the rule’s consequence is the target variable.</p>
<!-- Big survey: Interactive Information Visualization to Explore and Query Electronic Health Records [@rind2013interactive] -->
<!-- Cohort analysis aims to uncover meaningful associations between risk factors (or preventive factors) and an outcome.  -->
<!-- Nowadays, large medical data with comprehensive historical data about patients, possibly over time spans of many years, have become available.  -->
<!-- Domain experts lack the technical expertise to perform tasks such as data management, analysis and summarization on very large datasets.  -->
<!-- For example, whereas in the past it was sufficient to have working knowledge of basic statistics and a spreadsheet software like Microsoft Excel, data storage and retrieving has become technically challenging, as data from heterogeneous sources (tables, images, text and speech recordings) have to be queried.  -->
<!-- Thus, domain experts usually rely on technical experts to help them perform these tasks.  -->
<!-- However, this process is often slow, tedious and expensive.  -->
<!-- It would be better to equip the domain expert with a technical tool that allows them to quickly perform exploratory analyses on their own.  -->
</div>
<div id="workflows-for-expert-machine-interaction-for-cohort-construction-and-analysis" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> Workflows for Expert-Machine Interaction for Cohort Construction and Analysis<a class="anchor" aria-label="anchor" href="#workflows-for-expert-machine-interaction-for-cohort-construction-and-analysis"><i class="fas fa-link"></i></a>
</h3>
<!-- CAVA -->
<p>Zhang et al. <span class="citation"><a href="references.html#ref-Zhang:CAVA2015" role="doc-biblioref">[91]</a></span> addressed the increasing technical challenges of medical expert-driven subpopulation discovery due to increasingly large and complex medical data, often including information from hundreds of variables for thousands of patients in the form of tables, images, or text.
In the past, it was sufficient for a physician to have some basic knowledge of statistics and spreadsheet software such as Microsoft Excel to analyze a small table of patient data.
Today, more effective and efficient approaches for managing, analyzing, and summarizing extensive medical data are available <span class="citation"><a href="references.html#ref-Zhang:CAVA2015" role="doc-biblioref">[91]</a></span>.
However, domain experts typically rely on technical experts to help them perform these tasks.
This back-and-forth is often slow, tedious, and expensive.
Therefore, it would be better to provide the domain expert with a technical tool that allows them to perform exploratory analysis by themselves quickly.
Zhang et al. <span class="citation"><a href="references.html#ref-Zhang:CAVA2015" role="doc-biblioref">[91]</a></span> presented CAVA, a system that includes various subgroup visualizations (called “views”) and analytical components (called “analytics”) for subgroup comparison.
The main panel in Figure <a href="imm.html#fig:03-cava">3.1</a> shows one of the views: a flowchart <span class="citation"><a href="references.html#ref-wongsuphasawat2012exploring" role="doc-biblioref">[92]</a></span> of patient subgroups with the same sequence of symptoms.
The user can obtain additional summaries by interacting with the visualization, for example, by dragging and dropping one of the boxes in the flowchart onto one of the entries in the analysis panel.
The user can also expand the selected cohort by having the tool search for patients who do not strictly meet the current inclusion criteria but are somewhat <em>similar</em> to the selected patient subpopulation of interest <span class="citation"><a href="references.html#ref-ebadollahi2010predicting" role="doc-biblioref">[93]</a></span>.</p>

<div class="figure" style="text-align: center">
<span id="fig:03-cava"></span>
<img src="figures/03-cava.png" alt="CAVA’s graphical user interface. The flowchart visualizes subgroups of cardiac patients organized by the common occurrence of symptoms. Arc color represents the hospitalization risk. The user can switch between graphical representations and data processing methods by dragging and dropping. The upper right panel contains detailed information about the currently selected patients. The lower right panel contains a provenance graph that allows the user to undo operations and revisit previous interaction steps. The figure is taken from [91]." width="100%"><p class="caption">
Figure 3.1: <strong>CAVA’s graphical user interface.</strong> The flowchart visualizes subgroups of cardiac patients organized by the common occurrence of symptoms. Arc color represents the hospitalization risk. The user can switch between graphical representations and data processing methods by dragging and dropping. The upper right panel contains detailed information about the currently selected patients. The lower right panel contains a provenance graph that allows the user to undo operations and revisit previous interaction steps. The figure is taken from <span class="citation"><a href="references.html#ref-Zhang:CAVA2015" role="doc-biblioref">[91]</a></span>.
</p>
</div>
<!-- PROSPECTOR -->
<p>Krause et al. <span class="citation"><a href="references.html#ref-Krause:Prospector2016" role="doc-biblioref">[94]</a></span> argued that model selection should not be based only on global performance metrics such as accuracy, as these statistics do not contribute to a better understanding of the model’s reasoning.
Moreover, a complex but highly accurate model does not automatically guarantee actionable insights.
Krause et al. propose Prospector <span class="citation"><a href="references.html#ref-Krause:Prospector2016" role="doc-biblioref">[94]</a></span>, a system that provides diagnostic components for complex classification models based on the concepts of partial dependence (PD) plots <span class="citation"><a href="references.html#ref-Friedman:PDP2001" role="doc-biblioref">[24]</a></span>.
PD plots are a popular tool for visualizing <em>marginal effects</em> of features on the predicted probability of the target.
Briefly, each point on a PD plot represents the model’s average prediction over all observations, assuming that these observations had a fixed value for a feature of interest.
A feature whose PD curve exhibits a high range or high variability is considered more influential on model prediction than a feature with a flat PD curve.
Closely related to PD plots are individual conditional expectation (ICE) plots, <span class="citation"><a href="references.html#ref-Goldstein:ICE2015" role="doc-biblioref">[95]</a></span> which display a curve for each observation, helping to reveal contrasting subpopulations that might “average out” in a PD plot.
Prospector combines PD and ICE curves to show the relationship between a feature and model prediction at a (<em>global</em>) model level and a (<em>local</em>) patient-individual level.
Besides, a custom color bar is provided as a more compact alternative to ICE curves (Figure <a href="imm.html#fig:03-prospector">3.2</a> (a)).
A stacked bar graph shows the distribution of predicted risk scores for each study group (Figure <a href="imm.html#fig:03-prospector">3.2</a> (b)).
The user can click on a specific decile to obtain a list of individual patients with their exact predicted score and label.
In this way, patients whose prediction scores are close to the decision threshold can be further investigated.
For each feature, the authors calculate the “most impactful feature change”: given a patient’s current feature values, they identify a near-counterfactual value that leads to a large change in the predicted risk score by minimizing the difference from the original feature value and maximizing the predicted risk score.
The top 5 of these so-called “suggested changes” are displayed – separately for increasing and decreasing disease risk - in a table (cf. Figure <a href="imm.html#fig:03-prospector">3.2</a> (c)) and integrated as interactive elements into the IC color bars (cf. Figure <a href="imm.html#fig:03-prospector">3.2</a> (d)).
<!--2016_Perer_Prospector_CHI2016--></p>

<div class="figure" style="text-align: center">
<span id="fig:03-prospector"></span>
<img src="figures/03-prospector.png" alt="Selected model diagnostics of Prospector. (a) The upper plot shows two curves for the characteristic “age”: the gray partial dependence (PD) curve represents the marginal prediction of the model over all patients, while the black individual conditional expectation (ICE) curve illustrates the effect of counterfactual ages on the predicted risk of diabetes for an example patient. The histogram shows the age distribution. The color bar below is a compact representation of the ICE curve above; the circled value represents the selected patient’s feature value. (b) Stacked bars show the distribution of predicted risk scores for each study group. Clicking on one of the bars opens a table showing the ID, predicted risk, and true label for all patients belonging to the selected decile of predicted risk. (c) Summary table of “most impactful feature changes” for a decreasing (upper group) and an increasing (lower group) predicted risk: each row shows the actual feature value and the “suggested change,” i.e., a similar but counterfactual value that would lead to a significant change in predicted risk. (d) Multiple PD color bars augmented with suggested changes (labels outlined in white). The figure is adapted from [94]." width="100%"><p class="caption">
Figure 3.2: <strong>Selected model diagnostics of Prospector.</strong> (a) The upper plot shows two curves for the characteristic “age”: the gray partial dependence (PD) curve represents the marginal prediction of the model over all patients, while the black individual conditional expectation (ICE) curve illustrates the effect of counterfactual ages on the predicted risk of diabetes for an example patient. The histogram shows the age distribution. The color bar below is a compact representation of the ICE curve above; the circled value represents the selected patient’s feature value. (b) Stacked bars show the distribution of predicted risk scores for each study group. Clicking on one of the bars opens a table showing the ID, predicted risk, and true label for all patients belonging to the selected decile of predicted risk. (c) Summary table of “most impactful feature changes” for a decreasing (upper group) and an increasing (lower group) predicted risk: each row shows the actual feature value and the “suggested change,” i.e., a similar but counterfactual value that would lead to a significant change in predicted risk. (d) Multiple PD color bars augmented with suggested changes (labels outlined in white). The figure is adapted from <span class="citation"><a href="references.html#ref-Krause:Prospector2016" role="doc-biblioref">[94]</a></span>.
</p>
</div>
<p>Pahins et al. <span class="citation"><a href="references.html#ref-Pahins:COVIZ2019" role="doc-biblioref">[96]</a></span> presented COVIZ, a system for cohort construction in large spatiotemporal datasets.
COVIZ includes components for exploratory data analysis of treatment pathways and event trajectories, visual cohort comparison, and visual querying.
One of the design goals of COVIZ was to be fast, e.g., by using efficient data structures such as Quantile Data Structure <span class="citation"><a href="references.html#ref-de2019real" role="doc-biblioref">[97]</a></span> to ensure low latency for all computational operations and thus suitability for large data sets.
<!-- A visual-Interactive System for Prostate Cancer Cohort Analysis -->
Bernard et al. <span class="citation"><a href="references.html#ref-bernard2015visual" role="doc-biblioref">[98]</a></span> proposed a system for cohort construction in temporal prostate cancer cohort data that included visualizations for subpopulations and individual patients.
To guide users during exploration, visual markers indicate interesting relationships between attributes derived from statistical tests.
Recently, Corvo et al. <span class="citation"><a href="references.html#ref-Corvo2020" role="doc-biblioref">[29]</a></span> presented a comprehensive visual analytics system for pathological high-throughput data, which encompasses all major steps of a typical data analysis pipeline, such as preprocessing raw histopathology images by interactive segmentation, components for exploratory data analysis, and interactive cohort construction in a high-dimensional feature space, feature engineering which includes extraction of potentially predictive biomarker features, modeling, as well as visualization and summarization of the modeling results.
Preim and Lawonn provided comprehensive reviews of visual analytics methods and applications in public health <span class="citation"><a href="references.html#ref-preim2020survey" role="doc-biblioref">[27]</a></span> and epidemiology <span class="citation"><a href="references.html#ref-Preim16" role="doc-biblioref">[99]</a></span> in particular.</p>
</div>
<div id="previous-work-on-subpopulation-discovery-with-the-ship-data" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> Previous Work on Subpopulation Discovery with the SHIP Data<a class="anchor" aria-label="anchor" href="#previous-work-on-subpopulation-discovery-with-the-ship-data"><i class="fas fa-link"></i></a>
</h3>
<!-- **Previous Work on SHIP.** -->
<p>Since we carry out the proof-of-concept for our workflow on the SHIP data, we list major scientific preparatory work hereafter.
Preim et al. <span class="citation"><a href="references.html#ref-Preim:EurographicsMedPrice2019" role="doc-biblioref">[100]</a></span> provided an overview of the research that developed data mining and visual analytics methods to gain insights into the SHIP data.
Among them is the “3D Regression Cube” of Klemm et al. <span class="citation"><a href="references.html#ref-Klemm:RegressionHeatmap2015" role="doc-biblioref">[101]</a></span>, a system that allows interactive exploration of feature correlations in epidemiological datasets.
The system generates many multiple linear regression models from different combinations of one dependent and up to three independent variables and displays their goodness of fit in a three-dimensional heat map.
The system allows the user to modify the regression equation, for example, by changing the number of independent variables, specifying wild cards and interaction terms, fixing one of the variables to reduce computational complexity, or focusing specifically on a variable of interest.
Our approach is also able to identify variables that are strongly associated with the target variable.
However, we search for subpopulation-specific relationships rather than generating a global model for the entire dataset, and we additionally provide predictive value ranges.
Klemm et al. <span class="citation"><a href="references.html#ref-Klemm14" role="doc-biblioref">[16]</a></span> presented a system that combines visual representations of non-image and image data.
They identify clusters of back pain patients for the SHIP data.
Since we specify hepatic steatosis as the target variable, we instead build supervised models and classification rules that directly capture the relationships between predictors and the target variable.
<!-- S-ADVIsED -->
Alemzadeh et al. <span class="citation"><a href="references.html#ref-eurova.20171118" role="doc-biblioref">[102]</a></span> presented S-ADVIsED, a system for interactive exploration of subspace clusters that incorporates various visualization types such as donut diagrams, correlation heatmaps, scatterplot matrices, mosaic diagrams, and error bar graphs.
While S-ADVIsED requires the user to input mining results obtained in advance outside the system, our tool enables expert-driven interactive <em>subpopulation discovery</em> instead of expert-driven interactive <em>result exploration</em>.
<!-- our tool is more interactive -->
Hielscher et al. <span class="citation"><a href="references.html#ref-Hielscher16" role="doc-biblioref">[103]</a></span> developed a semi-supervised constrained-based subspace clustering algorithm to find diverse sets of <em>interesting</em> feature subsets using the SHIP data.
To guide the search for predictive feature subsets, the expert can provide their domain knowledge in the form of a small number of instance-level constraints, forcing pairs of instances (i.e., study participants) to be assigned either to the same or a different cluster.
Hielscher et al. <span class="citation"><a href="references.html#ref-Hielscher2018" role="doc-biblioref">[104]</a></span> extended their work and introduced a mechanism to validate subpopulations on independent cohorts.</p>
</div>
</div>
<div id="imm-workflow" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Subpopulation Discovery Workflow and Interactive Mining Assistant<a class="anchor" aria-label="anchor" href="#imm-workflow"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we present our subpopulation discovery workflow.
We build classification models on the whole dataset and different partitions, as described in Section <a href="imm.html#imm-workflow-classification">3.2.1</a>.
In Section <a href="imm.html#imm-workflow-rule-discovery">3.2.2</a>, we introduce relevant underpinnings of classification rule discovery, followed by a description of the primarily used HotSpot <span class="citation"><a href="references.html#ref-hotspot2012" role="doc-biblioref">[105]</a></span> algorithm in Section <a href="imm.html#imm-workflow-hotspot">3.2.3</a>.
We present our interactive mining assistant in Section <a href="imm.html#imm-workflow-imm">3.2.4</a>.
The dataset used for population partitioning and class separation on the target variable hepatic steatosis comes from the “Study of Health in Pomerania” (SHIP), recall Section <a href="background.html#background-data-ship">2.2.1</a>.
In Section <a href="imm.html#imm-workflow-target">3.2.5</a>, we describe the origin and availability of the target variable.
In Section <a href="imm.html#imm-workflow-partitioning">3.2.6</a>, the motivation for data partitioning and the partitioning steps are presented.</p>
<div id="imm-workflow-classification" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Classification<a class="anchor" aria-label="anchor" href="#imm-workflow-classification"><i class="fas fa-link"></i></a>
</h3>
<p>For the classification of cohort participants, we focus on algorithms that provide interpretable models, as we aim to identify predictive <em>conditions</em>, i.e., variables and values/ranges in the models.
Therefore, we consider decision trees, classification rules, and regression trees.
We use the J4.8 decision tree classification algorithm (equivalent to the C4.5 algorithm <span class="citation"><a href="references.html#ref-Q92" role="doc-biblioref">[106]</a></span>) from the Waikato Environment for Knowledge Analysis (Weka) Workbench <span class="citation"><a href="references.html#ref-FrankEtAl:Weka2016" role="doc-biblioref">[107]</a></span>.
This algorithm builds a tree successively by partitioning each node (a subset of the dataset) to the variable that maximizes the information gain within that node.
The original algorithm works only with variables that take categorical values, creating one child node per value.
However, the Weka implementation also provides an option that forces the algorithm always to create exactly two child nodes: one for the best separating value and one for all other values.
We use this option in our experiments because it yields better quality trees.
The Weka algorithm also supports variables that take numeric values:
A node is split into two child nodes by partitioning the variable’s range of values into two intervals.</p>
<p>To deal with the skewed distribution, we consider the following classification variants:</p>
<ul>
<li>
<em>Naive</em>: the problem of imbalanced data is ignored.</li>
<li>
<em>InfoGain</em>: we keep only the top 30 of the 66 variables by sorting the variables on information gain towards the target variable.</li>
<li>
<em>Oversampling</em>: We use SMOTE <span class="citation"><a href="references.html#ref-CBHea02" role="doc-biblioref">[108]</a></span> to resample the dataset with minority-oversampling: for class B, 100% new instances are generated; for class C, 300% new instances are generated, resulting in the following distribution A: 438, B: 216, C: 128.</li>
<li>
<em>CostMatrix</em>: We prefer to misclassify a negative case rather than not detecting a positive case, so we penalize false negatives (FN) more than false positives (FP).
We use the cost matrix depicted in Table <a href="imm.html#tab:03-costmatrix">3.1</a>.</li>
</ul>
<div class="inline-table"><table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'>
<caption>
<span id="tab:03-costmatrix">Table 3.1: </span><strong>Cost matrix.</strong> Cost matrix to penalize misclassification under class imbalance.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="2">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Predicted
</div>
</th>
</tr>
<tr>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
</th>
<th style="text-align:center;">
A
</th>
<th style="text-align:center;">
B
</th>
<th style="text-align:center;">
C
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;font-weight: bold;vertical-align: middle !important;" rowspan="3">
True
</td>
<td style="text-align:center;">
A
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
2
</td>
</tr>
<tr>
<td style="text-align:center;">
B
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
</tr>
<tr>
<td style="text-align:center;">
C
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="imm-workflow-rule-discovery" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Classification Rule Discovery<a class="anchor" aria-label="anchor" href="#imm-workflow-rule-discovery"><i class="fas fa-link"></i></a>
</h3>
<p>Classification rules can reveal interesting relationships between one or more features and the target variable <span class="citation"><a href="references.html#ref-Fuernkranz:12" role="doc-biblioref">[109]</a>, <a href="references.html#ref-Herrera11" role="doc-biblioref">[110]</a></span>.
Compared to model families such as deep neural networks, support vector machines and random forests, classification rules usually achieve lower accuracy.
However, they are easier to interpret and infer and are therefore more suitable for interactive subpopulation discovery.
In epidemiological research, interesting subpopulations could subsequently be used to formulate and validate a small set of hypotheses or investigate associations between risk factors for a particular target variable.
A subpopulation of interest could be formulated as follows: “In the sample of this study, the prevalence of goiter is 32%, whereas the probability in the subpopulation described by <em>thyroid-stimulating hormone</em> less than or equal to 1.63 mU/l and <em>body mass index</em> greater than 32.5 kg/m<sup>2</sup> is 49%.”</p>
<p>Classification rule algorithms induce descriptions of <em>interesting</em> subpopulations where interestingness is quantified by a quality function.
A classification rule is an association rule whose consequent is fixed to a specific class value.
Consider the exemplary classification rule <span class="math inline">\(r_1\)</span>:
<span class="math display" id="eq:03-rule">\[\begin{equation}
r_1: \underbrace{som\_waist\_s2 &lt; 80 \wedge age\_ship\_s2 &gt; 59 \left(\wedge \ldots \right)}_{\text{Antecedent}} \longrightarrow \underbrace{\vphantom{som\_waist\_s2 &lt; 80 \wedge age\_ship\_s2 &gt; 59 \left(\wedge \ldots \right)}hepatic\_steatosis = pos}_{\text{Consequent}}
\tag{3.1}
\end{equation}\]</span></p>
<!-- #### Underpinnings {-} -->
<p>Classification rules are expressed in the form of <span class="math inline">\(r: \text{antecedent} \longrightarrow T=v\)</span>.
The conjunction of <em>conditions</em> (i.e., feature - feature value pairs) left to the arrow constitutes the rule’s <span class="math inline">\(\text{antecedent}\)</span> (or left-hand side).
In the <span class="math inline">\(\text{consequent}\)</span> (or right-hand side), <span class="math inline">\(v\)</span> is the requested value for the target variable <span class="math inline">\(T\)</span>.</p>
<p>We define <span class="math inline">\(s(r)\)</span> as the subpopulation or <em>cover set</em> of <span class="math inline">\(r\)</span>, i.e., the set of instances that satisfy the antecedent of <span class="math inline">\(r\)</span>.
The <em>coverage</em> of <span class="math inline">\(r\)</span>, which is the fraction of instances covered by <span class="math inline">\(r\)</span>, is then defined as <span class="math inline">\(Cov(r)=|s(r)|/N\)</span>, where <span class="math inline">\(N\)</span> is the total number of instances.
The <em>support</em> of <span class="math inline">\(r\)</span> quantifies the percentage of instances covered by <span class="math inline">\(r\)</span> that additionally have <span class="math inline">\(T=v\)</span>, calculated as <span class="math inline">\(Sup(r)=|s(r)_{T=v}|/N\)</span>.
The <em>confidence</em> of <span class="math inline">\(r\)</span> (also referred to as precision or accuracy) is defined as <span class="math inline">\(Conf(r)= |s(r)_{T=v}|/|s(r)|\)</span> and expresses the relative frequency of instances satisfying the complete rule (i.e., both the antecedent and the consequent) among those satisfying only the antecedent.
The <em>recall</em> or <em>sensitivity</em> of <span class="math inline">\(r\)</span> with respect to <span class="math inline">\(T=v\)</span> is defined as <span class="math inline">\(Recall(r)=Sensitivity(r)=\frac{|s(r)_{T=v}|}{n_{T=v}}\)</span>.
The <em>Weighted Relative Accuracy</em> of a rule is an interestingness measure that balances coverage and confidence gain and is often used as an internal criterion for candidate generation <span class="citation"><a href="references.html#ref-Herrera11" role="doc-biblioref">[110]</a></span>.
It is defined as</p>
<p><span class="math display" id="eq:03-wra">\[\begin{equation}
WRA(r) = Cov(r)\cdot \left(Conf(r)-\frac{n_{T=v}}{N} \right).
\tag{3.2}
\end{equation}\]</span></p>
<p>The <em>odds ratio</em> of <span class="math inline">\(r\)</span> with respect to <span class="math inline">\(T=v\)</span> is defined as
<span class="math display" id="eq:03-odds">\[\begin{equation}
OR(r) = \frac{ |s(r)_{T=v}| }{|s(r)_{T\neq v}|} / \frac{n_{T=v} -  |s(r)_{T=v}| }{ n_{T\neq v} -  |s(r)_{T\neq v}|}.
\tag{3.3}
\end{equation}\]</span></p>
<!-- odds.P <- (cov.P / (cov - cov.P)) / (data$Target.P.n / data$Target.N.n) -->
<p>As an example, Figure <a href="imm.html#fig:03-rule-intro">3.3</a> illustrates an exemplary rule <span class="math inline">\(r_2\)</span> in a dataset with 10 instances and a binary target, where circles in cyan color represent instances from the negative class and red circles are positive instances.
The cover set of <span class="math inline">\(r_2\)</span> contains instances 7, 8, 9 and 10, hence <span class="math inline">\(Cov(r_2)\)</span> = 0.40.
Further, <span class="math inline">\(Sup(r_2)\)</span> = 0.30, <span class="math inline">\(Conf(r_2)\)</span> = 0.75, <span class="math inline">\(WRA(r_2)\)</span> = 0.40 <span class="math inline">\(\cdot\)</span> (0.75 - 0.40) = 0.14 and <span class="math inline">\(OR(r_2)\)</span> = (3/1) / (1/5) = 15.</p>

<div class="figure" style="text-align: center">
<span id="fig:03-rule-intro"></span>
<img src="figures/03-rule-intro.png" alt="Exemplary classification rule. The gray area represents the data space of the covered instances." width="50%"><p class="caption">
Figure 3.3: <strong>Exemplary classification rule.</strong> The gray area represents the data space of the covered instances.
</p>
</div>
</div>
<div id="imm-workflow-hotspot" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> HotSpot<a class="anchor" aria-label="anchor" href="#imm-workflow-hotspot"><i class="fas fa-link"></i></a>
</h3>
<p>For classification rule discovery, we use the HotSpot <span class="citation"><a href="references.html#ref-hotspot2012" role="doc-biblioref">[105]</a></span> algorithm provided for Weka <span class="citation"><a href="references.html#ref-FrankEtAl:Weka2016" role="doc-biblioref">[107]</a></span>.
HotSpot is a beamwidth search algorithm that implements a general-to-specific approach to rule extraction.
A single rule is constructed by successively adding the condition to the antecedent that locally maximizes confidence.
Unlike general hill-climbing, which considers only the best rule candidate at each iteration, HotSpot’s beam search retains the b highest-ranked candidates and refines them in later steps.
Consequently, HotSpot reduces the <em>“myopia”</em> <span class="citation"><a href="references.html#ref-Fuernkranz:12" role="doc-biblioref">[109]</a></span> from which Hill-Climbing search typically suffers.
Briefly, hill-climbing approaches consider only the locally optimal candidate at each iteration.
As a result, a globally optimal rule will not be found if it is not locally optimal in each iteration.
It is also desirable to generate more than one rule from an application perspective since alternative descriptions of subpopulations can facilitate hypothesis generation.
The beamwidth can be specified as a <code>maximum branching factor</code>, i.e., the maximum number of conditions that can be added to a candidate rule.
In each iteration, the rule candidates must satisfy the <code>minimum value count</code>, the sensitivity threshold.
To avoid adding a condition only leads to a marginal improvement of the confidence, the parameter <code>minimum improvement</code>, i.e., the minimum relative improvement of the confidence by adding another condition, can be specified.
The rule search’s computational complexity can be reduced by specifying a <code>maximum rule length</code>, i.e., the number of conditions in the antecedent.
In our experiments we set the parameters as follows: <code>maximum branching factor</code> = 20, <code>maximum value count</code> = 1/3, <code>minimum improvement</code> = 0.1, <code>maximum rule length</code> = 3.</p>
</div>
<div id="imm-workflow-imm" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Interactive Medical Miner<a class="anchor" aria-label="anchor" href="#imm-workflow-imm"><i class="fas fa-link"></i></a>
</h3>
<p>Classification rules can provide valuable insights into potentially prevalent conditions for different subpopulations of the cohort under study.
However, when the number of rules created is large, as is usually the case with large epidemiological data, the rules’ conditions overlap.
Hence, some conditions are present under each of the classes of the target variable.
Therefore, the medical expert needs inspection tools to decide which rules are informative and which features should be investigated further.
Our Interactive Medical Miner (IMM) allows the expert to</p>
<ul>
<li>discover classification rules,</li>
<li>inspect the frequency of these rules (a) against each class and (b) against the unlabeled subset of the cohort, and</li>
<li>examine the statistics of each rule for the values of selected variables.</li>
</ul>
<p>We describe these functionalities below, referring to the screenshot in Figure <a href="imm.html#fig:03-imm-modified">3.4</a>.</p>

<div class="figure" style="text-align: center">
<span id="fig:03-imm-modified"></span>
<img src="figures/03-imm-modified.png" alt="The user interface of the Interactive Medical Miner. Classification rules are discovered for class B and shown in the bottom left panel. For the selected rule som_huef_s2 &gt; 109 &amp; crea_u_s2 &gt; 5.38 \(\longrightarrow\) mrt_liverfat_s2 = B, the distribution of the participants covered by the rule among all three classes is shown in absolute values (top middle panel) and as a histogram (bottom right panel) with respect to age (top right panel)." width="100%"><p class="caption">
Figure 3.4: <strong>The user interface of the Interactive Medical Miner.</strong> Classification rules are discovered for class B and shown in the bottom left panel. For the selected rule som_huef_s2 &gt; 109 &amp; crea_u_s2 &gt; 5.38 <span class="math inline">\(\longrightarrow\)</span> mrt_liverfat_s2 = B, the distribution of the participants covered by the rule among all three classes is shown in absolute values (top middle panel) and as a histogram (bottom right panel) with respect to age (top right panel).
</p>
</div>
<p>The user interface consists of six panels.
In the “Settings” panel (top left), the medical expert can set the parameters for rule induction before pressing the “Build Rules” button.
Below this panel, the discovered rules are displayed.
In the “Sorting preference” panel, the expert can specify whether the rules should be sorted by confidence, by coverage, or rather alphabetically for a better overview of overlapping rules.</p>
<p>Before rule generation, the user can specify a sub-cohort of the dataset.
By clicking on the button <code>Select Subpopulation</code>, a popup window appears, where multiple filter queries in the form of <code>&lt;variable&gt; &lt;operator&gt; &lt;value&gt;</code> can be added, e.g., <code>som_bmi_s2 &gt;= 30</code>.
The defined constraints are displayed in a table and can be undone.
Furthermore, the user can select variables for model creation, e.g., exclude a variable that is already known to be highly correlated with another variable that is already considered for model learning.</p>
<p>Mining criteria include the dataset (choose between the whole dataset and one of the partitions), the class for which rules are to be generated (drop-down list “Class”), and the constraints related to this class, i.e., “Minimum number of values” (which can also be specified as a relative value), “Maximum rule length,” “Maximum branching factor” and “Minimum improvement.”
As an example of how these parameters affect rule search, consider the selected rule in Figure <a href="imm.html#fig:03-imm-modified">3.4</a>, som_huef_s2 &gt; 109 &amp; crea_u_s2 &gt; 5.38 <span class="math inline">\(\longrightarrow\)</span> mrt_liverfat_s2 = B, which has a coverage of 0.12 and a confidence of 0.56.
The sensitivity of 38/108 = 0.352 satisfies the minimum value count threshold of 0.33.
From the Apriori property, it is evident that each of the two conditions in the antecedent of the rule, namely som_huef_s2 &gt; 109 and crea_u_s2 &gt; 5.38, must also exceed this threshold.
The position of a condition within the antecedent indicates at which refinement step the condition was added to the rule candidate.
For example, the first condition som_huef_s2 &gt; 109 with a confidence of 44/107 = 0.41 was extended by the second condition crea_u_s2 &gt; 5.38 because the confidence gain exceeds the minimum improvement threshold, i.e., 38/68 - 44/107 = 0.15 &gt; 0.05.
However, this rule cannot be extended further because the maximum rule length is set to 2.
The maximum branching factor was conservatively set to 1000 to prevent potentially interesting rules from not being generated due to a small beamwidth.
The expert can lower this parameter interactively if the number of rules found is too high or rule induction takes too long.</p>
<p>The output list of an execution run (area below the “Settings”) is scrollable and interactive.
When the expert clicks on a rule, the upper-middle area “Summary Statistics” is updated.
The first row shows the distribution of cohort participants across classes for the entire dataset.
The second row shows how the participants covered by the rule (column “Total” in the second row) are distributed across classes.
Thus, the expert can specify the discovery of classification rules for one of the classes and then examine how often each rule’s antecedent occurs among participants in the other classes.
For example, a rule that covers most of the participants in the selected class (class B in Figure <a href="imm.html#fig:03-imm-modified">3.4</a>) is not necessarily interesting if it also covers a high number of participants in the other classes.
The rule som_huef_s2 &gt; 109 &amp; crea_u_s2 &gt; 5.38 <span class="math inline">\(\longrightarrow\)</span> mrt_liverfat_s2 = B covers a total of 68 participants, of which 38 are of class B.
To reduce the number of covered participants from other classes, i.e., to increase the confidence, the user can decrease the minimum value count threshold to allow generating rules with a lower sensitivity but higher homogeneity with respect to the selected class.</p>
<p>Some of the data may be incomplete.
For example, not all participants in the cohort underwent liver MRI.
Therefore, it is also of interest to know the distribution of unlabeled participants who support a given rule’s antecedent.
For this purpose, the “Histogram” panel can be used: The expert selects another feature from the interactive “Variable selection” area in the upper right panel and can then see how the values of this variable are distributed among the study participants – both labeled and unlabeled; the latter are marked as “Missing” in the color legend.
For plotting the histograms, we use the free Java chart library JFreeChart <span class="citation"><a href="references.html#ref-GilbertJFree" role="doc-biblioref">[111]</a></span>.
Numerical variables are discretized using “Scott’s rule” <span class="citation"><a href="references.html#ref-scott1979optimal" role="doc-biblioref">[112]</a></span> as follows:
let <span class="math inline">\(X_{s(r)}\)</span> be the set of values for a numeric variable <span class="math inline">\(X\)</span> with respect to the cover set <span class="math inline">\(s(r)\)</span>.
The bin width <span class="math inline">\(h\)</span> is then calculated as <span class="math inline">\(h(X_{s(r)})=\frac{\max{X_{s(r)}}-\min{X_{s(r)}}}{3.49\sigma_{s(r)}}\cdot |s(r)|^{\frac{1}{3}}\)</span>.</p>
<p>If the expert does not select a variable, the target variable is used by default, and only the distribution of labeled participants is visible.
The histogram in Figure <a href="imm.html#fig:03-imm-modified">3.4</a> shows the age distribution of both labeled and unlabeled participants covered by our example rule som_huef_s2 &gt; 109 &amp; crea_u_s2 &gt; 5.38 <span class="math inline">\(\longrightarrow\)</span> mrt_liverfat_s2 = B.
The distribution of values among the labeled participants indicates that age may be a risk factor for the indicated subpopulation, as the probability of class B increases with age.
This visual finding suggests adding the condition age_ship_s2 &gt; 56.8 to the antecedent of the rule.
Indeed, the confidence of this more specific rule increases from 38/68 = 0.56 to 27/40 = 0.675.
However, as the sensitivity decreases from 38/108 = 0.352 to 27/108 = 0.250, the minimum value count threshold is no longer met.
Thus, visualizing participant statistics for selected rules can provide clues to subpopulations that should be monitored more closely and clues to how to modify algorithm parameters for subsequent runs, in our example, to decrease the minimum value count to 0.25 and increase the maximum rule length to 3.</p>
</div>
<div id="imm-workflow-target" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> The Target Variable<a class="anchor" aria-label="anchor" href="#imm-workflow-target"><i class="fas fa-link"></i></a>
</h3>
<p>The target variable is derived from participants’ liver fat concentration calculated by magnetic resonance imaging (MRI).
At the time of writing the original manuscript, MRI results were only available for 578 (from a total of 2333; ca. 24.7%) SHIP-2 participants.
We use the data from these participants for classifier learning, while our Interactive Medical Miner also contrasts these data with data from the remaining 1755 participants for whom MRI scans were not made available.</p>
<p>After discussions with domain experts, we decided to assign participants with a liver fat concentration of 10% or less to class A (“negative” class, i.e., absence of the disorder); values greater than 10% and less than 25% represent class B (increased liver fat/fatty liver tendency) and values greater than 25% class C (high liver fat).
We consider classes B and C as “positive.”
The cutoff value of 10% is intentionally higher than the value of 5% proposed by Kühn et al. <span class="citation"><a href="references.html#ref-KuehnEtAl:2011" role="doc-biblioref">[113]</a></span> to separate subjects with and without hepatic steatosis because the primary interest from a medical perspective was to identify predictive variables for subjects likely to be ill.
Selecting a high cutoff value exacerbates class imbalance and makes data analysis more difficult.
Figure <a href="imm.html#fig:03-fatty-liver-mosaic">3.5</a> depicts the class distribution stratified by sex.
Of the 578 participants, 438 belong to class A (approximately 76%), 108 to B (19%), and 32 to C (6%).
Men were more likely to have elevated or high liver fat concentration than women (30.7% vs. 18.8% in classes B or C).</p>

<div class="figure" style="text-align: center">
<span id="fig:03-fatty-liver-mosaic"></span>
<img src="figures/03-fatty-liver-mosaic.png" alt="Sex-specific distribution of the target variable. The boxes’ relative sizes depict the number of female and male participants for each of the classes." width="50%"><p class="caption">
Figure 3.5: <strong>Sex-specific distribution of the target variable.</strong> The boxes’ relative sizes depict the number of female and male participants for each of the classes.
</p>
</div>
<p>In addition to the target variable, the data set contains 66 variables extracted from participants’ questionnaire responses and medical tests (cf. <span class="citation"><a href="references.html#ref-Voelzke:SHIP11" role="doc-biblioref">[12]</a></span>).
These are variables on socio-demographics (e.g., sex and age),
self-reported lifestyle indicators (e.g., alcohol and cigarette consumption), single-nucleotide polymorphism (SNP), laboratory measurements (e.g., serum concentrations), and liver ultrasound.
The two available variables of liver ultrasound are <code>stea_s2</code> and <code>stea_alt75_s2</code>.
Both take symbolic values reflecting the probability that the participant has a fatty liver; the latter is a combination of the former and the participant’s alanine transaminase (ALAT) concentration; details are in the caption of Table <a href="imm.html#tab:03-rule-list-women">3.3</a> and in <span class="citation"><a href="references.html#ref-Voelzke:SHIP11" role="doc-biblioref">[12]</a></span>.
Almost all variables mentioned below have the suffix <code>_s2</code> indicating SHIP-2 follow-up measurements, in contrast to SHIP-0 (<code>_s0</code>) and SHIP-1 (<code>_s1</code>).
Exceptions are sex, highest school degree, and the 10 SNP variables.</p>
</div>
<div id="imm-workflow-partitioning" class="section level3" number="3.2.6">
<h3>
<span class="header-section-number">3.2.6</span> Partitioning the Dataset into Subpopulations<a class="anchor" aria-label="anchor" href="#imm-workflow-partitioning"><i class="fas fa-link"></i></a>
</h3>
<p>Because the dataset is imbalanced with respect to sex (314 females, 264 males), we decided to partition the dataset before classification.
First, we examined the class distributions for each sex.
We observed that the distributions were very different, especially for class B (see Figure <a href="imm.html#fig:03-fatty-liver-mosaic">3.5</a>).
Second, we examined the class distribution by sex and age.
We found that age was associated with the female subpopulation, but not with the male subpopulation.
Third, we identified a cutoff point for age by introducing a heuristic that determines the age value that minimizes the target variable’s standard deviation.
We then performed supervised learning separately on the partitions of female and male participants, referred to as <code>PartitionF</code> and <code>PartitionM</code> hereafter.
We also created an additional learner for the subpopulation of older female participants aged above the cutoff point of 52 (Partition <code>F:age&gt;52</code>).</p>
<p>To understand how age affects the class distribution, we introduced a heuristic that determines the cutoff age value at which <code>PartitionF</code> splits into two bins so that the standard deviations of the liver fat concentration in each bin are minimized.
Let <span class="math inline">\(splitAge\)</span> denote the cutoff value and <span class="math inline">\(X_y=\{x\in\mathtt{PartitionF}|\text{age of } x \leq splitAge\}\)</span>, <span class="math inline">\(X_z=\{x\in\mathtt{PartitionF}|\text{age of } x &gt; splitAge\}\)</span> denote the bins.
Further, let <span class="math inline">\(n\)</span> be the cardinality of <span class="math inline">\(X_y\cup{}X_z\)</span>, i.e., of <code>PartitionF</code>.
Then, we define the Sum of Weighted Standard Deviations (<span class="math inline">\(SWSD\)</span>) as</p>
<p><span class="math display" id="eq:03-swsd">\[\begin{equation}
SwSD\left(X_y,X_z\right) = \frac{|X_y|}{n}\sigma({X_y})+\frac{|X_z|}{n}\sigma({X_z})
\tag{3.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(|X_i|\)</span> is the cardinality of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\sigma(X_i)\)</span> the standard deviation of the original liver fat values.
Our heuristic selects the <span class="math inline">\(\mathsf{splitAge}\)</span> such that <span class="math inline">\(SwSD\)</span> is minimal.
For <code>PartitionF</code>, the minimum value was 7.44 at the age of 52, i.e., close to the onset of menopause.</p>

<div class="figure" style="text-align: center">
<span id="fig:03-histogram-partitions-age-liverfat"></span>
<img src="figures/03-histogram-partitions-age-liverfat.png" alt="Distribution of liver fat concentration for each partition. Distribution of liver fat concentration in male participants (PartitionM), and females younger and older than 52 years. The horizontal axis shows the liver fat concentration in bins of 5%, while the vertical axis indicates the number of participants in each bin." width="100%"><p class="caption">
Figure 3.6: <strong>Distribution of liver fat concentration for each partition.</strong> Distribution of liver fat concentration in male participants (<code>PartitionM</code>), and females younger and older than 52 years. The horizontal axis shows the liver fat concentration in bins of 5%, while the vertical axis indicates the number of participants in each bin.
</p>
</div>
<p>The histograms in Figure <a href="imm.html#fig:03-histogram-partitions-age-liverfat">3.6</a> depict the differences in the liver fat concentration distributions at the age cutoff value of 52.
Next to <code>PartitionM</code> (n=264), we show the subpartitions <span class="math inline">\(\mathsf{F:age\leq{}52}\)</span> (n=131) and <code>F:age&gt;52</code> (n=183) of <code>PartitionF</code>.
Most of the female participants in <span class="math inline">\(\mathsf{F:age\leq{}52}\)</span> have no more than 5% liver fat concentration, and ca. 95% have no more than 10%, i.e., they belong to the negative class A.
In contrast, ca. 28% of <code>F:age&gt;52</code> have a liver fat concentration of more than 10%; they belong to the positive classes B and C.</p>
</div>
</div>
<div id="imm-experiments" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Experiments and Findings<a class="anchor" aria-label="anchor" href="#imm-experiments"><i class="fas fa-link"></i></a>
</h2>
<!-- We learned models on the full dataset and on each partition for each of the classification variants described in&nbsp;\@ref(subsub:decisiontrees) and for HotSpot rules. We also studied tree regression on the complete dataset. However, the predicitive power of the regression trees was very poor: either the regression tree consisted solely of one node with the mean of the complete dataset as predictor, i.e. the regression algorithm could not find appropriate split attributes, or two or more leaf nodes had very similar prediction values, whereupon interpreting the tree was very hard. We therefore focussed on classification trees and classification rules. We report on our findings with these methods hereafter. -->
<div id="imm-experiments-trees" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Results of Decision Tree Classifiers<a class="anchor" aria-label="anchor" href="#imm-experiments-trees"><i class="fas fa-link"></i></a>
</h3>
<p>For the evaluation of decision tree classifiers, we consider accuracy, i.e., the ratio of correctly classified participants, sensitivity and specificity, and the F-measure, i.e., the harmonic mean between precision and recall.
We consider the two classes B and C together as the positive class for specificity, precision, and recall.</p>
<p><em>Oversampling</em> achieved the best performance with an accuracy of about 80% and an F-measure score of 62%.
We found the best decision trees for <code>F:age&gt;52</code>, followed by those for <code>PartitionF</code>, then <code>PartitionM</code>.
The large discrepancy between the accuracy and F-measure scores also appears in the partitions’ models, suggesting that the accuracy scores are unreliable in such a skewed distribution. Therefore, we do not report on accuracy below.</p>
<p>On partition <code>F:age&gt;52</code>, the overall best decision tree is achieved by the oversampling variant.
On the larger <code>PartitionF</code>, the best performance was achieved by the decision tree created with the InfoGain variant.
In contrast, the best decision tree on <code>PartitionM</code> was created with the CostMatrix variant.
The sensitivity and specificity values for these trees are given in Table <a href="imm.html#tab:03-tree-performance-sens-spec">3.2</a>, while the trees themselves are shown in Figures <a href="imm.html#fig:03-tree-menopause">3.7</a> - <a href="imm.html#fig:03-tree-men">3.9</a> and discussed in Section <a href="imm.html#imm-experiments-important-features">3.3.3</a>.</p>

<div class="inline-table"><table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto;'>
<caption>
<span id="tab:03-tree-performance-sens-spec">Table 3.2: </span><strong>Best decision trees for the three partitions.</strong> Best separation is achieved in <code>F:age&gt;52</code>; <code>PartitionM</code> is the most heterogeneous one, the performance values are lowest.
</caption>
<thead><tr>
<th style="text-align:left;font-weight: bold;">
Partition
</th>
<th style="text-align:left;font-weight: bold;">
Variant
</th>
<th style="text-align:right;font-weight: bold;">
Sensitivity (%)
</th>
<th style="text-align:right;font-weight: bold;">
Specificity (%)
</th>
<th style="text-align:right;font-weight: bold;">
F-measure (%)
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>F:age&gt;52</code>
</td>
<td style="text-align:left;">
Oversampling
</td>
<td style="text-align:right;">
63.5
</td>
<td style="text-align:right;">
93.9
</td>
<td style="text-align:right;">
81.5
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>PartitionF</code>
</td>
<td style="text-align:left;">
InfoGain
</td>
<td style="text-align:right;">
52.4
</td>
<td style="text-align:right;">
94.9
</td>
<td style="text-align:right;">
69.7
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>PartitionM</code>
</td>
<td style="text-align:left;">
CostMatrix
</td>
<td style="text-align:right;">
38.3
</td>
<td style="text-align:right;">
86.3
</td>
<td style="text-align:right;">
53.0
</td>
</tr>
</tbody>
</table></div>
<p>Table <a href="imm.html#tab:03-tree-performance-sens-spec">3.2</a> indicates that the decision tree variants perform differently on different partitions.
Oversampling is beneficial for <code>F:age&gt;52</code> because it partially compensates for the class imbalance problem.
As <code>PartitionM</code> has the most heterogeneous class distribution out of all partitions, all variants perform relatively poorly on it.
Hence, we expected most insights from the decision trees on <code>F:age&gt;52</code> and <code>PartitionF</code>, where better separation is achieved.</p>

<div class="figure" style="text-align: center">
<span id="fig:03-tree-menopause"></span>
<img src="figures/03-tree-menopause.png" alt="Best decision tree for F:age&gt;52, achieved by the variant Oversampling." width="100%"><p class="caption">
Figure 3.7: <strong>Best decision tree for <code>F:age&gt;52</code></strong>, achieved by the variant <em>Oversampling</em>.
</p>
</div>

<div class="figure" style="text-align: center">
<span id="fig:03-tree-women"></span>
<img src="figures/03-tree-women.png" alt="Best decision tree for PartitionF, achieved by the variant InfoGain." width="100%"><p class="caption">
Figure 3.8: <strong>Best decision tree for <code>PartitionF</code></strong>, achieved by the variant <em>InfoGain</em>.
</p>
</div>

<div class="figure" style="text-align: center">
<span id="fig:03-tree-men"></span>
<img src="figures/03-tree-men.png" alt="Best decision tree for PartitionM, achieved by the variant CostMatrix." width="100%"><p class="caption">
Figure 3.9: <strong>Best decision tree for <code>PartitionM</code></strong>, achieved by the variant <em>CostMatrix</em>.
</p>
</div>
</div>
<div id="imm-experiments-rules" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> Discovered Classification Rules<a class="anchor" aria-label="anchor" href="#imm-experiments-rules"><i class="fas fa-link"></i></a>
</h3>
<p>While the classification rules found by HotSpot on the whole dataset were conclusive for class A but not for the positive classes B and C, we omit to report these rules as they are not useful for diagnostic purposes.
The classification rules found on the partitions were more informative.
However, classification rules with only one feature in the antecedent had low confidence.
To ensure high confidence, we restricted the output on rules with at least two features in the antecedent.
To ensure still high coverage, we allowed for at most three features.
A selection of high confidence and high coverage rules for each partition and class are shown in Tables <a href="imm.html#tab:03-rule-list-women">3.3</a> - <a href="imm.html#tab:03-rule-list-men">3.5</a>, respectively.
We describe the most important features in the antecedent of these rules in the next subsection, together with the most important features of the best decision trees.</p>

<div class="inline-table"><table class=" lightable-classic table" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;'>
<caption>
<span id="tab:03-rule-list-women">Table 3.3: </span><strong>Classification rules (<code>PartitionF</code>).</strong> Best HotSpot classification rules (<em>maxLength</em> = 3) for <code>PartitionF</code> (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; blt_beg_s2: time of blood sampling; ggt_s_s2: serum Gamma-glutamyltransferase (GGT; <span class="math inline">\(\mu\)</span>mol/sl); gluc_s_s2: serum glucose (mmol/l); gx_rs11597390: genetic marker; hrs_s_s2: serum uric acid concentration (µmol/l); ldl_s_s2: serum low-density lipoprotein (LDL; mmol/l); sleeph_s2: sleep hours; sleepp_s2: sleep problems; som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) and alanine aminotransferase (ALAT) concentration <span class="math inline">\(\geq\)</span> 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis); tg_s_s2: serum triglycerides (mmol/l); tsh_s2: thyroid-stimulating hormone (TSH; mu/l).
</caption>
<thead>
<tr>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Rule antecedent
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Cov
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Sup
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Conf
</div>
</th>
</tr>
<tr>
<th style="text-align:left;font-weight: bold;">
Variable 1
</th>
<th style="text-align:left;font-weight: bold;">
Variable 2
</th>
<th style="text-align:left;font-weight: bold;">
Variable 3
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
</tr>
</thead>
<tbody>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: A</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_waist_s2 <span class="math inline">\(\leq\)</span> 80
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
52
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 <span class="math inline">\(\leq\)</span> 24.82
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_huef_s2 <span class="math inline">\(\leq\)</span> 97.8
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
118
</td>
<td style="text-align:right;">
117
</td>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
99
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
218
</td>
<td style="text-align:right;">
214
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:right;">
98
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_alt75_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
202
</td>
<td style="text-align:right;">
198
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
98
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: B</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
gx_rs11597390 = 1
</td>
<td style="text-align:left;">
age_ship_s2 &gt; 59
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
85
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_alt75_s2 = 1
</td>
<td style="text-align:left;">
hrs_s_s2 &gt; 263
</td>
<td style="text-align:left;">
age_ship_s2 &gt; 59
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
85
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_alt75_s2 = 1
</td>
<td style="text-align:left;">
hrs_s_s2 &gt; 263
</td>
<td style="text-align:left;">
ldl_s_s2 &gt; 3.22
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
85
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
age_ship_s2 &gt; 66
</td>
<td style="text-align:left;">
tg_s_s2 &gt; 1.58
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
82
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
age_ship_s2 &gt; 64
</td>
<td style="text-align:left;">
hrs_s_s2 &gt; 263
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
82
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: C</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
gluc_s_s2 &gt; 7
</td>
<td style="text-align:left;">
tsh_s2 &gt; 0.996
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 38.42
</td>
<td style="text-align:left;">
age_ship_s2 <span class="math inline">\(\leq\)</span> 66
</td>
<td style="text-align:left;">
asat_s_s2 &gt; 0.22
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 38.42
</td>
<td style="text-align:left;">
sleeph_s2 &gt; 6
</td>
<td style="text-align:left;">
blt_beg_s2 <span class="math inline">\(\leq\)</span> 38340
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 38.42
</td>
<td style="text-align:left;">
sleeph_s2 &gt; 6
</td>
<td style="text-align:left;">
stea_s2 = 1
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
hrs_s_s2 &gt; 371
</td>
<td style="text-align:left;">
sleepp_s2 = 0
</td>
<td style="text-align:left;">
ggt_s_s2 &gt; 0.55
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
100
</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class=" lightable-classic table" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;'>
<caption>
<span id="tab:03-rule-list-menopause">Table 3.4: </span><strong>Classification rules (<code>F:age&gt;52</code>).</strong> Best HotSpot classification rules (<em>maxLength</em> = 3) for <code>F:age&gt;52</code> (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; crea_u_s2: urine creatinine (mmol/l); fib_cl_s2: fibrinogen (Clauss) (g/l); gluc_s_s2: serum glucose (mmol/l); ggt_s_s2: serum Gamma-glutamyltransferase (GGT; <span class="math inline">\(\mu\)</span>mol/sl); gx_rs11597390: genetic marker; hdl_s_s2: high-density lipoprotein (mmol/l); hrs_s_s2: serum uric acid concentration (µmol/l); som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) and alanine aminotransferase (ALAT) concentration <span class="math inline">\(\geq\)</span> 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis).
</caption>
<thead>
<tr>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Rule antecedent
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Cov
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Sup
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Conf
</div>
</th>
</tr>
<tr>
<th style="text-align:left;font-weight: bold;">
Variable 1
</th>
<th style="text-align:left;font-weight: bold;">
Variable 2
</th>
<th style="text-align:left;font-weight: bold;">
Variable 3
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
</tr>
</thead>
<tbody>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: A</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
crea_u_s2 <span class="math inline">\(\leq\)</span> 5.39
</td>
<td style="text-align:left;">
stea_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
57
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
crea_u_s2 <span class="math inline">\(\leq\)</span> 5.39
</td>
<td style="text-align:left;">
stea_alt75_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_waist_s2 <span class="math inline">\(\leq\)</span> 80
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 <span class="math inline">\(\leq\)</span> 24.82
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
crea_u_s2 <span class="math inline">\(\leq\)</span> 5.39
</td>
<td style="text-align:left;">
ggt_s_s2 <span class="math inline">\(\leq\)</span> 0.43
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: B</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
ggt_s_s2 &gt; 0.48
</td>
<td style="text-align:left;">
ggt_s_s2 <span class="math inline">\(\leq\)</span> 0.63
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
gx_rs11597390 = 1
</td>
<td style="text-align:left;">
hdl_s_s2 <span class="math inline">\(\leq\)</span> 1.53
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
95
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
gx_rs11597390 = 1
</td>
<td style="text-align:left;">
fib_cl_s2 &gt; 3.4
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
93
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
crea_s_s2 <span class="math inline">\(\leq\)</span> 61
</td>
<td style="text-align:left;">
som_waist_s2 &gt; 86
</td>
<td style="text-align:left;">
stea_s2 = 1
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
93
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 1
</td>
<td style="text-align:left;">
gx_rs11597390 = 1
</td>
<td style="text-align:left;">
hrs_s_s2 &gt; 261
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
90
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: C</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 38.42
</td>
<td style="text-align:left;">
age_ship_s2 <span class="math inline">\(\leq\)</span> 66
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 38.42
</td>
<td style="text-align:left;">
stea_alt75_s2 = 3
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_huef_s2 &gt; 124
</td>
<td style="text-align:left;">
stea_alt75_s2 = 3
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_waist_s2 &gt; 108
</td>
<td style="text-align:left;">
gluc_s_s2 &gt; 6.2
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_alt75_s2 = 3
</td>
<td style="text-align:left;">
som_bmi_s2 &gt; 37.32
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
100
</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class=" lightable-classic table" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;'>
<caption>
<span id="tab:03-rule-list-men">Table 3.5: </span><strong>Classification rules (<code>PartitionM</code>).</strong> Best HotSpot classification rules (<em>maxLength</em> = 3) for <code>PartitionM</code> (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; ATC_C09AA02_s2: enalapril intake; chol_s_s2: serum cholesterol (mmol/l); crea_u_s2: urine creatinine (mmol/l); crea_s_s2: serum creatinine (µmol/l); fig_cl_s2: Fibrinogen (Clauss) (g/l); ggt_s_s2: serum Gamma-glutamyltransferase (GGT; <span class="math inline">\(\mu\)</span>mol/sl); gout_s2: treated gout (self-report); hdl_s_s2: high-density lipoprotein (mmol/l); hgb_s2: haemoglobin (g/l); hrs_s_s2: serum uric acid concentration (µmol/l); jodid_u_s2: urine iodide (µg/dl); quick_s2: thromboplastin time Quick test (%); sleeph_s2: sleep hours; stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) and alanine aminotransferase (ALAT) concentration <span class="math inline">\(\geq\)</span> 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis); som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); tg_s_s2: serum triglycerides (mmol/l).
</caption>
<thead>
<tr>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="3">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Rule antecedent
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Cov
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="2">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Sup
</div>
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; " colspan="1">
<div style="border-bottom: 1px solid #111111; margin-bottom: -1px; ">
Conf
</div>
</th>
</tr>
<tr>
<th style="text-align:left;font-weight: bold;">
Variable 1
</th>
<th style="text-align:left;font-weight: bold;">
Variable 2
</th>
<th style="text-align:left;font-weight: bold;">
Variable 3
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Abs
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
<th style="text-align:right;font-weight: bold;">
Rel (%)
</th>
</tr>
</thead>
<tbody>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: A</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_alt75_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
106
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
95
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
stea_s2 = 0
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
138
</td>
<td style="text-align:right;">
131
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
95
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 <span class="math inline">\(\leq\)</span> 0.52
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
73
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
92
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
hrs_s_s2 <span class="math inline">\(\leq\)</span> 310
</td>
<td style="text-align:left;">
ggt_s_s2 <span class="math inline">\(\leq\)</span> 0.77
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
81
</td>
<td style="text-align:right;">
74
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
91
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_waist_s2 <span class="math inline">\(\leq\)</span> 90.8
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
91
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: B</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_huef_s2 &gt; 108.1
</td>
<td style="text-align:left;">
age_ship_s2 &gt; 39
</td>
<td style="text-align:left;">
crea_u_s2 &gt; 7.59
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
79
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 32.29
</td>
<td style="text-align:left;">
hdl_s_s2 &gt; 0.94
</td>
<td style="text-align:left;">
ATC_C09AA02_s2 = 0
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
76
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_bmi_s2 &gt; 32.29
</td>
<td style="text-align:left;">
hgb_s2 &gt; 8.1
</td>
<td style="text-align:left;">
gout_s2 = 0
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
76
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_waist_s2 &gt; 109
</td>
<td style="text-align:left;">
sleeph_s2 <span class="math inline">\(\leq\)</span> 8
</td>
<td style="text-align:left;">
jodid_u_s2 &gt; 9.44
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
76
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
som_huef_s2 &gt; 108.1
</td>
<td style="text-align:left;">
hdl_s_s2 &gt; 0.97
</td>
<td style="text-align:left;">
crea_u_s2 &gt; 5.38
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
76
</td>
</tr>
<tr grouplength="5">
<td colspan="7" style="border-bottom: 0;">
<strong>Target class: C</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 &gt; 1.9
</td>
<td style="text-align:left;">
crea_s_s2 <span class="math inline">\(\leq\)</span> 90
</td>
<td style="text-align:left;">
quick_s2 &gt; 59
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 &gt; 1.9
</td>
<td style="text-align:left;">
crea_s_s2 <span class="math inline">\(\leq\)</span> 90
</td>
<td style="text-align:left;">
chol_s_s2 &gt; 4.3
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 &gt; 1.9
</td>
<td style="text-align:left;">
crea_s_s2 <span class="math inline">\(\leq\)</span> 90
</td>
<td style="text-align:left;">
fib_cl_s2 &gt; 1.9
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 &gt; 1.9
</td>
<td style="text-align:left;">
crea_s_s2 <span class="math inline">\(\leq\)</span> 90
</td>
<td style="text-align:left;">
crea_u_s2 &gt; 4.74
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
100
</td>
</tr>
<tr>
<td style="text-align:left; padding-left:  2em;" indentlevel="1">
ggt_s_s2 &gt; 1.9
</td>
<td style="text-align:left;">
tg_s_s2 &gt; 2.01
</td>
<td style="text-align:left;">
som_waist_s2 &gt; 93.5
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
100
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="imm-experiments-important-features" class="section level3" number="3.3.3">
<h3>
<span class="header-section-number">3.3.3</span> Important Features for Each Subpopulation<a class="anchor" aria-label="anchor" href="#imm-experiments-important-features"><i class="fas fa-link"></i></a>
</h3>
<p>The most important features in the decision trees of Figures <a href="imm.html#fig:03-tree-menopause">3.7</a> - <a href="imm.html#fig:03-tree-men">3.9</a> are those closer to the root.
For readability, the tree nodes in the figures contain short descriptions instead of the original variable names.
In all three decision trees, the root node is the ultrasound diagnosis variable stea_s2.
A negative ultrasound diagnosis points to negative class A, but a positive ultrasound diagnosis does not directly lead to the positive classes B and C.
The decision trees of the three partitions differ in the nodes placed near the root.</p>
<!-- \subsubsection{Important Features for  `PartitionF`}  -->
<p><strong>Important features for PartitionF.</strong> In the best decision tree of <code>PartitionF</code> (cf. Figure <a href="imm.html#fig:03-tree-women">3.8</a>), it can be observed that if the ultrasound report is positive <em>and</em> the HbA1C concentration is more than 6.8%, the class is C.
The classification rules with high coverage and confidence in Table <a href="imm.html#tab:03-rule-list-women">3.3</a>) point to further interesting features:
a waist circumference of at most 80 cm, a BMI of no more than 24.82 kg/m<sup>2</sup>, a hip circumference of 97.8 cm or less characterize participants of the negative class.
All 6 participants with a serum glucose concentration greater than 7 mmol/l <em>and</em> a TSH concentration greater than 0.996 mu/l belong to class C.
Further, severe obesity (a BMI value of more than 38.42 kg/m<sup>2</sup> points to class C with high confidence – but only in combination with other variables.</p>
<!-- \subsubsection{Important Features for  `F:age>52`}  -->
<p><strong>Important features for F:age&gt;52</strong> In contrast to the best tree for <code>PartitionF</code>, the best decision tree for the subpartition <code>F:age&gt;52</code> (cf. Figure <a href="imm.html#fig:03-tree-menopause">3.7</a>) also contains nodes with SNPs, indicating potentially genetic associations to fatty liver for these participants.
Classification rules with high coverage and confidence for class B also contain SNPs, as shown in Table <a href="imm.html#tab:03-rule-list-menopause">3.4</a>.
Similar to <code>PartitionF</code>, high BMI values point to a positive class when combined with other features.
Table <a href="imm.html#tab:03-rule-list-menopause">3.4</a> shows that all four participants with stea_alt75_s2 = 3 (i.e., a positive ultrasound diagnosis combined with a critical ALAT value) and a BMI larger than 38.42 kg/m<sup>2</sup> belong to class C.
A similar association holds for stea_alt75_s2 = 3 combined with a high waist circumference (&gt; 124 cm).
19 out of 20 participants in class B with a positive ultrasound diagnosis, a genetic marker gx_rs11597390 = 1, and a high-density lipoprotein (HDL) serum concentration of at most 1.53 mmol/l.</p>
<!-- \subsubsection{Important Features for `PartitionM`} -->
<p><strong>Important features for PartitionM.</strong> The role of the ultrasound report in predicting the negative class is the same for <code>PartitionM</code> (cf. Figure <a href="imm.html#fig:03-tree-men">3.9</a> as for <code>PartitionF</code>).
As with the best tree for <code>F:age&gt;52</code>, the best tree for <code>PartitionM</code> contains nodes with SNPs and serum Gamma-glutamyltransferase (GGT) value ranges.
Such features are also in the antecedent of top Hotspot rules (cf. Table <a href="imm.html#tab:03-rule-list-men">3.5</a>): a Serum GGT concentration of more than 1.9 <span class="math inline">\(\mu\)</span>mol/sl in combination with creatinine concentration of at most 90 mmol/l or a thromboplastin time ratio (quick_s2) of more than 59% points to class C.
Similarly, positive ultrasound diagnosis and a serum HDL concentration not exceeding 0.84 mmol/l point to class C.</p>
<!-- \subsubsection{Conclusion on important features}  -->
<p>The decision trees and classification rules provide insights into features that appear diagnostically important.
However, the medical expert needs additional information to decide whether a feature is worth further investigation.
Decision trees highlight the importance of a feature only in the context of the subtree in which it is found; a subtree describes a typically very small subpopulation.
In contrast, classification rules provide information about larger subpopulations.
However, these subpopulations may overlap; for example, the first four rules on class C for <code>PartitionM</code> (cf. Table <a href="imm.html#tab:03-rule-list-men">3.5</a>) may refer to the same 6 participants.</p>
<p>Furthermore, unless a classification rule has a confidence value close to 100%, participants in the other classes may also support it.
Therefore, to decide whether the features in the antecedent of the rule deserve further investigation, the expert also needs knowledge about the statistics of the rule for the other classes.
To assist the expert in this task, we have proposed the Interactive Medical Miner.
This tool discovers classification rules for each class <em>and</em> provides information about the statistics of these rules for all classes.</p>
</div>
</div>
<div id="imm-conclusion" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Conclusion<a class="anchor" aria-label="anchor" href="#imm-conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>To date, analysis of population-based cohort data has been mostly hypothesis-driven.
We have presented a workflow and an interactive application for data-driven analysis of population-based cohort data using hepatic steatosis as an example.
Our mining workflow includes steps</p>
<ol style="list-style-type: lower-roman">
<li>to discover subpopulations that have different distributions with respect to the target variable,</li>
<li>to classify each subpopulation taking class imbalance into account, and</li>
<li>to identify variables associated with the target variable.</li>
</ol>
<p>Our workflow has shown that it is appropriate (a) to build subpopulations before classification to reduce class imbalance and (b) to drill-down on the derived models to identify important variables and subpopulations worthy of further investigation.</p>
<p>To assist the domain expert with the latter objective (b), we have developed the Interactive Medical Miner, an interactive application that allows the user to explore classification rules further and understand how the cohort participants supporting each rule are distributed across the three classes.
This exploration step is essential for identifying not-yet-known associations between some variables and the target.
These variables must then be further investigated – in hypothesis-driven studies.
Therefore, our workflow and Interactive Medical Miner carry the potential of data-driven analysis to provide insights into a multifactorial disease and generate hypotheses for hypothesis-driven studies.
Our Interactive Medical Miner has been extended by Schleicher et al. <span class="citation"><a href="references.html#ref-Schleicher:CBMS17" role="doc-biblioref">[114]</a></span>, who added panels that include tables showing additional rule statistics such as lift and p-value.
Besides, a mosaic plot contrasts the class distributions of a subpopulation and its “complements,” i.e., subsets of participants who do not meet one or both conditions of a length-2 rule describing that subpopulation.</p>
<p>In terms of the multifactorial disorder of interest, our results confirm the potential of our data-driven approach because most of the variables in the top positions of our decision trees and classification rules have been previously shown to be associated with hepatic steatosis in independent studies.
In particular, indices of fat accumulation in the body (BMI, waist circumference) and the liver enzyme GGT were proposed by Bedogni et al. <span class="citation"><a href="references.html#ref-BedogniEtAl:2006" role="doc-biblioref">[115]</a></span> as a reliable “Fatty Liver Index.”
According to Yuan et al. <span class="citation"><a href="references.html#ref-yuan2008population" role="doc-biblioref">[116]</a></span>, the SNPs rs11597390, rs2143571, and rs11597086 are among the “Independent SNPs Associated with Liver-Enzyme Levels with Genome-wide Significance in Combined GWAS Analysis of Discovery and Replication Data Sets.”
Regarding the effects of alcohol consumption, toxic effects of alcohol on the liver are well established and ascribe an even more significant role to obesity than heavy alcohol consumption concerning fat accumulation in the liver <span class="citation"><a href="references.html#ref-BaumeisterEtAl:2008" role="doc-biblioref">[117]</a>, <a href="references.html#ref-BellentaniEtAl:InternMed2000" role="doc-biblioref">[118]</a></span>.
Indeed, a variable related to alcohol consumption appears only in our decision tree on <code>F:age&gt;52</code> (see Figure <a href="imm.html#fig:03-tree-menopause">3.7</a>) and not among our top classification rules, where we tend to see variables associated with a person’s weight and obesity (cf. variables som_bmi_s2, som_huef_s2, som_waist_s2 in all figures and tables in Section <a href="imm.html#imm-experiments">3.3</a>).
The subpopulation <code>F:age&gt;52</code> itself was identified without prior knowledge of this subpopulation’s semantics.
Still, it is noteworthy that the age of 52 years is close to the onset of menopause – Völzke et al. <span class="citation"><a href="references.html#ref-VoelzkeEtAl:Gut2007" role="doc-biblioref">[119]</a></span> showed that menopausal status is associated with hepatic steatosis.
Our results also verify another fact that was known to medical experts by independent observation: the sonographic variables (cf. stea_s2, stea_alt75_s2 in all figures and tables in Section <a href="imm.html#imm-experiments">3.3</a>) is associated with liver fat concentration found on MRI, but ultrasound alone does not predict hepatic steatosis <span class="citation"><a href="references.html#ref-BedogniEtAl:2006" role="doc-biblioref">[115]</a>, <a href="references.html#ref-BellentaniEtAl:InternMed2000" role="doc-biblioref">[118]</a></span>.</p>
<p>Our algorithms not only provide variables associated with the target but also identify the value intervals related to a specific class, see, for example, the value intervals of BMI associated with class B for <code>PartitionM</code> (Table <a href="imm.html#tab:03-rule-list-men">3.5</a>) and with classes A and C for <code>F:age&gt;52</code> (Table <a href="imm.html#tab:03-rule-list-menopause">3.4</a>).
These intervals do not imply that a person with a BMI within the specific interval actually belongs to the corresponding class but can serve as a starting point for hypothesis-driven analyses.</p>
<!-- A limitation of our approach concerns the interaction with the medical expert. The Interactive Medical Miner has been designed with the demands of the medical expert in mind, but it has not yet been evaluated by medical experts. As a result, we maximized flexibility through a set of parameters, but it remains to be shown whether the presentation of these parameters is intuitive to the user. Also, the visualization aids (histograms) are rudimentary and must yet be evaluated with respect to the expert's intuition. To this purpose, we  intend to set up an appropriate environment in which an expert will interact with the tool and will give us feedback. With respect to the concrete findings for hepatic steatosis, a shortcoming is that some confounders for chemical shift MR fat quantifications have not been corrected in the target variable we used (cf. beginning of section \ref{subsec:dataset}), hence the exact value intervals of the associated variables should not be taken at face value. This is not a shortcoming of the approach itself, though; our next step is to apply it on the corrected dataset. -->
<p>Our approach allows us to study subpopulations at two points in time.
Before the modeling step, we identify subpopulations that have different class distributions.
During the modeling step, our Interactive Medical Miner highlights the subpopulation that supports each classification rule; these are <em>overlapping</em> subpopulations.
Overlapping subpopulations are not necessarily a disadvantage, especially for very small subpopulations.
However, working with overlapping data sets can be unintuitive and tedious for a domain expert.
In Chapter <a href="sdclu.html#sdclu">4</a>, we explore the potential of clustering to identify and reorganize <em>overlapping</em> rules to reduce the user’s cognitive load by displaying only semantically unique and representative rules.</p>
<p>Although the workflow’s main application is a <em>longitudinal</em> cohort study, it does not exploit temporal characteristics in the data.
However, indicators of lifestyle change are potentially predictive for the later occurrence of a disease.
Chapter <a href="evo.html#evo">6</a> presents a follow-up method that expands the feature space by extracting temporal variables that describe how a study participant changes over time to derive new informative variables for hypothesis generation.</p>
<!-- [@qing2020improved] -->
<!-- But like Apriori algorithm, HotSpot algorithm has one obvious shortcoming: support selection needs to be set artificially based on experience and cannot be set accurately according to the actual scale of the problem [3]. If the support threshold is set too low, a cumbersome and complicated tree structure may be generated; if the support threshold is set too high, some associated intervals existing in the rare target attribute values may be ignored. Therefore, in the process of support selection, multiple comparison experiments are needed to determine the optimal support based on the mining results. -->

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="background.html"><span class="header-section-number">2</span> Medical Background and Datasets</a></div>
<div class="next"><a href="sdclu.html"><span class="header-section-number">4</span> Identifying Distinct Subpopulations</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#imm"><span class="header-section-number">3</span> Interactive Discovery and Inspection of Subpopulations</a></li>
<li>
<a class="nav-link" href="#imm-intro"><span class="header-section-number">3.1</span> Motivation and Comparison to Related Work</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#role-of-subpopulations-in-classifier-learning-for-cohorts"><span class="header-section-number">3.1.1</span> Role of Subpopulations in Classifier Learning for Cohorts</a></li>
<li><a class="nav-link" href="#workflows-for-expert-machine-interaction-for-cohort-construction-and-analysis"><span class="header-section-number">3.1.2</span> Workflows for Expert-Machine Interaction for Cohort Construction and Analysis</a></li>
<li><a class="nav-link" href="#previous-work-on-subpopulation-discovery-with-the-ship-data"><span class="header-section-number">3.1.3</span> Previous Work on Subpopulation Discovery with the SHIP Data</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#imm-workflow"><span class="header-section-number">3.2</span> Subpopulation Discovery Workflow and Interactive Mining Assistant</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#imm-workflow-classification"><span class="header-section-number">3.2.1</span> Classification</a></li>
<li><a class="nav-link" href="#imm-workflow-rule-discovery"><span class="header-section-number">3.2.2</span> Classification Rule Discovery</a></li>
<li><a class="nav-link" href="#imm-workflow-hotspot"><span class="header-section-number">3.2.3</span> HotSpot</a></li>
<li><a class="nav-link" href="#imm-workflow-imm"><span class="header-section-number">3.2.4</span> Interactive Medical Miner</a></li>
<li><a class="nav-link" href="#imm-workflow-target"><span class="header-section-number">3.2.5</span> The Target Variable</a></li>
<li><a class="nav-link" href="#imm-workflow-partitioning"><span class="header-section-number">3.2.6</span> Partitioning the Dataset into Subpopulations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#imm-experiments"><span class="header-section-number">3.3</span> Experiments and Findings</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#imm-experiments-trees"><span class="header-section-number">3.3.1</span> Results of Decision Tree Classifiers</a></li>
<li><a class="nav-link" href="#imm-experiments-rules"><span class="header-section-number">3.3.2</span> Discovered Classification Rules</a></li>
<li><a class="nav-link" href="#imm-experiments-important-features"><span class="header-section-number">3.3.3</span> Important Features for Each Subpopulation</a></li>
</ul>
</li>
<li><a class="nav-link" href="#imm-conclusion"><span class="header-section-number">3.4</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Intelligent Assistance for Expert-Driven Subpopulation Discovery in High-Dimensional Timestamped Medical Data</strong>" was written by Uli Niemann. It was last built on 09.03.2021.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>

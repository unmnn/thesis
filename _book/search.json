[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"‚Äôm Uli HTML preview thesis entitled ‚ÄúIntelligent Assistance Expert-Driven Subpopulation Discovery High-Dimensional Time-Stamped Medical Data‚Äù.hopefully submit final manuscript March 25, 2021.current writing progress per chapter shown .can download PDF version GitHub repository.","code":""},{"path":"index.html","id":"chapter-progress","chapter":"Welcome!","heading":"Chapter Progress","text":"1 üü¢ Introduction\r\n2 üü¢ Medical BackgroundPART SUBPOPULATION DISCOVERY HIGH-DIMENSIONAL DATA3 üü¢ Interactive Discovery Inspection Subpopulations\r\n4 üü¢ Identification Distinct Subpopulations\r\n5 üü¢ Visual Identification Informative FeaturesPART II EXPLOITING DYNAMICS6 üü¢ Extraction Evolution Features Cohort Data\r\n7 üî¥ Extraction Features Short Temporal Sequences (written 14.02)PART III POST-MINING INTERPRETATION8 üü¢ Post-Hoc Interpretation Classification Models\r\n9 üî¥ Subpopulation-Specific Learning Post-Hoc Model Interpretation (written 07.02)PART IV SUMMARY10 üî¥ Conclusion Future Work (written 21.02)Legend:\r\nüèÅüèÅ = submission-ready\r\nüèÅ = feedback reviewers incorporated\r\nüü¢ = (preliminary) draft ready\r\nüîµ = maturing\r\nüî¥ = unwritten","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"motivation-and-objectives","chapter":"1 Introduction","heading":"1.1 Motivation and Objectives","text":"Common objectives data analysis medical research include () identifying long-term determinants protective factors medical condition interest, (ii) discovering subpopulations increased outcome prevalence, (iii) generating robust statistical models can explain relationships one independent variables target variable.\r\nexample, epidemiologists attempt discover associations multiple risk factors outcome cohort studies collecting data include extensive information participants obtained questionnaires, medical examinations, laboratory analyses, imaging.\r\nOften data collected repeatedly time, example longitudinal studies.\r\nmeans latent often overlooked temporal information data, investigation can potentially lead new insights.find associations variables, medical researchers usually first carefully derive hypotheses clinical practice, experimental studies, extensive literature reviews test formally statistical significance.\r\nHowever, ever-increasing volume heterogeneity medical data, traditional hypothesis-driven workflows becoming increasingly impractical, reason important inherent associations variables may go undetected.\r\nMachine learning can improve medical research discovering understandable descriptions patient study participant subpopulations similar outcome, thus can used derive new hypotheses.proliferation medical machine learning applications triggered several reasons, desire make automated use plethora information collected study subjects, sometimes just based ubiquity deep learning success stories media.\r\nHowever, ease creating complex data-driven models guarantee insights can effortlessly derived.\r\nstate---art machine learning algorithms deep neural networks gradient boosting machines generate -called black-box models multiple layers complexity involve many multivariate, nonlinear interactions variables difficult represent intuitively.\r\ncritical application expert, practitioner scientist working clinical epidemiological setting, equipped tools understand, explore, visualize models can drill specific individual patterns gain actionable insights ultimately contribute prevention, diagnosis, treatment clinical practice.\r\nmedical data come wide variety sources key characteristics collected datasets vary, requiring adaptation methods specifics application scenario.goal work develop methods serve intelligent assistance medical researchers analysis high-dimensional, temporal medical data.\r\nHence, core research question thesis : derive accurate yet understandable patterns subpopulation discovery high-dimensional temporal medical data?\r\n, , generation machine learning models, several challenges must overcome order domain expert able derive actionable knowledge.\r\nchallenges can translated following four requirements.\r\n(1) Comprehensibility patterns: extracted models, including clusters, rules, patterns, must made understandable; preferably, model generation process also comprehensible.\r\n(2) Exploitation time: latent temporal information must exploited, satisfying requirement 1.\r\n\r\n(3) Minimization redundancy: redundancy must minimized, satisfying requirement (1).\r\nPatterns may overlap terms topics cover.\r\nleads redundancy patterns, negative impact perceived quality model.\r\ntask extract, process display relevant (temporal) patterns expert-driven model exploration.","code":""},{"path":"intro.html","id":"structure-and-contributions-of-this-thesis","chapter":"1 Introduction","heading":"1.2 Structure and Contributions of This Thesis","text":"thesis presents solutions support medical researchers data-driven analysis high-dimensional, temporal medical data.\r\nDesign decisions developments partly inspired suggestions respective domain experts cooperation partners, including specialist internal medicine, epidemiologist statistical expertise, diabetes expert three tinnitus specialists.\r\nthesis organized three parts ten chapters tackling aforementioned research question challenges.\r\nPart¬†covers methods discovering subpopulations high-dimensional data.\r\nPart¬†II focuses specifically temporal aspects medical datasets provides approaches extract informative representations latent temporal data.\r\nPart¬†II addresses post-hoc analysis machine learning models includes solutions derive model-, observation-, subpopulation-level insights otherwise ‚Äúopaque‚Äù black boxes.Chapter¬†2 (Medical Background) presents relevant medical background information, brief comparison medical study types overview case studies solutions tailored .Chapter¬†3 (Interactive Discovery Inspection Subpopulations) presents workflow interactive data-driven analysis population-based cohort data using hepatic steatosis example. mining workflow includes steps () identify subpopulations different distributions respect target variable, (ii) classify subpopulation taking class imbalance account, (iii) identify variables associated outcome. show workflow suited () identify subpopulations classification reduce class imbalance, (b) drill-derived models identify important variables subpopulations worthy investigation.Chapter¬†4 (Identifying Distinct Subpopulations) examines redundancy large rule sets describing subpopulations. present workflow extracts smaller number representative rules. rules selected avoid instance overlap much possible, thus covering different concepts data space. evaluate workflow two samples longitudinal cohort study two target variables, respectively.Chapter¬†5 (Visual Identification Informative Features) describes approach identify distinct tinnitus phenotypes parameter-free clustering, presents novel visualizations juxtapose phenotypes high-dimensional feature space explore phenotype-specific characteristics.Chapter¬†6 (Constructing Evolution Features Capture Study Participant Change Time) present framework cohort analysis longitudinal cohort studies constructs ‚Äúevolution features‚Äù latent temporal information describing change cohort participants time. show exploiting novel features improves generalization performance classification models report results longitudinal cohort study.Chapter¬†7 (Feature Extraction Short Temporal Sequences Clustering) present approach build representations short temporal sequences via clustering example pressure- posture-dependent plantar temperature pressure patients diabetic foot syndrome.Chapter¬†8 (Post-Hoc Interpretation Classification Models) focuses making already learned, complex classification models understandable domain experts. provide workflow combines classification high-dimensional medical data model explanation using post-hoc interpretation methods.\r\nend, use Shapely value explanations (SHAP), LASSO coefficients, partial dependency graphs.\r\napproach delivers statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance, help illuminate complex black-box machine learning models.\r\nreport results three applications: () tinnitus-related distress tinnitus patients, (ii) depressivity tinnitus patients, (iii) rupture risk intracranial aneurysms.Chapter¬†9 (Subpopulation-Specific Learning Post-Hoc Model Interpretation) describes approach examines subpopulations differ respect predictive characteristics temporal data. , derive post-hoc interpretation measure assess difference association predictors two subpopulations. report results CHA gender differences (subpopulations female male patients) two outcomes tinnitus-related distress depression, effect treatment outcomes.Chapter¬†10 (Conclusion Future Work) concludes thesis giving summary contributions detailed perspectives presented work.","code":""},{"path":"background.html","id":"background","chapter":"2 Medical Background & Datasets","heading":"2 Medical Background & Datasets","text":"Medical research data clinical epidemiological studies lays foundation decisions diagnosis treatment multifactorial conditions diseases disorders.\r\nMajor goals identify long-term determinants protective factors outcome interest, discover subpopulations increased disease prevalence, study intervention effects generating statistical models explain cause--effect relationships.\r\ndate, analysis population-based cohort data mostly hypothesis-driven.\r\nTraditional medical data analysis pipelines usually structured hypothesis-driven way follows:medical scientist formulates hypothesis based observations clinical practice current research. Possible examples include: ‚Äúrisk factor alcohol abuse affect prevalence particular outcome?‚Äù ‚Äúeffect novel therapy patients depressive symptoms?‚Äùtest hypothesis, small set relevant variables can controlled confounders selected. Variable selection may include controlling confounders. data necessary test hypothesis collected.strength associations selected variables outcome assessed using regression models statistical methods.Based results, inferential statistical calculations performed conclusions drawn may support implementation new preventive interventions use appropriate treatments high-risk patients.chapter divided two parts.\r\nFirst, give brief comparison medical study types Section¬†2.1.\r\n, Section¬†2.2, present studies data samples developed methods presented thesis.","code":""},{"path":"background.html","id":"background-med-research","chapter":"2 Medical Background & Datasets","heading":"2.1 Brief Comparison of Medical Study Types","text":"Primary medical research can divided basic, clinical epidemiological studies¬†[1], [2].Basic research.\r\nBasic medical research (experimental research) aims improve understanding cellular, molecular, physiological mechanisms human health disease conducting cellular molecular investigations, animal studies, drug material property studies tightly controlled laboratory environments¬†[2].\r\nstudy effects one variables interest outcome, variables usually held constant variables interest varied.\r\ncarefully standardized experimental conditions basic medical studies ensure high internal validity, conditions often easily transferred clinical practice without compromising generalizability results.Clinical studies. Clinical studies generally classified interventional experimental studies, non-interventional observational studies.\r\ngeneral objective intervention study compare different treatments within patient population whose members differ little possible except treatment arm.\r\ncommon example pharmaceutical study aims validate efficacy safety drug investigating establishing drug‚Äôs main side effects, absorption, metabolism, excretion.\r\nSelection bias can avoided appropriate measures, particular randomly assigning patients groups.\r\nTreatment may medication, surgery, therapeutic use medical device (e.g., stent), well physical therapy, acupuncture, psychosocial intervention, rehabilitation, training form, diet.\r\nrandomized controlled trial (RCT) considered gold standard study design minimizes selection bias () randomly assigning patients treatment control groups (b) ensuring equal distribution known unknown influencing variables (confounders), risk factors, comorbidities, genetic variability.\r\nRCTs thus suitable obtaining unambiguous answer clear question proving causality.Non-interventional clinical trials patient-based observational studies patients either receive individually defined treatment patients receive exact treatment.\r\nexample non-interventional design study investigating regular use drugs therapies.\r\n, treatment well diagnosis monitoring follow predefined study protocol, rather follow medical practice alone.\r\nData analysis often retrospective.\r\nWhether study design prospective retrospective depends sequence hypothesis generation data collection.\r\nprospective studies, hypothesis generation comes data collection.\r\nFirst, hypotheses tested defined, e.g., regarding new treatment procedure.\r\n, data collected specifically hypothesis testing.\r\nfirst formulating testable hypotheses, possible ensure research questions can actually answered measured data.\r\nretrospective study design means data collection took place study began.Epidemiological studies.\r\nEpidemiological studies usually interested distribution change time incidence diseases causes general population subpopulations.\r\nCohort studies examine individuals, health outcomes interest beginning observation period, assess exposure status various health-related conditions.\r\nincluded subjects followed time called longitudinal studies (opposed cross-sectional studies), outcomes interest recorded multiple waves.\r\ndata, researchers can establish subgroups subjects exposure status, sort exposure, compare incidence prevalence disease among exposure categories.\r\n\r\nLongitudinal studies categorized trend panel design.\r\ntrend study, wave can involve different participant sample, .e., individual participant followed time.\r\ncontrast, panel study investigates population multiple points time allows also measure intra-individual temporal changes.","code":""},{"path":"background.html","id":"background-data","chapter":"2 Medical Background & Datasets","heading":"2.2 Datasets Investigated in This Thesis","text":"However, advent big data¬†[3] various fields, including medicine, volume heterogeneity data increasing dramatically, making traditional hypothesis-driven workflows increasingly inadequate important relationships variables may go undetected.\r\nthesis, present methods deal different aspects high-dimensional, time-stamped medical data.\r\nevaluate methods variety datasets diverse study types.\r\nsection, present data associated studies, namelythe Study Health Pomerania Section¬†2.2.1,Diabetic Foot Clinical Trial Section¬†2.2.2,Intracranial Aneurysm Angiography Image Dataset¬†2.2.3, andthe Charit√© Tinnitus Patients Observational Therapy Study Dataset¬†2.2.4.","code":""},{"path":"background.html","id":"background-data-ship","chapter":"2 Medical Background & Datasets","heading":"2.2.1 The Study of Health in Pomerania (SHIP)","text":"reunification Germany, found life expectancy significantly lower East West¬†[4].\r\naddition, regional differences within new states, lowest life expectancy found Northeast¬†[4], [5].\r\ninvestigate causal relationship high mortality northeastern German population risk factors, Community Medicine Research Center Greifswald established Study Health Pomerania (SHIP)¬†[6], longitudinal epidemiological study two independent cohorts northeastern Germany.\r\nSHIP seeks describe broad spectrum health conditions rather focusing specific target disease¬†[6].\r\nparticular, major study objectives include investigations prevalence common diseases risk factors, correlation interaction risk factors diseases, progression subclinical manifest diseases, identification subgroups increased health risk, prediction concomitant diseases, well usage costs medical service.Cohort inclusion criteria age 20 79 years, main residency study region German nationality.\r\nParticipants SHIP underwent extensive, recurring (ca. every 5-6 years) examination program encompasses personal interviews, body measurements, exercise electrocardiogram, laboratory analysis, ultrasound examinations full body magnetic resonance tomography (MRT).\r\nBaseline examinations first cohort performed 1997 2001 (SHIP-0, N = 4308).\r\nFollowup examinations done 2002-2006 (SHIP-1, n = 3300), 2008-2012 (SHIP-2, n = 2333), 2014 - 2016 (SHIP-3, n = 1718) since 2019 (SHIP-4).\r\nFigure¬†2.1 illustrates participant response age distribution show-ups across study waves.\r\nBaseline information second, independent cohort (SHIP-Trend-0, N = 4420) collected 2008 2012 followup conducted 2016 2019.\r\nMajor strengths SHIP high level quality assurance, standardized examination protocols, high cohort representativeness.\r\nFigure 2.1: Participation response age distribution show-ups across study waves SHIP. Average population age shown black vertical line histograms.\r\nexamination program changed across waves.\r\nexample, magnetic resonance imaging (MRI) performed SHIP-2; liver ultrasound performed SHIP-0 SHIP-2 SHIP-1; dermatologic examinations performed SHIP-1 SHIP-2 SHIP-0.analyses, focus disorder hepatic steatosis, also known fatty liver, characterized high accumulation fat liver occurs approximately 30% adults¬†[6], [7].\r\nRisk factors include alcohol abuse, obesity, metabolic syndrome, diabetes, hyperlipidemia¬†[8].\r\nLiver biopsy considered diagnostic gold standard¬†[8] associated intermediate risk patient.\r\nNon-invasive diagnostic modalities include MRI, CT, ultrasound.\r\nhepatic steatosis usually asymptomatic, often goes undetected, can develop serious disease, steatohepatitis, cirrhosis, hepatocellular carcinoma, even liver failure.Methods developed applied SHIP dataset covered Chapters¬†3, 4 ¬†6.","code":""},{"path":"background.html","id":"background-data-diabf","chapter":"2 Medical Background & Datasets","heading":"2.2.2 The Diabetic Foot Clinical Trial (DiabF)","text":"Diabetic foot syndrome umbrella term foot-related problems diabetic patients.\r\none four diabetic patients develop foot ulcer lifetime¬†[9], many risk amputations next four years¬†[10].\r\n85% foot amputations due foot ulcers¬†[11], [12].\r\nrate foot amputations diabetic patients estimated 17-40 times higher general population¬†[14].\r\nDFS patients predisposed peripheral sensory neuropathy, results, example, patients unaware temperature feet pressure applied .\r\nAffected individuals may even injure without realizing .\r\nExcessive plantar pressures can exacerbate tissue destruction increase lifetime risk foot ulceration¬†[16].\r\nHowever, understanding pathomechanisms underlying tissue destruction absence trauma limited.university hospital Magdeburg, Germany, experimental study 31 healthy volunteers 30 diabetes patients diagnosed severe polyneuropathy conducted quantify pressure- posture-dependent changes plantar temperatures surrogate tissue perfusion.\r\npurpose, plantar pressure temperature changes feet recorded extended phases standing.\r\nCustom-made shoe insoles¬†[17] equipped eight temperature sensors eight pressure sensors preselected positions used data acquisition (2.2 ()).\r\ninsoles positioned closed protective shoes specifically developed diabetes patients.\r\nWithin shoes temperature increases time due exchange body temperature user also affected environmental temperature.\r\nclosely monitor -shoe temperature changes one sensor placed bottom insole without contact feet, denoted ‚Äúambient temperature sensor.‚Äù\r\nFigure 2.2: Positions pressure- temperature sensors insole () temporal, pressure-dependent temperature change. () Sensor positioning insole relation foot placement. (b) Termographic infrared images showing healthy subject seated position pressure applied feet () placement 20 kg weight thighs. measured temperature ranged 29¬∞C (blue) 34¬∞C (red). time-dependent temperature decrease observed predominantly forefoot region, visualized yellow color pressure application. rapid temperature increase noted within 1 min pressure relief. MTB: metatarsal bone. Images adapted ¬†[18].\r\nData collection began immediately shoes put .\r\nParticipants asked follow predefined sequence actions, .e., alternating standing (stance phase) sitting (pause).\r\nSessions consisted 6 standing episodes lasting 5, 10, 20, 5, 10, 20 minutes, respectively, separated sitting episodes lasting 5 minutes .\r\nParticipants instructed apply equal pressure feet standing.\r\nParticipants receive immediate feedback actual application pressure sessions, verbally encouraged study nurses maintain pressure standing without releasing .\r\nseated position, participants instructed release pressure 5 minutes maintaining contact insole.\r\nParticipants explicitly asked adhere instructions, .e., temporarily release pressure standing episode.\r\nstudy protocol included measurements performed twice, room temperature approximately 22¬∞C outdoors ambient temperature approximately 16¬∞C.\r\ntwo measurements performed two independent days.termographic images Figure¬†2.2 (b) visualize exemplary changes plantar temperature healthy subject sitting position pressure application (1), placing 20 kg weight front thigh (2-6), removing additional weight (7-8).\r\npressure application, gradual temporal decrease temperature noted predominantly forefoot.\r\npressure relief, rapid temperature increase observed within 1 min.Methods developed applied DiabF dataset covered Chapter¬†7.","code":""},{"path":"background.html","id":"background-data-aneurd","chapter":"2 Medical Background & Datasets","heading":"2.2.3 The Intracranial Aneurysm Angiography Image Dataset (AneurD)","text":"Intracranial aneurysms pathologic dilations intracranial vessel wall, often form dilation. \r\nbear risk rupture lead subarachnoidal hemorrhages often fatal consequences patient.\r\nSince treatment can also cause severe complications, extensive studies conducted assess patient-individual rupture risk based various parameters, including aneurysm symptomatology, size location, well patient age gender¬†[19].\r\nstudies identified parameters, aspect ratio, undulation index nonsphericity index statistically significant respect aneurysm rupture status¬†[20], [21].\r\nHowever, although studies allow retrospective analysis, clinician needs guidance case asymptomatic aneurysm (accidental finding) detected rupture risk determined.developed methods retrospective ‚ÄúIntracranial Aneurysm Angiography Image Dataset‚Äù (IAD) comprising 3D rotational angiography data 74 patients (age: 33-85 years, 17 male 57 female patients) university hospital Magdeburg, Germany, adding total 100 intracranial aneurysms.\r\nidentified two primary goals dataset: () build models can accurately predict rupture status based morphological parameters , (ii) assess importance parameters models optimal accuracy.Motivated results Baharoglu et al.¬†[22] found differences sidewall bifurcation aneurysms (cf.¬†Figure¬†2.3) terms relationship several morphological parameters rupture status, learn different models subset sidewall aneurysms (9 (37.5%) 24 ruptured) subset bifurcation aneurysms (29 (46.8%) 62 ruptured).\r\naddition, run experiments combined group (43 100 ruptures) containing 14 additional samples clearly determined either sidewall bifurcation aneurysms.\r\nFigure 2.3: Sidewall bifurcation aneurysm. Illustration sidewall aneurysm side parent vessel wall (left) bifurcation aneurysm vessel bifurcation (right). Image adapted ¬†[23].\r\nMethods developed applied AneurD dataset covered Chapter¬†8.","code":""},{"path":"background.html","id":"background-data-cha","chapter":"2 Medical Background & Datasets","heading":"2.2.4 The Charit√© Tinnitus Patients Observational Therapy Study Dataset (CHA)","text":"Tinnitus perception phantom sound absence external sound source.\r\ncomplex, multifactorial caused maintained phenomenon estimated affect 10% 15% adult population¬†[24].\r\nassociated annual economic burden 19.4 billion USD \r\nUnited States¬†[25] 6.8 billion EUR Netherlands alone¬†[26].\r\nClinical evaluation tinnitus challenging due patient heterogeneity tinnitus perception (laterality, pitch, noise characteristics, frequency, duration, chronicity), risk factors (including hearing loss, temporomandibular joint disorder, aging), comorbidities (including hyperacusis, depression, sleep disorders), perceived distress, treatment response¬†[27].\r\ndifferences complicate identification appropriate effective treatment modality.\r\nCurrently, treatment gold standard: sound therapy (masking), informational counseling (minimal contact education), cognitive behavioral therapy, tinnitus retraining shown effective patients, also evidence patients benefit equally forms treatment¬†[28]‚Äì[32].\r\nDue heterogeneous nature tinnitus symptom, well unclear evidence base regarding treatment management, identification patient subgroups critical stratify individual pathophysiology treatment pathways¬†[33]‚Äì[35].‚ÄúCharit√© tinnitus patients observational therapy study dataset‚Äù (CHA) includes self-report data 4,103 tinnitus patients treated Tinnitus Center Charit√© Universit√§tsmedizin Berlin, Germany, January 2011 October 2015.\r\npatients 18 years age older suffered tinnitus least 3 months.\r\nExclusion criteria presence acute psychotic illness addiction, deafness, insufficient knowledge German language.\r\nTreatment included 7-day multimodal program intensive daily informational counseling, detailed ear-nose-throat well psychological diagnosis, cognitive behavioral therapy interventions, hearing exercises, progressive muscle relaxation, physical therapy.\r\nbaseline (T0; start therapy) treatment (T1), patients asked complete several self-report questionnaires.\r\nquestionnaires selected obtain comprehensive assessment tinnitus, including tinnitus-related distress psychosomatic background tinnitus anxiety, depression, general quality life, experienced physical impairments.Table¬†2.1 provides overview questionnaires used analyses.\r\nquestionnaires contain multiple-choice items answers Likert scale. example, ‚ÄúTinnitus Questionnaire‚Äù¬†[38] (TQ) contains 52 statements, ‚Äúunable enjoy listening music noises.‚Äù respondents can give 3 possible answers: ‚Äútrue‚Äù (coded 0), ‚Äúpartly true‚Äù (1), ‚Äútrue‚Äù (2).\r\nquestionnaires also include aggregate variables called ‚Äúsubscales‚Äù ‚Äútotal scores.‚Äù\r\nexample, TQ total score (TQ_distress) calculated sum 40 item values, 2 items used twice¬†[38] resulting range values 0 84, higher values representing higher tinnitus-related distress.\r\ncutoff value 46¬†[38] used distinguish compensated (0-46) decompensated (47-84) tinnitus.\r\nFurthermore, average time answer item recorded questionnaire.\r\nFigure 2.4 provides graphical representation demographic data 3,803 (92.7%) patients complete data sociodemographics questionnaire¬†[41] (SOZK) TQ score.Table 2.1: Description questionnaires form basis CHA. |F|: total number items, subscales total scales.\r\nFigure 2.4: Patient demographics (CHA). Overview patient demographics degree tinnitus distress measured therapy commencement.\r\nMethods developed applied CHA dataset covered Chapters¬†5, 8 ¬†9.","code":""},{"path":"imm.html","id":"imm","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3 Interactive Discovery and Inspection of Subpopulations","text":"chapter partly based :Uli Niemann, Henry V√∂lzke, Jens-Peter K√ºhn, Myra Spiliopoulou. ‚ÄúLearning inspecting classification rules longitudinal epidemiological data identify predictive features hepatic steatosis.‚Äù : Expert Systems Applications 41.11 (2014), pp.¬†5405-5415. DOI: 10.1016/j.eswa.2014.02.040.Uli Niemann, Myra Spiliopoulou, Henry V√∂lzke, Jens-Peter K√ºhn. ‚ÄúInteractive Medical Miner: Interactively exploring subpopulations epidemiological datasets.‚Äù : ECML PKDD 2014, Part III, LNCS 8726. Springer, 2014, pp.¬†460-463. DOI: 10.1007/978-3-662-44845-8_35.chapter organized follows.\r\nSection¬†3.1, provide motivation classification interactive subpopulation discovery epidemiological cohort studies.\r\npresent workflow interactive assistant Section¬†3.2.\r\nSection¬†3.3, report results important findings.\r\nchapter closes summary discussion main contributions Section¬†3.4.","code":""},{"path":"imm.html","id":"brief-chapter-summary","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"Brief Chapter Summary","text":"Analysis population-based cohort data mostly hypothesis-driven.\r\npresent workflow interactive application data-driven analysis population-based cohort data using hepatic steatosis example.\r\nmining workflow includes steps () identify subpopulations different distributions respect target variable, (ii) classify subpopulation taking class imbalance account, (iii) identify variables associated outcome.\r\nshow workflow suited () identify subpopulations classification reduce class imbalance, (b) drill-derived models identify important variables subpopulations worthy investigation.","code":""},{"path":"imm.html","id":"imm-intro","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1 Motivation and Comparison to Related Work","text":"Medical decisions diagnosis treatment multifactorial conditions diseases disorders based clinical epidemiological studies; latter contain information participants without disease allow learning discriminatory models , longitudinal designs, understanding disease progression.\r\nexample, several studies identified risk factors (obesity alcohol consumption) comorbidities (cardiovascular disease) associated hepatic steatosis (‚Äúfatty liver‚Äù)¬†[67]‚Äì[71].\r\nHowever, studies identified risk factors associated outcomes relate entire population.\r\nwork arose need identify factors outcomes subpopulations promote personalized diagnosis treatment, expected personalized medicine¬†[72], [73].Classification subpopulations studied Zhanga Kodell¬†[74], pointed classifier performance whole dataset may low entire population heterogeneous.\r\nTherefore, first trained ensemble classifiers used predictions ensemble member create new feature space.\r\nperformed hierarchical clustering partition instances three subpopulations: one prediction accuracy high, one intermediate range, one low.\r\nUsing approach, Zhanga Kodell partition original data set subpopulations easy hard classify.\r\nmethod seems appealing general, appears inappropriate three-class problem SHIP data, highly skewed distribution, clear low classification accuracy caused (part) class imbalance.\r\nTherefore, exploratively examined data set classification identify less skewed subpopulations, exploratively classification identify ‚Äì within subpopulation ‚Äì variables strongly associated outcome.Pinheiro et al.¬†performed association rule discovery patients liver cancer¬†[75].\r\nauthors pointed early detection liver cancer can help reduce five-year mortality rate, early detection difficult patient often notice symptoms early stages liver cancer¬†[75].\r\nPinheiro et al.¬†used association rule algorithm FP-growth¬†[76] discover high-confidence association rules high-confidence classification rules related mortality liver cancer patients.\r\nalso considered association rules promising medical data analysis easy compute produce results understandable humans.\r\nTherefore, also used association rules basic method, epidemiological data classification rather mortality prediction.\r\nuse association rules classification, specified consequence rule target variable.Zhang et al.¬†[77] addressed increasing technical challenges medical expert-driven subpopulation discovery due increasingly large complex medical data, often including information hundreds variables thousands patients form tables, images, text.\r\npast sufficient physician basic knowledge statistics spreadsheet software Microsoft Excel analyze small table patient data, today effective efficient approaches managing, analyzing, summarizing large medical data required.\r\nresult, domain experts typically rely technical experts help perform tasks.\r\nHowever, back--forth often slow, tedious, expensive.\r\nTherefore, better provide domain expert technical tool allows quickly perform exploratory analysis .\r\nZhang et al.¬†[77] presented CAVA, system includes various subgroup visualizations (called ‚Äúviews‚Äù) analytical components (called ‚Äúanalytics‚Äù) subgroup comparison.\r\nmain panel Figure¬†3.1 shows one views: flowchart¬†[78] patient subgroups sequence symptoms.\r\nuser can obtain additional summaries interacting visualization, example, dragging dropping one boxes flowchart onto one entries analysis panel.\r\naddition, user can expand selected cohort tool search patients strictly meet current inclusion criteria, somewhat similar current patient subpopulation interest¬†[79].\r\nFigure 3.1: CAVA‚Äôs graphical user interface. flowchart visualizes subgroups cardiac patients organized common occurrence symptoms. Arc color represents hospitalization risk. user can switch graphical representations data processing methods dragging dropping. upper right field contains detailed information currently selected patients. lower right panel contains provenance graph allows user undo operations revisit previous interaction steps. :¬†[77].\r\nKrause et al.¬†[80] argued model selection based global performance metrics accuracy, statistics contribute better understanding model‚Äôs reasoning.\r\nMoreover, complex highly accurate model automatically guarantee actionable insights.\r\nKrause et al.¬†propose Prospector¬†[80], system provides diagnostic components complex classification models based concepts Partial Dependence (PD) plots¬†[85].\r\nPD plots popular tool visualizing marginal effect feature predicted probability outcome.\r\nBriefly, point PD curve represents average prediction model observations, assuming observations fixed value feature interest.\r\nfeature whose PD curve high range variability considered influential model prediction feature flat PD curve.\r\nClosely related PD plots individual conditional expectation (ICE) plots,¬†[86] display curve observation, helping reveal contrasting subpopulations might ‚Äúaverage ‚Äù PD plot.\r\nProspector combines PD ICE curves show relationship feature model prediction (global) model level (local) patient-individual level.\r\naddition, custom color bar provided compact alternative ICE curves (Figure¬†3.2) ().\r\nstacked bar graph shows distribution predicted risk scores study group (Figure¬†3.2 (b)), user can click specific decile obtain list individual patients exact predicted score label.\r\nway, patients whose prediction scores close decision threshold can investigated.\r\ncharacteristic, authors calculate ‚Äúimpactful feature change‚Äù: given patient‚Äôs current characteristic value, identify near-counterfactual value leads large change predicted risk score minimizing difference original feature value maximizing predicted risk score.\r\ntop 5 -called ‚Äúsuggested changes‚Äù displayed ‚Äì separately increasing decreasing disease risk - table (cf.¬†Figure¬†3.2 (c)) integrated interactive elements IC color bars (cf.¬†Figure¬†3.2 (d)).\r\n\r\nFigure 3.2: Selected model diagnostics Prospector. () upper plot shows two curves characteristic ‚Äúage‚Äù: gray partial dependence (PD) curve represents marginal prediction model patients, black individual conditional expectation (ICE) curve illustrates effect counterfactual ages predicted risk diabetes example patient. histogram shows age distribution. color bar compact representation ICE curve ; circled value represents selected patient‚Äôs feature value. (b) Stacked bars show distribution predicted risk scores study group. Clicking one bars opens table showing ID, predicted risk, true label patients belonging selected decile predicted risk. (c) Summary table ‚Äúimpactful feature changes‚Äù decreasing (upper group) increasing (lower group) predicted risk: row shows true feature value ‚Äúproposed change,‚Äù .e., similar counterfactual value resulted significant change predicted risk. (d) Multiple PD color bars augmented proposed changes (labels outlined white). Adapted ¬†[80].\r\nPahins et al.¬†[87] presented COVIZ, system cohort construction large spatiotemporal datasets.\r\nCOVIZ includes mechanisms exploratory data analysis treatment pathways event trajectories, visual cohort comparison, visual querying.\r\nOne design goals COVIZ fast, e.g., using efficient data structures Quantile Data Structure¬†[88] ensure low latency computational operations thus suitability large data sets.\r\n\r\nBernard et al.¬†[89] proposed system cohort construction temporal prostate cancer cohort data included visualizations subgroups individual patients.\r\nguide users exploration, visual markers indicate interesting relationships attributes derived statistical tests.\r\nRecently, Corvo et al.¬†[90] presented comprehensive visual analytics system pathological high-throughput data encompasses major steps typical data analysis pipeline, preprocessing raw histopathology images interactive segmentation, components exploratory data analysis interactive cohort construction high-dimensional feature space, feature engineering includes extraction potentially predictive biomarker features, modeling, well visualization summarization modeling results.\r\nPreim et al.¬†provided comprehensive reviews visual analytics methods application public health¬†[91] epidemiology¬†[92] particular.Previous Work SHIP.\r\nKlemm et al.¬†[93] presented ‚Äú3D Regression Cube,‚Äù system allows interactive exploration feature correlations epidemiological datasets.\r\nsystem generates large number multiple regression models different combinations one dependent three independent variables displays goodness fit three-dimensional heat map.\r\nsystem allows user modify regression equation, example, changing number independent variables, specifying wild cards, interaction terms, fixing one variables reduce computational complexity focus specifically variable interest.\r\napproach also able identify variables highly associated outcome, search subpopulation-specific relationships rather generating global model entire dataset, additionally provide predictive value ranges.\r\nKlemm et al.¬†[96] presented system combines visual representations non-image image data.\r\nidentify clusters back pain patients SHIP data.\r\nSince specified hepatic steatosis outcome, chose rather build supervised models classification rules directly capture relationships predictive variables outcome.\r\n\r\nAlemzadeh et al.¬†[97] presented S-ADVIsED, system interactive exploration subspace clusters incorporates various visualization types donut diagrams, correlation heatmaps, scatterplot matrices, mosaic diagrams, error bar graphs.\r\nS-ADVIsED requires user input mining results obtained advance outside system, tool enables expert-driven interactive subpopulation discovery instead expert-driven interactive result exploration.\r\n\r\nHielscher et al.¬†[100] developed semi-supervised constrained-based subspace clustering algorithm find diverse sets interesting feature subsets using SHIP data.\r\nguide search interesting feature subsets, expert can provide domain knowledge form small number instance-level constraints, thus forcing pairs instances (.e., study participants) assigned either different cluster.\r\nHielscher et al.¬†[101] extended work introduced mechanism validate subpopulations independent cohorts.\r\nPreim et al.¬†[102] provided overview aforementioned research developed data mining visual analytics methods gain insights SHIP data.","code":""},{"path":"imm.html","id":"imm-workflow","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2 Subpopulation Discovery Workflow and Interactive Mining Assistant","text":"section, present subpopulation discovery workflow.\r\ndataset used population partitioning class separation target variable hepatic steatosis come Study Health Pomerania (SHIP) described Section¬†2.2.1.\r\nSection¬†3.2.1, explain origin availability target variable.\r\nSection¬†3.2.2, motivation partitioning data partitioning steps presented.\r\n, used classification methods whole dataset partitions discussed Section¬†3.2.3.\r\nSection¬†3.2.4 introduce underpinnings classification rule discovery, followed description primarily used HotSpot¬†[103] algorithm Section¬†3.2.5.\r\nFinally, present interactive mining assistant Section¬†3.2.6.","code":""},{"path":"imm.html","id":"imm-workflow-target","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.1 Target variable","text":"outcome variable derived participants‚Äô liver fat concentration calculated magnetic resonance imaging (MRI).\r\ntime writing original manuscript, MRI results available 578 SHIP-2 participants.\r\nuse data participants classifier learning, Interactive Medical Miner also contrasts data data remaining 1755 participants MRI scans made available.Participants liver fat concentration 10% less assigned class (‚Äúnegative‚Äù class, .e., absence disorder), values greater 10% less 25% assigned class B (increased liver fat/fatty liver tendency), values greater 25% assigned class C (high liver fat).\r\nconsider classes B C ‚Äúpositive.‚Äù\r\ncut-value 10% intentionally higher value 5% proposed Kuhn et al.¬†[104] separate subjects without hepatic steatosis, primary interest medical perspective identify important variables subjects likely ill.\r\nSelecting high cut-value exacerbates class imbalance makes data analysis difficult.\r\nFigure¬†3.3 depicts class distribution stratified gender.\r\n578 participants, 438 belong class (approximately 76%), 108 B (approximately 19%), 32 C (approximately 6%).\r\nMen likely elevated high liver fat concentration women (30.7% vs.¬†18.8% classes B C).\r\nFigure 3.3: Gender-specific distribution target variable. relative sizes rectangles indicate number female male participants classes.\r\naddition target variable, data set contains 66 variables extracted participants‚Äô questionnaire responses medical tests (cf.¬†[105]).\r\nvariables sociodemographics (gender, age, etc.),\r\nvariables consumption behavior (e.g., alcohol cigarettes), SNPs (genetic information), variables laboratory data (e.g., serum concentrations), two variables liver ultrasound results ‚Äì stea_s2 stea_alt75_s2.\r\nvariables take symbolic values reflecting probability participant fatty liver; latter combination former participant‚Äôs ALAT intake; details ¬†[105].\r\nAlmost variables mentioned suffix _s2 indicating SHIP-2 follow-measurements, contrast SHIP-0 (_s0) SHIP-1 (_s1).\r\nExceptions gender, highest school degree, 10 SNP variables.","code":""},{"path":"imm.html","id":"imm-workflow-partitioning","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.2 Partitioning the Dataset into Subpopulations","text":"dataset imbalanced respect gender (314 females, 264 males), decided partition dataset classification.\r\nFirst, examined class distributions two partitions respect gender.\r\nobserved distributions different, especially respect class B (see Figure¬†3.3.\r\nSecond, examined class distribution sex age, whereupon found age associated subpopulation female participants, subpopulation male participants.\r\nThird, identified cut-point age introducing heuristic identifies age value minimizes standard deviation respect target variable.\r\nperformed supervised learning separately partitions female male participants, referred partitionF partitionM hereafter.\r\nalso created additional learner subpopulation older female participants aged cut-point 52 (Partition F:age>52).understand age affects class distribution, introduced heuristic determines cutoff age value partitionF splits two bins, standard deviations liver fat concentration bin minimized.\r\nLet \\(splitAge\\) denote cutoff value \\(X_y=\\{x\\\\mathtt{PartitionF}|\\text{age } x \\leq splitAge\\}\\), \\(X_z=\\{x\\\\mathtt{PartitionF}|\\text{age } x > splitAge\\}\\) denote bins.\r\n, let \\(n\\) cardinality \\(X_y\\cup{}X_z\\) .e.¬†PartitionF.\r\n, define Sum Weighted Standard Deviations (\\(SWSD\\)) \\[\\begin{equation}\r\nSwSD\\left(X_y,X_z\\right) = \\frac{|X_y|}{n}\\sigma({X_y})+\\frac{|X_z|}{n}\\sigma({X_z})\r\n\\tag{3.1}\r\n\\end{equation}\\]\\(|X_i|\\) cardinality \\(X_i\\) \\(\\sigma(X_i)\\) standard deviation original liver fat values.\r\nheuristic selects \\(\\mathsf{splitAge}\\) \\(SwSD\\) minimal.\r\nPartitionF, minimum value 7.44 age 52,\r\n.e.¬†close onset menopause.\r\nFigure 3.4: Distribution liver fat concentration partition. Distribution liver fat concentration male participants (PartitionM), female younger older 52 years. horizontal axis shows liver fat concentration bins 5%, vertical axis shows number participants bin.\r\nhistograms Figure¬†3.4 depict differences liver fat concentration distributions age cutoff value 52.\r\nNext PartitionM (n=264), show subpartitions \\(\\mathsf{F:age\\leq{}52}\\) (n=131) F:age>52 (n=183) PartitionF.\r\nfemale participants \\(\\mathsf{F:age\\leq{}52}\\) 5% liver fat concentration ca. 95% 10%, .e., belong negative class .\r\ncontrast, ca. 28% participants F:age>52 liver fat concentration 10%; belong positive classes B C.","code":""},{"path":"imm.html","id":"imm-workflow-classification","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.3 Classification","text":"classification cohort participants, focused algorithms provided interpretable models, wanted identify predictive conditions, .e., variables values/ranges models.\r\nTherefore, considered decision trees, classification rules, regression trees.\r\nused J4.8 decision tree classification algorithm (equivalent C4.5 algorithm¬†[106]) Waikato Environment Knowledge Analysis (Weka) Workbench¬†[107].\r\nalgorithm builds tree successively partitioning node (subset dataset) variable maximizes information gain within node.\r\noriginal algorithm works variables take categorical values, creating one child node per value.\r\nHowever, implementation Weka library also provides option forces algorithm always create exactly two child nodes: one best separating value one values.\r\nused option experiments yields better quality trees.\r\naddition, Weka algorithm also supports variables take numeric values:\r\nnode split two child nodes partitioning range values variable two intervals.deal skewed distribution, considered following classification variants:Naive: problem imbalanced data ignored.InfoGain: keep top-30 66 variables, sorting variables information gain towards target variable.Oversampling: use SMOTE¬†[108] resample dataset minority-oversampling: class B, 100% new instances generated, class C 300% new instances generated, resulting following distribution :438, B:216, C:128.CostMatrix: preferred misclassify negative case rather detecting positive case, penalized false negatives (FN) false positives (FP).\r\nused cost matrix depicted Table¬†3.1.\r\nTable 3.1: Cost matrix. Cost matrix penalize misclassification class imbalance.\r\n","code":""},{"path":"imm.html","id":"imm-workflow-rule-discovery","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.4 Classification Rule Discovery","text":"Classification rules can reveal interesting relationships one features outcome¬†[109], [110].\r\nCompared model families deep neural networks, support vector machines random forests, classification rules usually achieve lower accuracy.\r\nHowever, easier interpret infer therefore suitable interactive discovery subpopulations.\r\nepidemiological research, interesting subpopulations subsequently used formulate validate small set hypotheses simply investigate associations risk factors particular outcome.\r\nsubpopulation interest formulated follows, ‚Äúsample study, prevalence goiter 32%, whereas probability subpopulation described thyroid-stimulating hormone less equal 1.63 mU/l body mass index greater 32.5 kg/m2 49%.‚ÄùClassification rule algorithms induce descriptions interesting subpopulations data interestingness quantified quality function.\r\nclassification rule association rule whose consequent fixed specific class value.\r\nConsider exemplary classification rule \\(r_1\\):\r\n\\[\\begin{equation}\r\nr_1: \\underbrace{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}_{\\text{Antecedent}} \\longrightarrow \\underbrace{\\vphantom{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}hepatic\\_steatosis = pos}_{\\text{Consequent}}\r\n\\tag{3.2}\r\n\\end{equation}\\]Classification rules expressed form \\(r: \\text{antecedent} \\longrightarrow T=v\\).\r\nconjunction conditions (.e.¬†feature - feature value pairs) left arrow constitutes rule‚Äôs \\(\\text{antecedent}\\) (left hand site).\r\n\\(\\text{consequent}\\) (right hand side), \\(v\\) requested value target variable \\(T\\).define \\(s(r)\\) subpopulation cover set \\(r\\), .e.¬†set instances satisfy antecedent \\(r\\).\r\ncoverage \\(r\\), fraction instances covered \\(r\\), defined \\(Cov(r)=|s(r)|/N\\), \\(N\\) total number instances.\r\nsupport \\(r\\) quantifies percentage instances covered \\(r\\) additionally \\(T=v\\), calculated \\(Sup(r)=|s(r)_{T=v}|/N\\).\r\nconfidence \\(r\\) (also referred precision accuracy) defined \\(Conf(r)= |s(r)_{T=v}|/|s(r)|\\) expresses relative frequency instances satisfying complete rule (.e., antecedent consequent) among satisfying antecedent.\r\nrecall sensitivity \\(r\\) respect \\(T=v\\) defined \\(Recall(r)=Sensitivity(r)=\\frac{|s(r)_{T=v}|}{n_{T=v}}\\).\r\nWeighted Relative Accuracy rule interestingness measure balances coverage confidence gain often used internal quality criterion candidate generation¬†[110].\r\ndefined \\[\\begin{equation}\r\nWRA(r) = Cov(r)\\cdot \\left(Conf(r)-\\frac{n_{T=v}}{N} \\right).\r\n\\tag{3.3}\r\n\\end{equation}\\]odds ratio \\(r\\) respect \\(T=v\\) defined \r\n\\[\\begin{equation}\r\n(r) = \\frac{\\frac{ |s(r)_{T=v}| }{|s(r)_{T\\neq v}|} }{ \\frac{n_{T=v} -  |s(r)_{T=v}| }{ n_{T\\neq v} -  |s(r)_{T\\neq v}|} }.\r\n\\tag{3.4}\r\n\\end{equation}\\]example, Figure¬†3.5 illustrates exemplary rule \\(r_2\\) dataset 10 instances binary target, circles cyan color represent instances negative class red circles positive instances.\r\ncover set \\(r_2\\) contains instances 7, 8, 9 10, hence \\(Cov(r_2)\\) = 0.40.\r\n, \\(Sup(r_2)\\) = 0.30, \\(Conf(r_2)\\) = 0.75, \\(WRA(r_2)\\) = 0.4 \\(\\cdot\\) (0.75 - 0.4) = 0.14 \\((r_2)\\) = (3/1) / (1/5) = 15.\r\nFigure 3.5: Exemplary classification rule.\r\n","code":""},{"path":"imm.html","id":"imm-workflow-hotspot","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.5 HotSpot","text":"classification rule discovery, use HotSpot¬†[103] algorithm provided Waikato Environment Knowledge Analyis (WEKA) Workbench¬†[107].\r\nHotSpot beam-width search algorithm implements general specific approach rule extraction.\r\nsingle rule constructed successively adding condition antecedent locally maximizes confidence.\r\nUnlike general hill-climbing, considers best rule candidate iteration, HotSpot‚Äôs beam search retains b highest-ranked candidates refines later steps.\r\nConsequently, HotSpot reduces ‚Äúmyopia‚Äù¬†[109] Hill-Climbing search typically suffers.\r\nBriefly, hill-climbing approaches consider locally optimal candidate iteration.\r\nresult, globally optimal rule found locally optimal iteration.\r\napplication perspective, also desirable generate one rule, since alternative descriptions subpopulations can facilitate hypothesis generation.\r\nbeam width can specified maximum branching factor, .e., maximum number conditions may added candidate rule.\r\niteration, rule candidates must satisfy minimum value count, sensitivity threshold.\r\navoid adding condition leads marginal improvement confidence, parameter minimum improvement, .e.¬†minimum relative improvement confidence adding another condition, can specified.\r\ncomputational complexity rule search can reduced specifying maximum rule length, .e.¬†number conditions antecedent.\r\nexperiments set parameters follows: maximum branching factor = 20, maximum value count = 1/3, minimum improvement = 0.1, maximum rule length = 3.","code":""},{"path":"imm.html","id":"imm-workflow-imm","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.6 Interactive Medical Miner","text":"Classification rules can provide valuable insights potentially prevalent conditions different subpopulations cohort study.\r\nHowever, number rules created large, usually case large epidemiological data, conditions rules overlap conditions present classes target characteristic.\r\nTherefore, medical expert needs inspection tools decide rules informative features investigated .\r\nInteractive Medical Miner (IMM) allows expert () discover classification rules, inspect frequency rules (b) class (c) unlabeled subset cohort, (d) examine statistics rule values selected variables.\r\ndescribe functionalities , referring screenshot Figure¬†3.6.\r\nFigure 3.6: User interface Interactive Medical Miner. Classification rules discovered class B shown bottom left panel. selected rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, distribution participants covered rule among three classes shown absolute values (top middle panel) histogram (bottom right panel) respect age (top right panel).\r\nuser interface consists six panels.\r\n‚ÄúSettings‚Äù panel (top left), medical expert can set parameters rule induction pressing ‚ÄúBuild Rules‚Äù button.\r\npanel, discovered rules displayed.\r\n‚ÄúSorting preference‚Äù panel, expert can specify whether rules sorted confidence, coverage, rather alphabetically better overview overlapping rules.rule generation, user can specify sub-cohort dataset.\r\nclicking button Select Subpopulation, popup window appears, multiple filter queries form <variable> <operator> <value> can added, e.g.¬†som_bmi_s2 >= 30.\r\ndefined constraints displayed table can undone.\r\nFurthermore, user can select variables model creation, e.g.¬†exclude variable known highly correlated another variable already considered model learning.Mining criteria include dataset (choose whole dataset one partitions), class rules generated (drop-list ‚ÄúClass‚Äù) constraints related class, .e.¬†‚ÄúMinimum number values‚Äù (can also specified relative number), ‚ÄúMaximum rule length,‚Äù ‚ÄúMaximum branching factor‚Äù ‚ÄúMinimum improvement.‚Äù\r\nexample parameters affect rule search, consider selected rule Figure¬†3.6, som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, coverage 0.12 confidence 0.56.\r\nsensitivity 38/108 = 0.352 satisfies minimum value count threshold 0.33.\r\nApriori property, evident two conditions antecedent rule, namely som_huef_s2 > 109 cre_u_s2 > 5.38, must also exceed threshold.\r\nposition condition within antecedent indicates refinement step added rule candidate.\r\nexample, first condition som_huef_s2 > 109 confidence 44/107 = 0.41 extended second condition crea_u_s2 > 5.38 confidence gain exceeds minimum improvement threshold, .e., 38/68 - 44/107 = 0.15 > 0.05.\r\nHowever, rule extended maximum rule length set 2.\r\nmaximum branching factor conservatively set 1000 prevent potentially interesting rules generated due small beam width.\r\nparameter can lowered interactively number rules found high rule induction takes long.output list execution run (area ‚ÄúSettings‚Äù) scrollable interactive.\r\nexpert clicks rule, upper middle area ‚ÄúSummary Statistics‚Äù updated.\r\nfirst row shows distribution cohort participants across classes entire dataset, second row shows participants covered rule (column ‚ÄúTotal‚Äù second row) distributed across classes.\r\nThus, expert can specify discovery classification rules one classes examine often antecedent rule occurs among participants classes.\r\nexample, rule covers participants selected class (class B Figure¬†3.6) necessarily interesting also covers high number participants classes.\r\nrule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B covers total 68 participants, 38 class B.\r\nreduce number covered participants classes, .e., increase confidence, user can decrease number minimum values allow generation rules lower sensitivity homogeneity respect selected class.data may incomplete.\r\nexample, participants cohort underwent liver MRI.\r\nTherefore, also interest know distribution unlabeled participants support antecedent given rule.\r\npurpose, ‚ÄúHistogram‚Äù panel can used: expert selects another feature interactive ‚ÄúVariable selection‚Äù area upper right panel can see values variable distributed among study participants ‚Äì labeled unlabeled; latter marked ‚ÄúMissing‚Äù color legend.\r\nplotting histograms, use free Java chart library JFreeChart¬†[111].\r\nNumerical variables discretized using ‚ÄúScott‚Äôs rule‚Äù¬†[112] follows:\r\nlet \\(X_{s(r)}\\) set values numeric variable \\(X\\) respect cover set \\(s(r)\\).\r\nbin width \\(h\\) calculated \\(h(X_{s(r)})=\\frac{\\max{X_{s(r)}}-\\min{X_{s(r)}}}{3.49\\sigma_{s(r)}}\\cdot |s(r)|^{\\frac{1}{3}}\\).expert select variable, target variable used default distribution labeled participants visible.\r\nhistogram Figure¬†3.6 shows age distribution labeled unlabeled participants covered example rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B.\r\ndistribution values among labeled participants indicates age may risk factor indicated subpopulation, probability class B increases age.\r\nvisual finding suggests adding condition age_ship_s2 > 56.8 antecedent rule.\r\nIndeed, confidence specific rule increases 38/68 = 0.56 27/40 = 0.675.\r\nHowever, sensitivity decreases 38/108 = 0.352 27/108 = 0.250, threshold minimum value count longer met.\r\nThus, visualizing participant statistics selected rules can provide clues subpopulations monitored closely clues modify algorithm parameters subsequent runs, example, decrease minimum value count 0.25 increase maximum rule length 3.","code":""},{"path":"imm.html","id":"imm-experiments","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3 Experiments and Findings","text":"","code":""},{"path":"imm.html","id":"imm-experiments-trees","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.1 Results of Decision Tree Classifiers","text":"evaluation decision tree classifiers, consider accuracy, .e., ratio correctly classified participants, sensitivity specificity, \\(F_1\\) score, .e., harmonic mean precision recall.\r\nspecificity, precision recall, consider two classes B C together positive class.Oversampling achieved best performance accuracy 80% \\(F_1\\) score 62%.\r\nbest decision trees found partition F:age>52, followed partitionF, partitionM.\r\nlarge discrepancy accuracy \\(F_1\\) scores also appears models partitions, suggesting accuracy scores unreliable skewed distribution. Therefore, report accuracy .partition F:age>52, overall best decision tree achieved oversampling variant.\r\nlarger PartitionF, best performance achieved decision tree created InfoGain variant, best decision tree PartitionM created CostMatrix variant.\r\nsensitivity specificity values trees given Table¬†3.2, trees shown Figures¬†3.8 - 3.9 discussed Section¬†3.3.3.\r\nTable 3.2: Best decision trees three partitions. Best separation achieved F:age>52; PartitionM heterogeneous one, performance values lowest.\r\nTable¬†3.2 indicates decision tree variants perform differently different partitions.\r\nOversampling beneficial F:age>52, partially compensates class imbalance problem.\r\nPartitionM heterogeneous class distribution partitions, variants perform relatively poor .\r\nHence, expected insights decision trees F:age>52 PartitionF, better separation achieved.\r\nFigure 3.7: Best decision tree PartitionF, achieved variant InfoGain.\r\n\r\nFigure 3.8: Best decision tree F:age>52, achieved variant Oversampling.\r\n\r\nFigure 3.9: Best decision tree PartitionM, achieved variant CostMatrix.\r\n","code":""},{"path":"imm.html","id":"imm-experiments-rules","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.2 Discovered Classification Rules","text":"classification rules found HotSpot whole dataset conclusive class positive classes B C, omit reporting rules useful diagnostic purposes.\r\nclassification rules found partitions informative.\r\nHowever, classification rules one feature antecedent low confidence.\r\nensure high confidence, restricted output rules least two features antecedent.\r\nensure still high coverage, allowed three features.\r\nselection high confidence high coverage rules partition class shown Tables¬†3.3 - 3.5, respectively.\r\ndescribe important features antecedent rules next subsection, together important features best decision trees.\r\nTable 3.3: Classification rules (PartitionF). Best HotSpot classification rules (maxLength = 3) PartitionF (excerpt).\r\n\r\nTable 3.4: Classification rules F:age>52. Best HotSpot classification rules (maxLength = 3) F:age>52 (excerpt).\r\n\r\nTable 3.5: Classification rules (PartitionM). Best HotSpot classification rules (maxLength = 3) PartitionM (excerpt).\r\n","code":""},{"path":"imm.html","id":"imm-experiments-important-features","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.3 Important Features for Each Subpopulation","text":"important features decision trees Figures¬†3.8 - 3.9 closer root.\r\nreadability, tree nodes figures contain short descriptions instead original variable names.\r\nthree decision trees, root node ultrasound diagnosis variable stea_s2.\r\nnegative ultrasound diagnosis points negative class , positive ultrasound diagnosis directly lead one positive classes B C.\r\ndecision trees three partitions differ nodes placed near root.Important features PartitionF. best decision tree PartitionF (cf.¬†Figure¬†3.7) can observed ultrasound report positive HbA1C concentration 6.8%, class C.\r\nclassification rules high coverage confidence Table¬†3.3) point interesting features:\r\nwaist circumference 80 cm, BMI 24.82 kg/m2, hip circumference 97.8 cm less characterize participants negative class.\r\n6 participants serum glucose concentration greater 7 mmol/l TSH concentration greater 0.996 mu/l belong class C.\r\n, severe obesity (BMI value 38.42 kg/m2 points class C high confidence ‚Äì combination variables.Important features F:age>52 contrast best tree PartitionF, best decision tree subpartition F:age>52 (cf.¬†Figure¬†3.8) also contains nodes SNPs, indicating potentially genetic associations fatty liver participants.\r\nClassification rules high coverage confidence class B also contain SNPs, can seen Table¬†3.4.\r\nSimilarly PartitionF, high BMI values point positive class combined features: Table¬†3.4, see four participants stea_alt75_s2 = 3 (.e.¬†positive ultrasound diagnosis combined critical ALAT value) BMI larger 38.42 kg/m2 belong class C.\r\nsimilar association holds stea_alt75_s2 = 3 combined high waist circumference (> 124 cm).\r\n19 20 participants class B positive ultrasound diagnosis, genetic marker gx_rs11597390 = 1 serum HDL concentration 1.53 mmol/l.Important features PartitionM. role ultrasound report predicting negative class PartitionM (cf.¬†Figure¬†3.9 PartitionF).\r\nbest tree F:age>52, best tree PartitionM contains nodes SNPs serum GGT value ranges.\r\nfeatures also antecedent top Hotspot rules (cf.¬†Table¬†3.5): Serum GGT concentration 1.9,\\(\\mu\\)mol/sl combination creatinine concentration 90 mmol/l thromboplastin time ratio (quick_s2) 59% point class C.\r\nSimilarly, positive ultrasound diagnosis serum HDL concentration exceeding 0.84 mmol/l point class C.decision trees classification rules provide insights features appear diagnostically important.\r\nHowever, medical expert needs additional information decide whether feature worth investigation.\r\nparticular, decision trees highlight importance feature context subtree found; subtree describes subpopulation typically small.\r\ncontrast, classification rules provide information larger subpopulations.\r\nHowever, subpopulations may overlap; example, first four rules class C PartitionM (cf.¬†Table¬†3.5) may refer 6 participants.\r\nFurthermore, unless classification rule confidence close 100%, may participants classes also support .\r\nTherefore, decide whether features antecedent rule deserve investigation, expert also needs knowledge statistics rule classes.\r\nassist expert task, proposed Interactive Medical Miner, tool discovers classification rules class provides information statistics rules classes.","code":""},{"path":"imm.html","id":"imm-conclusions","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.4 Conclusions on Interactive Discovery and Inspection of Subpopulations","text":"date, analysis population-based cohort data mostly hypothesis-driven.\r\npresented workflow interactive applications data-driven analysis population-based cohort data using hepatic steatosis example.\r\nmining workflow includes steps () identify subpopulations different distributions respect target variable, (ii) classify subpopulation taking class imbalance account, (iii) identify variables associated outcome.\r\nworkflow shown appropriate () identify subpopulations classification reduce class imbalance, (b) drill-derived models identify important variables subpopulations worthy investigation.assist domain expert latter objective (b), developed Interactive Medical Miner, interactive application allows user explore classification rules understand cohort participants supporting rule distributed across three classes.\r\nexploration step essential identifying -yet-known associations variables target outcome.\r\nvariables must investigated ‚Äì hypothesis-driven studies.\r\nTherefore, workflow Interactive Medical Miner carry potential data-driven analysis comes providing insights multifactorial disease generating hypotheses hypothesis-driven studies.\r\nInteractive Medical Miner extended Schleicher et al.¬†[113] added panels include tables showing additional rule statistics lift p-value.\r\naddition, mosaic plot contrasts class distributions subpopulation ‚Äúcomplements,‚Äù .e., subsets participants meet one conditions length-2 rule describing subpopulation.terms multifactorial disorder interest, results verify potential data-driven approach variables top positions decision trees classification rules previously shown associated hepatic steatosis independent studies.\r\nparticular, indices fat accumulation body (BMI, waist circumference) liver enzyme GGT proposed Bedogni et al.¬†[114] reliable ‚ÄúFatty Liver Index.‚Äù\r\nAccording Yuan et al.¬†[115], SNPs rs11597390, rs2143571, rs11597086 among ‚ÄúIndependent SNPs Associated Liver-Enzyme Levels Genome-wide Significance Combined GWAS Analysis Discovery Replication Data Sets.‚Äù\r\nRegarding effects alcohol consumption, Baumeister et al.¬†mention toxic effects alcohol liver well established, also ascribe even greater role obesity heavy alcohol consumption respect accumulation fat liver¬†[116], [117].\r\nIndeed, variable related alcohol consumption appears decision tree F:age>52 (see Figure¬†3.8) among top classification rules, tend see variables associated person‚Äôs weight obesity (cf.¬†variables som_bmi_s2, som_huef_s2, som_waist_s2 figures tables Section¬†3.3).\r\nsubpopulation F:age>52 identified without prior knowledge semantics subpopulation, noteworthy age 52 years close onset menopause ‚Äì V√∂lzke et al.¬†[118] showed menopausal status associated hepatic steatosis.\r\nresults also verified another fact known medical experts independent observation: sonographic result (cf.¬†variables stea_s2, stea_alt75_s2 figures tables Section¬†3.3) associated liver fat concentration found MRI, ultrasound alone predict hepatic steatosis¬†[114], [117].algorithms provide variables associated outcome, also identify value intervals associated specific class, see, example, value intervals BMI associated class B PartitionM (Table¬†3.5) classes C F:age>52 (Table¬†3.4).\r\nintervals imply person BMI within specific interval actually belongs corresponding class, rather can serve starting point hypothesis-driven analyses.approach allows us study subpopulations two points time.\r\nmodeling step, identify subpopulations different class distributions.\r\nmodeling step, Interactive Medical Miner highlights subpopulation supports classification rule; overlapping subpopulations.\r\nOverlapping subpopulations necessarily disadvantage, especially small subpopulations.\r\nHowever, working overlapping data sets can unintuitive tedious domain expert.\r\nChapter¬†4, explore potential clustering identify reorganize similar rules, goal reducing cognitive load user displaying semantically unique representative rules.Although main application workflow longitudinal cohort study, exploit latent temporal information.\r\nHowever, characteristics describing changes lifestyle potentially valuable predicting presence disease.\r\nChapter¬†6, present follow-method expands feature space extracting temporal variables describe study participant changes time, goal deriving new informative variables hypothesis generation.","code":""},{"path":"sdclu.html","id":"sdclu","chapter":"4 Identifying Distinct Subpopulations","heading":"4 Identifying Distinct Subpopulations","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Bernhard Preim, Till Ittermann, \r\nHenry V√∂lzke. ‚ÄúCombining Subgroup Discovery Clustering Identify\r\nDiverse Subpopulations Cohort Study Data.‚Äù : Proc. IEEE Int.\r\nSymposium Computer-Based Medical Systems (CBMS). 2017, pp.¬†582-587.\r\nDOI: 10.1109/CBMS.2017.15.Epidemiologists search significant relationships risk factors outcome large heterogeneous datasets encompass participant health information gathered questionnaires medical examinations.\r\nprevious chapter, described expert-driven workflow can help epidemiologists automatically detect relationships form classification rules, descriptions risk factors predictive value ranges specific subpopulation outcome interest.\r\nHowever, rule induction algorithms often produce large overlapping rule sets requiring expert manually pick interesting rules remove less interesting redundant rules.\r\npost-filtering step time-consuming tedious.\r\nchapter presents clustering-based algorithm hierarchically reorganizes large rule sets summarizes important concepts maintaining distinctiveness clusters.\r\ncluster, representative rule shown expert turn can drill-cluster members.\r\nevaluate algorithm two subsets SHIP outcomes hepatic steatosis goiter serve target variable, respectively.\r\n, report effectiveness algorithm present selected subpopulations.chapter presents SD-Clu, approach combines subgroup discovery clustering return set \\(k\\) representative classification rules.\r\nBuilding set potentially highly overlapping rules generated SD algorithm, leverage hierarchical agglomerative clustering find groups rules cover different sets instances.\r\ncluster, nominate one rule group‚Äôs representative exhibits best trade-rule confidence coverage towards target variable.\r\ndefine similarity pair subgroups based fraction mutually covered instances individually covered instances.\r\nRules covering (almost) instances likely condensed cluster thus likely represented proxy rule.\r\nevaluate algorithm two samples SHIP investigating diseases hepatic steatosis goiter.Section¬†4.1 serves motivation related field subgroup discovery redundancy classification rules.\r\n\r\nSection¬†4.2 presents SD-Clu algorithm generates distinct rules.\r\nSection¬†4.3 contains experimental setup.\r\nSection¬†4.4, describe evaluation results.\r\nSection¬†4.4.1, discuss findings regarding hepatic steatosis goiter SHIP data.\r\nSection¬†4.5, summarize contributions.\r\n","code":""},{"path":"sdclu.html","id":"brief-chapter-summary-1","chapter":"4 Identifying Distinct Subpopulations","heading":"Brief Chapter Summary","text":"study redundancy large rule sets describing subpopulations.\r\npresent workflow extracts smaller number representative rules.\r\nrules selected avoid instance overlap much possible, thus covering different concepts data space.\r\nevaluate workflow samples SHIP hepatic steatosis goiter target variables.","code":""},{"path":"sdclu.html","id":"sdclu-intro","chapter":"4 Identifying Distinct Subpopulations","heading":"4.1 Motivation and Comparison to Related Work","text":"Subgroup discovery (SD) algorithms aim uncover interesting relationships one conditions (variables value ranges) target variable form classification rules¬†[110], [119].\r\nCompared accurate predominantly opaque black-box models neural networks, support vector machines, random forests, SD algorithms provide higher confidence interpretability results, making highly suitable domain expert-guided subpopulation discovery.\r\nSD algorithms used several medical studies descriptive knowledge needed inferred, e.g., extract potential drug targets multi-relational data sources treatment dementia¬†[120], identify predictive auditory-perceptual, speech-acoustic, articulatory-kinematic features preschool children speech sound disorders¬†[121], discover discriminative features patient subpopulations different admission times psychiatric emergency departments¬†[122].However, SD methods often yield large sets rules domain experts willing tediously go manually separate interesting irrelevant redundant rules.\r\ncommon observation groups rules cover almost set instances, shown Figure¬†4.1.\r\nInstead presenting rules found SD algorithm , propose organize rule sets hierarchically domain expert able explore compact set different concepts, equipped mechanisms drill specific rules interest.\r\nFigure 4.1: Example two redundant rules. \\(r_1\\) \\(r_2\\) cover instances 7,8,9,10; \\(r_2\\) additionally covers instance 2. cover set overlap due high correlation #Teeth Age, BMI Waist circumference. rules describe concept, .e., elderly overweight people higher risk outcome.\r\nSimilar HotSpot algorithm described Section¬†3.2.5, popular SD algorithms SubgroupMiner¬†[123],\r\nSD¬†[124] CN2-SD¬†[125] use fixed beam width limit number expanded subgroup candidates iteration.\r\npost-pruning step can applied reduce cardinality rule set ‚Äì e.g., return top-k rules ‚Äì using quality criterion weighted relative accuracy p-value statistical test.\r\nEven beam-width search top-\\(k\\) pruning applied, result often still contains redundant rules.\r\ndue correlation (non-target) variables, leads large number variations given finding, cf.¬†Figure¬†4.2 illustrative example.\r\nparticular, top-\\(k\\) pruning leads different variations concepts (.e.¬†large instance overlap)¬†[126].\r\nFigure 4.2: Graphical representation 1115 HotSpot rules found SHIP data 886 labeled instances. gray cell indicates instance x-position covered rule associated y-position. Instances rules sorted agglomerative hierarchical clustering. partitioning 10 clusters based covered instances, rules cluster describe similar subpopulations.\r\nUnlike SD, generates multiple rules overlap terms coverage sets, predictive rule learning algorithms CN2¬†[127] RIPPER¬†[128] designed generate rules capture different spaces data.\r\nwork iteratively according divide--conquer strategy¬†[109]:\r\nFirst, rule maximizes quality function algorithm generated set instances yet covered.\r\nSecond, covered instances removed training set.\r\nprocess rule induction removal instances training set repeated instances covered least one rule.\r\noutput algorithm often decision list.\r\nclassify instance, prediction first rule covers instance used.\r\nalgorithms avoid problem rule redundancy, important concepts may remain uncovered.\r\nexample, algorithm might induce rule includes instances high BMI, might able find slightly weaker association income instances immediately removed instance candidate space.\r\naddition, coverage rules generally decreases iteration.\r\nRules low coverage negligible epidemiological studies may represent spurious correlations study sample.Instead simply eliminating covered instances rule induction subsequent iterations, weighted coverage approaches take account many times instances covered far rule candidate expansion step¬†[125].\r\nleniency removing instances allows larger number rules, new hyperparameter introduced control tradeoff reusing already covered instances minimum rule confidence.\r\nparameter unintuitive difficult set, especially domain experts.\r\nMoreover, traditional predictive rule learning algorithms weighted covering extensions introduce order dependencies rules: rule depends previous rules rule list instances target variable covers, may make sense interpret single rule.","code":""},{"path":"sdclu.html","id":"sdclu-method","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2 Finding Distinct Classification Rules","text":"section, present algorithm SD-Clu, combines subgroup discovery clustering return set \\(k\\) distinct classification rules.\r\nalgorithm consists three main steps.\r\nFirst, SD algorithm generates set potentially highly overlapping rules.\r\nUsing hierarchical agglomerative clustering, set rules grouped set distinct rule clusters covering different sets instances.\r\ncluster, rule best tradeoff confidence coverage target variable appointed representative group.\r\nRules covering instances grouped cluster therefore represented proxy rule.","code":""},{"path":"sdclu.html","id":"rule-clustering","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.1 Rule clustering","text":"use notation classification rule discovery Section¬†3.2.3.\r\nAgglomerative hierarchical clustering iteratively merges similar instances clusters, bottom-way.\r\norder merging two clusters depends linkage strategy: complete linkage distance two clusters defined maximum distance two instances.\r\npair clusters minimizing maximum distance selected merging.\r\ndendrogram tree representation stepwise process.\r\nOptionally, parameter \\(k\\) can specified obtain specific partitioning.\r\ndefine rule similarity clustering basis mutually covered instances adaption S√∏rensen‚ÄìDICE coefficient¬†[129].\r\ndistance two rules \\(r_1\\), \\(r_2\\) corresponding subpopulations \\(s(r_1)\\), \\(s(r_2)\\) given \r\n\\[\\begin{equation}\r\n\\text{dist}(r_1,r_2) = 1 - \\frac{2\\cdot\\left|s(r_1)\\cap s(r_2)\\right|}{\\left|s(r_1)\\right| + \\left|s(r_2)\\right|}.\r\n\\tag{4.1}\r\n\\end{equation}\\]\r\nnumber clusters can specified parameter \\(k\\).\r\nAlternatively, describe approach derives appropriate \\(k\\) rule set using cluster quality function Silhouette coefficient.\r\nclustering \\(\\xi\\) set rules \\(R\\), Silhouette coefficient calculated \r\n\\[\\begin{equation}\r\n\\text{Silh}(R,\\xi) = \\frac{1}{|R|}\\sum_{r\\R}{\\frac{b(r)-(r)}{\\max\\left\\{(r), b(r)\\right\\}}}\r\n\\tag{4.2}\r\n\\end{equation}\\]\r\n\r\n\\[\\begin{equation}\r\n(r)=\\frac{\\sum_{y\\{}Y}\\text{dist}(r,y)}{|Y|-1}\r\n\\tag{4.3}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Y\\\\xi\\) contains \\(r\\), \r\n\\[\\begin{equation}\r\nb(r)=\\frac{\\sum_{y\\{}Z}\\text{dist}(r,y)}{|Z|}\r\n\\tag{4.4}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Z\\\\xi\\) closest cluster \\(Y\\) containing \\(r\\).\r\n, traverse dendrogram bottom-, compute Silhouette set clusters \\(\\xi\\) select \\(\\xi_{opt}\\) set clusters best Silhouette value.\r\noptimal number clusters cardinality \\(|\\xi_{opt}|\\).\r\nFinally, map cluster \\(Y\\\\xi_{opt}\\) representative rule.\r\n, invoke rule interestingness measure Weighted Relative Accuracy (WRA hereafter) balances coverage confidence gain defined \r\n\\[\\begin{equation}\r\nWRA(r)=Cov(r)\\cdot\\left(Conf(r)-\\frac{n_{T=v}}{N}\\right)\r\n\\tag{4.5}\r\n\\end{equation}\\]\r\n\\(N\\) total number instances dataset \\(n_{T=v}\\) number instances target variable value interest.\r\ncompute WRA rule \\(r\\{Y}\\) select cluster proxy \\(cp(Y)\\) rule WRA maximum.","code":""},{"path":"sdclu.html","id":"representativeness-of-a-set-of-cluster-proxies","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.2 Representativeness of a set of cluster proxies","text":"rule proxies good representation total rule set.\r\nThus, instance covered equally often cluster proxies compared total rule set.\r\nHence, define representativeness difference average fraction proxy rules instances covered total average fraction rules instances covered .\r\ndifference two ratios small, representativeness cluster proxy rules high.Typically, set rule clusters \\(\\zeta\\) set rules \\(R\\) optimal set clusters, described previous subsection, can set clusters chosen user, long contains rules \\(R\\).\r\n\\(\\zeta\\), let \\(R_{\\zeta}=\\{cp(Y)|Y\\\\zeta\\}\\) denote set cluster proxy rules.\r\nquantify representative set rules , proceed follows.\r\nFirst, let \\(U\\subseteq{}R\\) arbitrary subset complete set rules, let \\(x\\) instance.\r\ncoverage rate \\(x\\) towards \\(U\\) calculated \r\n\\[\\begin{equation}\r\ncovRate(x,U) = \\frac{\\sum_{r\\{}U}isCovered(x,r)}{|U|}\r\n\\tag{4.6}\r\n\\end{equation}\\]\r\n\\(isCovered(x,r)\\) equal 1, \\(r\\) covers \\(x\\), .e., \\(x\\s(r)\\), 0 otherwise.\r\nobserve set rules \\(U\\), instance \\(x\\) covered \\(|U|\\) rules.\r\nLet \\(R_x\\) set rules cover instance \\(x\\), .e., \\(R_x=\\{r\\{}U | isCovered(x,r)=1 \\}\\).\r\nwhole set instances \\(X\\) create bins:\r\n\\[\\begin{equation}\r\nbin_i(U)= \\{x \\{}X | |R_x|=\\}.\r\n\\tag{4.7}\r\n\\end{equation}\\]\r\n, let \\(bin_0(U)= \\{x \\{}X | \\forall r\\U : isCovered(x,r)=0 \\}\\).\r\nEvidently, instance \\(x\\) can covered \\(0, 1, \\ldots, |U|\\) rules, .e.¬†\\(covRate(x,U)\\) can take one \\(|U|+1\\) values.\r\ncontrast, \\(covRate(x,R)\\) can take one \\(|R|+1\\) values, usually much larger number. therefore map possible values \\(covRate(x,R)\\) much smaller set possible values rounding, computing:\r\n\\[\\begin{equation}\r\nadjCovRate(x,U, R)=\\frac{\\lfloor covRate(x,R)\\cdot|U| \\rceil}{|U|}\r\n\\tag{4.8}\r\n\\end{equation}\\]\r\n\\(\\lfloor\\rceil\\) rounding operator.\r\n, complete set instances \\(X\\), set induced rules \\(R\\), clustering \\(\\zeta\\) \\(R\\) set cluster proxy rules \\(R_{\\zeta}\\), representativeness \\(R_{\\zeta}\\) defined \r\n\\[\\begin{equation}\r\nrepresentativeness(R_{\\zeta},R)=1-\r\n\\frac{1}{|X|}\\sum_{x\\{}X} |adjCovRate(x,U,R) - covRate(x,R_{\\zeta})|.\r\n\\tag{4.9}\r\n\\end{equation}\\]","code":""},{"path":"sdclu.html","id":"sdclu-experiments","chapter":"4 Identifying Distinct Subpopulations","heading":"4.3 Experimental Setup","text":"Datasets.\r\nevaluate method, used data SHIP study.\r\ndescription SHIP, see Section¬†2.2.1.\r\nconsidered hepatic steatosis (see section¬†3.2.1) goiter target variables.\r\nsample HepStea, derived binary outcome variable discretizing liver fat concentration obtained MRI report study participants concentration greater 10% assigned negative class values greater 10% assigned positive class indicating presence disease.\r\n886 participants MRI report SHIP-2 available time, 694 (78.3%) negative 192 (21.7%) positive.\r\nconsidered 99 variables selected exclusively SHIP-0 assess long-term effects expressed 10 years later SHIP-2.\r\nGoiter sample, outcome variable derived thyroid ultrasound.\r\npresence goiter defined thyroid volume greater 18ml women 25ml men¬†[130].\r\n4400 participants outcome variable available TREND-0, 3010 belong negative class (68.4%) 1390 (31.6%) positive class.\r\nApart target variable, use total 182 variables pre-selected medical expert potential risk factors.SD algorithms.\r\nsubgroup discovery, two algorithms HotSpot¬†[131] (cf.¬†Section¬†3.2.5 SD-Map¬†[132] used.\r\nSD-Map exhaustive algorithm adapts popular FP-Growth association rule learning method¬†[76].\r\nRules fall minimum coverage threshold pruned.\r\ndeep-first search performed candidate generation.\r\nRules ranked according user-defined quality function.\r\nused implementation VIKAMINE framework¬†[133].\r\nimplementation SD-Map supports categorical variables.\r\nTherefore, numeric variable discretized using minimum description length based approach Fayyad Irani¬†[134].\r\nSD-Map, set minimum coverage threshold 0.05 avoid overfitting rules small.\r\nuse WRA quality function define minimum threshold 0.025.\r\nHotSpot, set support threshold rule 0.05.\r\nbeam width set 500.\r\naddition, avoid rather meaningless literals, restrict extension rule body another literal relative confidence gain least 0.3.\r\navoid many overly specific rules cover small number study participants, limit length rule body, .e., number literals 3.","code":""},{"path":"sdclu.html","id":"sdclu-results","chapter":"4 Identifying Distinct Subpopulations","heading":"4.4 Results","text":"Figure¬†4.3, show optimal number clusters combination study sample SD algorithm.\r\nTable¬†4.1 shows optimal \\(k\\) ratio proxy rules vs.¬†total number rules.\r\nexample, clustering optimal silhouette coefficient algorithm HotSpot Goiter 76 clusters thus 76 rule cluster proxies (cf.¬†Table¬†4.1), 21.3% total number rules.\r\nThus, cluster proxies displayed expert beginning, time needed check rules reduced.\r\nFigure 4.3: Silhouette coefficients (\\(Silh\\)) SD-Clu using complete linkage combination dataset algorithm. cardinality clustering highest \\(Silh\\) score (\\(|\\zeta_{opt}|\\)) indicated dashed vertical line.\r\n\r\nTable 4.1: Statistics best runs per dataset algorithm. Number rules \\(|R|\\), optimal Silhouette coefficient \\(Silh(\\zeta_{opt})\\), corresponding cardinality optimal clustering \\(|\\zeta_{opt}|\\) percentage cluster proxies relative total number rules every combination data sample SD algorithm.\r\noptimal cardinality \\(|\\zeta_{opt}|\\) shown Figure¬†4.3 used suggestion, expert free specify number rules wishes obtain.\r\nexample, expert considers \\(|\\zeta_{opt}|\\) = 100 large HotSpot HepStea, diagram show reduction \\(|\\zeta_{opt}|\\) = 58 possible, reducing \\(Silh\\) slightly 0.48 0.37.\r\nAlso direction: \\(|\\zeta_{opt}|\\) rather low, diagram shows small increase change \\(Silh\\) much; therefore, added rules may also important.\r\nexpert even analyze diagram derive range instead single value, e.g., range \\(Silh\\) 90% maximum.assess representativeness cluster proxies, compare three baseline criteria return top \\(k\\) rules according odds ratio (baseline 1), coverage (baseline 2), WRA (baseline 3).\r\nFigure¬†4.4 juxtaposes representativeness SD-Clu three baselines different numbers rules \\(k\\) returned expert HotSpot algorithm sample HepStea.\r\nplots arranged rule selection method (rows) number representative rules \\(k\\) returned expert (columns).\r\ngraph shows \\(adjCovRate\\) (y-axis) instances (x-axis) respect \\(R\\) (solid black curve).\r\ninstances sorted number rules \\(R\\) covered , respective \\(covRate\\) shown dots.\r\ndotted curve represents locally weighted scatterplot smoothing (LOWESS) points.\r\nIdeally, curves close , meaning instances covered approximately proportion rules cluster proxy proportion rules general.\r\nFigure 4.4: Evaluation \\(representativeness\\) HepStea using HotSpot algorithm. \\(representativeness\\) SD-Clu three baseline approaches different numbers clusters \\(k\\) HepStea sample using HotSpot. Points depict instance‚Äôs \\(adjCovRate\\) respect set representative rules approach (row) \\(k\\) (column). Instances sorted \\(covRate\\) respect \\(R\\) , .e., set rules) descending order, shown solid black curve. dotted curve depicts LOWESS regression fit points. similarity solid dotted curve illustrates \\(representativeness\\) top-\\(k\\) rules respective approach. illustrated dark gray area -solid curve dotted curve smaller areas better reflect higher \\(representativeness\\) values.\r\napproaches, \\(representativeness\\) improves increasing number representative rules \\(k\\).\r\nexample, \\(representativeness\\) increases 0.87 0.96 SD-Clu \\(k=10\\) \\(k=50\\) means absolute difference \\(adjCovRate\\) \\(\\zeta\\) \\(covRate\\) \\(R\\) instances successively decreases.\r\n, given \\(k\\), representative rules baselines less representative SD-Clu‚Äôs cluster proxy rules, e.g.¬†0.91, 0.92, 0.91 vs.¬†0.96 \\(k=50\\), respectively (cf.¬†5th column plot matrix Figure¬†4.4).","code":""},{"path":"sdclu.html","id":"sdclu-discussion","chapter":"4 Identifying Distinct Subpopulations","heading":"4.4.1 Discussion of Findings","text":"Tables¬†4.2 ¬†4.3 show antecedent, support, confidence cluster proxy rules found two algorithms HepStea Goiter \\(k\\)=5.\r\nprevalence hepatic steatosis goiter significantly higher subpopulations described rules corresponding overall population.\r\nsubpopulations characterized known risk factors hepatic steatosis, large waist circumference BMI, blood pressure hypertension, advanced age, high values medical tests (ALAT LDL).\r\nFurthermore, apolipoprotein A1 (ApoA1), major protein component high-density lipoprotein (HDL) particles plasma, found associated outcome elderly patients (see fourth hotspot rule Tables¬†4.2).\r\nLipoprotein metabolism considered main process contributing development fatty liver¬†[135].\r\naddition, Poynard et al¬†[136] found patients hepatic steatosis higher levels ApoA1 patients hepatic fibrosis, turn higher levels patients cirrhosis.\r\nfifth hotspot rule describes subpopulation elevated levels liver high-sensitivity C-reactive protein (CRP) (approximately 0.80-quantile) elevated levels uric acid (approximately 0.36-quantile).\r\nLizardi-Cervera et al.¬†[137] reported increased levels ultra-sensitive CRP subjects hepatic steatosis independent metabolic states.\r\nSimilarly, Keenan et al.¬†[138] found elevated uric acid levels patients hepatic steatosis independent metabolic syndrome.\r\nTable 4.2: Representative rules (HepStea). Proxy rules \\(k\\) = 5 HepStea sample learned positive outcome target.\r\nSimilarly, identified subpopulations goiter (see Table¬†4.3) characterized common risk factors increased weight body mass index participants prescribed angiotensin II receptor blockers (see second HotSpot rule).\r\nFurthermore, first HotSpot rule describes participants intima-media thickness greater 0.73 mm (approximately 0.80-quantile).\r\nPrevious studies found associations intima-media thickness thyroid-stimulating hormone¬†[139] subclinical hypothyroidism\r\n[140], [141].\r\ncondition third HotSpot rule describes duration ECG phase.\r\nJabbar et al¬†[142] summarize review pathological thyroid hormone levels increase risk cardiovascular disease.\r\nassociation appears especially true elderly¬†[143].\r\nfourth rule suggests certain thrombocyte levels indicate increased thyroid volume, confirms Erikci et al¬†[144] found hypothyroid patients higher platelet volume platelet distribution width control group.\r\nTable 4.3: Representative rules (Goiter). Proxy rules \\(k\\)=5 Goiter sample learned positive outcome target.\r\n","code":""},{"path":"sdclu.html","id":"sdclu-conclusions","chapter":"4 Identifying Distinct Subpopulations","heading":"4.5 Conclusions on identifying distinct subpopulations","text":"SD-Clu tackles problem high instance overlap sets rules generated subgroup discovery algorithms.\r\nlimiting number rules time spent rule inspection reduced.\r\nSD-Clu nominates representative rule hierarchical clustering large set rules, thus returns rules express distinct concepts, .e., rules cover different sets instances.\r\nintroduced representativeness measure assesses whether instances similarly often covered representatives total rule set.\r\nSD-Clu evaluated two samples epidemiological study optimal set proxy rules selected () contains considerably less rules total rule set (ii) representative compared baseline approaches, respectively.","code":""},{"path":"phenotypes.html","id":"phenotypes","chapter":"5 Visual Identification of Informative Features","heading":"5 Visual Identification of Informative Features","text":"chapter partly based :Uli Niemann, Petra Brueggemann, Benjamin Boecking, Matthias Rose, Myra Spiliopoulou, Birgit Mazurek. ‚ÄúPhenotyping chronic tinnitus patients using self-report questionnaire data: cluster analysis visual comparison.‚Äù : Scientific Reports 10.1 (2020), p.¬†16411. DOI: 10.1038/s41598-020-73402-8.supervised methods classification subgroup discovery described previous chapters show great potential applications one small number well-defined target variables.\r\nHowever, medical conditions heterogeneous appearances well understood yet.\r\nexample, chronic tinnitus complex, multi-factorial heterogeneous symptom.\r\nClinical assessment selection suitable therapy strategies difficult patients benefit equally treatment.\r\nDue large number heterogeneity available assessment tools, unsupervised methods like cluster analysis appear promising approaches extract clinically relevant tinnitus phenotypes.\r\nClinical acceptance empirical results can strengthened comprehensive visualization intuitively illustrate major characteristics phenotype differences multiple phenotypes.\r\nchapter, describe workflow (1) identify distinct tinnitus phenotypes parameter-free clustering algorithm (2) visualize subgroups explore phenotype idiosyncrasies.\r\nSection¬†5.1, describe clinical value patient subgroup stratification, briefly review previous approaches list requirements clustering solution.\r\nspecify features used clustering Section¬†5.2, give overview clustering algorithm Section¬†5.3, introduce cluster visualization Section¬†5.4 interactive web application Section¬†5.5.\r\nSection¬†5.6, present phenotypes describe major characteristics.\r\nmedical interpretation findings developed together tinnitus experts provided Section¬†5.7.\r\n, discuss strength weaknesses workflow Section¬†5.8.\r\nFinally, conclude chapter summary Section¬†5.9.","code":""},{"path":"phenotypes.html","id":"brief-chapter-summary-2","chapter":"5 Visual Identification of Informative Features","heading":"Brief Chapter Summary","text":"identify distinct tinnitus phenotypes clustering screening data using parameter-free algorithm leverages Bayesian information criterion determine suitable number subgroups.\r\npresent novel radial bar radial line visualizations juxtapose phenotypes high-dimensional feature space explore phenotype-specific idiosyncrasies.\r\nprovide web application enhanced versions visualizations also depict treatment effects.","code":""},{"path":"phenotypes.html","id":"phenotypes-motivation","chapter":"5 Visual Identification of Informative Features","heading":"5.1 Motivation and Comparison to Related Work","text":"Challenges management treatment tinnitus caused large extent clinical heterogeneity, includes individual perception, risk factors, comorbidities, degrees perceived stress treatment response (cf.¬†Section¬†2.2.4).\r\nfactors make difficult clinicians () choose treatment effective individual patient, (b) design unified treatment strategy patients equally benefit .\r\nawareness existence distinct patient subgroups may stimulate development effective therapy modules.\r\nSince clinically relevant subgroups established yet, clustering emerges promising approach identify characteristic tinnitus phenotypes data-driven hypothesis-free way.Previous studies found subgroups tinnitus patients cluster analysis based small number audiometric features¬†[33], combination features extracted self-reports, audiometry psychoacoustics¬†[34], neuroimaging data \r\nsocio-demographics¬†[145].\r\nAlthough studies provided insights tinnitus subgroup patterns, acceptance among medical scholars may increased presenting clustering results intuitive visualizations show individual subgroup patterns enable visual juxtaposition multiple subgroups respect high-dimensional data.Addressing requirement, Schlee et al.¬†[146] proposed compact radar chart visualization allows compare degree health burden individuals subgroups based measurements self-report questionnaires.\r\nvisualization applied disease domain, Schlee et al.¬†demonstrated efficacy showing subgroup differences respect measurements tinnitus distress associated comorbidities.\r\nHowever, aim visualize clustering results, restricted pre-defined cohorts graphically comparing female male patients, patients low tinnitus frequency patients high tinnitus frequency.","code":""},{"path":"phenotypes.html","id":"phenotypes-features","chapter":"5 Visual Identification of Informative Features","heading":"5.2 Selection of Measurement Instruments","text":"Discussions tinnitus experts selection measurement instruments (hereafter denoted features) clustering resulted two main requirements:\r\n(1) features cover clinical heterogeneity tinnitus great degree, \r\n(2) available, robust compound scores preferred single items questionnaire.\r\nroutine questionnaire assessment battery (cf.¬†Section¬†2.2.4), selected total 64 features1 14 questionnaires.\r\ninclude questionnaire total scores, questionnaire subscale scores items questionnaires neither subscales total scores.\r\nfeatures measure\r\n(general) tinnitus characteristics,\r\nphysical quality life,\r\nexperiences pain,\r\nsomatic expressions,\r\naffective symptoms,\r\ntinnitus-related distress,\r\ninternal resources,\r\nperceived stress, \r\nmental quality life.total 4,103 patients, data 2,875 (70.1%) incomplete therefore excluded.\r\nN=1,228 patients included final sample slightly, yet significantly younger excluded ones (\\(\\mu\\)included = 50.0, \\(\\sigma\\)included = 11.9; \\(\\mu\\)excluded = 51.7, \\(\\sigma\\)excluded = 13.6; \\(t\\)(2630.8) = 4.0, p \\(<\\) 0.01).\r\nAdditionally, 989 included patients patients (80.5%) post-treatment data also available used visually explore treatment effect differences clusters.\r\nSince features greater scores higher health burden, reversed remaining features greater scores higher quality life.\r\nfeature \\(X\\) reversed \\(X_{reversed} = \\max{(X)} - X\\).\r\nasterisk suffix feature name (e.g.¬†ACSA_qualityoflife*) denotes reversed feature.\r\nDue widely differing value ranges, feature standardized via z-score normalization.\r\nfeature \\(X\\) expected value \\(E(X)=\\mu\\) variance \\(Var(X) = \\sigma^2\\) standardized \\(Z = \\frac{X - \\mu}{\\sigma}\\).\r\n\\(Z\\), holds true \\(\\mu\\) = 0 \\(\\sigma^2\\) = 1.","code":""},{"path":"phenotypes.html","id":"phenotypes-xmeans","chapter":"5 Visual Identification of Informative Features","heading":"5.3 Identification of Tinnitus Phenotypes using Clustering","text":"Practical considerations data clustering include set number clusters \\(K\\).\r\nSince ground truth often available, several heuristics automatically determine \\(K\\) proposed.\r\npopular approach called ‚Äúelbow‚Äù method involves running clustering algorithm different values \\(K\\) (cf.¬†Section¬†6.2.1 application elbow method density-based clustering).\r\nnumber clusters plotted cluster compactness.\r\npopular \\(K\\)-means algorithm, cluster compactness quantified total within-sum squares (WSS), sum squared distances observation centroid clusters.\r\nWSS similar goodness fit measures increase monotonically increasing \\(K\\), idea elbow method identify curve‚Äôs characteristic ‚Äúknee point‚Äù first point adding another cluster leads minor improvement compactness.\r\nplot guaranteed exhibit distinctive knee point universal compactness thresholds exist, approach sometimes impracticable.\r\nAnother popular clustering evaluation measure Silhouette coefficient favor clusterings similar objects assigned cluster dissimilar objects assigned different cluster.Instead evaluating clustering quality post-hoc, decided chose algorithm internally identifies appropriate number clusters.\r\nX-means¬†[147] parameter-free adaption popular K-means algorithm incorporates Bayesian information criterion¬†[148] (BIC) find good trade-low total sum squares small number clusters.Let \\(\\mathcal{D}\\) dataset \\(d\\) dimensions let \\(D\\) subset \\(\\mathcal{D}\\), .e., \\(D\\subseteq \\mathcal{D}\\).\r\nK-means clustering \\(D\\) creates set clusters\r\n\\(\\mathcal{C}=\\left\\{C_1,\\ldots,C_k,\\ldots,C_K\\right\\}\\), \\(c_k\\) centroid cluster \\(k\\), \\(r_k\\) number objects \\(D\\) assigned \\(C_k\\) \\(p\\) number free parameters, .e., \\(p = (d+1) \\cdot K\\).\r\nBIC cluster \\(C_k\\) using Schwarz criterion calculated \\[\\begin{equation}\r\n\\text{BIC}(C_k) = \\hat{l}_k(\\mathcal{D}) - \\frac{p_k}{2} \\cdot \\log |\\mathcal{D}| \r\n\\tag{5.1}\r\n\\end{equation}\\]\\(\\hat{l}_k(\\mathcal{D})\\) log-likelihood \\(\\mathcal{D}\\) according \\(C_k\\).\r\npoint probabilities computed \r\n\\[\\begin{equation}\r\n\\hat{P}(x_i)=\\frac{r_{()}}{|\\mathcal{D}|}\\cdot \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}}\\text{exp}\\left(\\frac{1}{2\\hat{\\sigma}^2}||x_i-c_{()}||\\right)\r\n\\tag{5.2}\r\n\\end{equation}\\]\r\n\r\nmaximum likelihood estimate variance (identical spherical Gaussian assumption) \r\n\\[\\begin{equation}\r\n\\hat{\\sigma}^2=\\frac{1}{|\\mathcal{D}|-K}\\sum_{=1}^{|\\mathcal{D}|}\\left(x_i - \\mu_{()}\\right)^2.\r\n\\tag{5.3}\r\n\\end{equation}\\]\r\nlog-likelihood \\(\\mathcal{D}\\) according \\(\\mathcal{C}\\) \r\n\\[\\begin{equation}\r\nl(\\mathcal{D})=\\log\\prod_{=1}^{|\\mathcal{D}|} P(x_i)=\\sum_{=1}^{|\\mathcal{D}|}\\left(\\log \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}} - \\frac{1}{2 \\sigma^2} ||x_i-c_{()}||^2 + \\log \\frac{r_{()}}{|\\mathcal{D}|}  \\right).\r\n\\tag{5.4}\r\n\\end{equation}\\]main steps X-means algorithm summarized Figure¬†5.1.\r\nstart, initial partitioning generated ordinary K-means \\(K\\) = \\(K\\)lower, \\(K\\)lower lower bound number clusters.\r\n, cluster bisected; resulting two child centroids placed opposite direction along randomly chosen vector distance proportional cluster radius.\r\npair child clusters, local K-means clustering \\(K=2\\) run.\r\nBIC score new partitioning exceeds BIC score parental one, child centroids kept, otherwise parent centroid retained.\r\niterative steps repeated cluster whose bisection leads better BIC score, number clusters exceeds optional upper bound \\(K\\)upper.\r\nused R implementation Ishioka¬†[149].\r\nSince aim restrict solution space respect number clusters, set \\(K\\)lower, .e., lowest possible value, set \\(K\\)upper.\r\n\r\ninternal validation, recorded number clusters produced \\(X\\)-means 500 bootstrap samples.\r\nFigure 5.1: Principal steps X-means (simplified). Adapted ¬†[147].\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-visualization","chapter":"5 Visual Identification of Informative Features","heading":"5.4 Visualisation of Phenotypes","text":"Visualizing clusters high-dimensional data challenging.\r\nScatterplot matrices (SPLOMs) can intuitively represent \r\nrelationship pairs features matrix two-dimensional scatterplots¬†[93], [150].\r\nHowever, number features increases, number scatterplots grows quadratically, leading scalability problems overplotting.\r\nSeveral advanced visualization techniques proposed remedy, simply adding transparency colors points sophisticated density contours, hexagon binning, layers aggregated geometric features (Minimal Spanning Trees, Alpha Shape, Convex Hull), animation, combinations several techniques \r\nsplatterplots¬†[151].\r\nHowever, SPLOMs traditional visualization techniques parallel coordinate graphs¬†[152] still suitable low-dimensional data.Dimensionality reduction (DR) techniques often used project original data onto low-dimensional projection allows visualization types used.\r\nIdeally, projection preserves important structures original data, relative pairwise distance, clusters, outliers, correlations.\r\nPrincipal Component Analysis¬†[153] (PCA) seminal DR algorithm generates linear, orthogonal combinations original dimensions.\r\nnew dimension, called principal component, contains loading indicating much variability data covers.\r\nTypically, first two three dimensions carry highest charges selected visualization.\r\nPCA robust outliers capture nonlinear relationships.\r\nMultidimensional scaling¬†[154] (MDS) another early DR technique emphasizes preservation pairwise distances, ie,\r\nObjects close high-dimensional space also close low-dimensional projected space.\r\ncomplex, arbitrarily shaped structures, pairwise distances may subject curse dimensionality, leading poor results.\r\n\\(t\\)-stochastic neighborhood embedding¬†[155] (\\(t\\)-SNE) Uniform Manifold Approximation Projection¬†[156] (UMAP) nonlinear dimensionality reduction methods represent matrix pairwise similarities.\r\nidea obtain global structures clusters local structures distances neighbors.\r\n\\(t\\)-SNE UMAP can produce superior projections compared traditional linear techniques, provided hyperparameters appropriately tuned.\r\nHowever, shortcoming techniques mapping original features quantified.\r\nMoreover, projection applied new observations; instead, new projection must recomputed.\r\nstochasticity, different runs hyperparameters may yield different results.\r\nSince semantics original dimensions lost, decided DR methods suitable application.Discussions tinnitus experts led following cluster visualization requirements:keep original (interpretable) features,represent high-dimensional data dozens features,compactly compare multiple clusters glance,Contrast cluster characteristics overall patient mean.Following requirements, implemented () radial bar chart visualization single cluster (Figure¬†5.2) (b) radial line chart visualization comparing multiple clusters ¬†5.3.\r\nradial bar chart used compare observations assigned single cluster overall population.\r\nmean values features within cluster represented bars arranged radial layout.\r\nbar begins black ‚Äúzero line,‚Äù represents feature means entire population, .e., subjects used clustering.\r\nfeatures standardized (.e., z-scored) prior clustering, bars inclined outside represent feature averages population average bars inclined inside represent feature averages population average.\r\nexample, bar whose top positioned -1 characterizes feature average within cluster 1 standard deviation smaller population average.\r\naddition combination bar height bar direction, within-cluster averages also mapped bar color sequential color gradient dark blue (lower boundary) yellow (population average) light red (upper boundary).\r\nGray error lines top bar represent within-cluster standard error.\r\nallow quick feature localization, features grouped expert-defined categories, displayed inner circle along cluster name number subjects assigned.\r\nFigure 5.2: Radial bar graphs 4 phenotypes (PT). () PT 1 characterizes subgroup lowest health burden. (b) PT 2 includes suffering subjects, mean values psychosomatic somatic characteristics exceed population mean 0.5 standard deviations (SD). (c) PT 3 indicates somatic indicator scores population mean. (d) PT 4 indicates subjects elevated distress scores, including subjective stress perceived quality life. Bars arranged circular layout, height direction bar representing within-cluster feature average gray line centered top bar illustrating 95% confidence interval. characteristics grouped 9 categories defined tinnitus experts. categories shown inside inner circle. See Appendix¬†feature description. Adapted ¬†[157].\r\nradial line chart (5.3) allows comparison clusters single display.\r\nInstead bars, feature mean values represented points.\r\nPoints cluster feature category connected line segments.\r\nPoints line segments colored according respective cluster.\r\nFigure 5.3: Radial line chart phenotypes comparison. Points show within-phenotype feature averages. Points depicting features category connected line segments. Points lines colored cluster. See Figure¬†5.2 description phenotypes Appendix¬†description features. Adapted ¬†[157].\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-app","chapter":"5 Visual Identification of Informative Features","heading":"5.5 Interactive Components and Exploration of Treatment Effects","text":"provide interactive demo cluster solutions visualizations web application2 (Figure¬†5.4).\r\nfollowing interactive components added:\r\nhovering bars, lines, feature labels visualizations opens tooltips additional cluster summaries compact feature descriptions.\r\nClicking feature triggers additional plot showing original (unscaled) distribution selected feature stratified clusters , selected, also treatment.\r\ncontinuous features, semi-transparent boxplots displayed violin plot¬†[158] layers.\r\ncategorical traits, points error lines represent category proportions 95% confidence intervals, respectively.\r\nclustering performed baseline data , included treatment effect indicators allow visual detection potential differences clusters treatment efficacy.\r\nend, extended radial bar chart plotting cluster averages treatment (T0) treatment (T1) adjacent bars.\r\nends pair bars connected line arrowhead pointing T0 score T1 score.\r\nhighlight large treatment effects, connecting lines feature labels colored according relative decrease T0 T1.\r\nGiven user-defined parameter \\(\\Delta\\), .e., minimum relative magnitude difference T0 T1 considered change, elements colored () red T1 score greater T0 score \\(\\Delta\\) , (b) green T0 score less T0 score \\(\\Delta\\) less, (c) black, respectively.\r\nFigure 5.4: User interface interactive demo. Radial bar charts enhanced interactive components: hovering bar feature label opens tooltip additional cluster summaries compact feature description. Clicking feature updates right plot showing distribution selected feature stratified cluster, selected, also treatment. Continuous features shown using semi-transparent boxplots placed violin plot¬†[158] layers whereas nominal features, category proportions alongside 95% confidence intervals displayed points error lines, respectively.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-results","chapter":"5 Visual Identification of Informative Features","heading":"5.6 Results","text":"Four clusters (also referred phenotypes hereafter) represent optimal solution present data according \\(X\\)-means (cf.¬†Figure¬†5.5).\r\nFigure 5.5: Results internal validation. Bars show distribution number clusters generated \\(X\\)-means 500 bootstrap samples. common cluster number 4, occurred 82 samples (16.4%).\r\nPhenotype 1 (PT 1) represents largest subgroup (697 1,228 patients; 56.8%), characterized substantially -average symptom expression tinnitus-related general psychosomatic symptom indices, including affective symptoms, perceived stress, tinnitus-related distress, somatic symptoms, well (-average) quality life internal resources (Figure¬†5.2 ()).\r\ngroup patients potentially help-seeking, presents clinic frequently, wishes participate multimodal treatment, can assumed experience psychological distress strive present unburdened possible.\r\nTherefore, phenotype referred ‚Äúavoidance group‚Äù.\r\nPatients subgroup comparatively high levels education, employment, duration illness psychotherapeutic treatment (Figure¬†5.6).PT 2 included 173 patients (14.1%) reported highest emotional somatic burden among PTs (Figure¬†5.2 (b)).\r\nspecifically, PT 2 represents patient subgroup high\r\npsychosomatic comorbidity therefore referred ‚Äúpsychosomatic group‚Äù.\r\npatient subgroup shows high tinnitus burden addition clinically relevant impairment affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear consistent somatoform expressions distress, including somatic symptoms.\r\nPatients subgroup report greatly reduced quality life coping behaviors, pessimism, less experienced self-efficacy, optimism.\r\nRelative overall population, subgroup higher proportion women, patients live alone, unemployed, overall lower educational status.\r\nPatients cluster also report consulting physicians, taking sick days, using psychotherapy.\r\nPT 2 patients reported tinnitus noise audible throughout head (.e., unilateral) higher percentage groups.PT 3 contains 187 patients (15.2%) characterized -average scores traits measuring somatic complaints near-average scores affective symptoms (Figure¬†5.2 (c)).\r\npain scores SF8_bodilyhealth* SSKAL_painfrequency similar magnitude PT2, patient subgroup referred ‚Äúsomatic group‚Äù.\r\nPT 3 includes oldest subgroup, largest proportion female patients largest reported time since tinnitus onset.contrast PT3, PT 4 (n=171; 13.9%) -average values affective scores, quality--life components, perceived stress (Figure¬†5.2 (d)), e.g., mental component summary score (SF8_mentalcomp*; 0.85) anxious depression score (BSF_anxdepression; 0.79).\r\nTherefore, PT 4 referred ‚Äúdistress group‚Äù.\r\nPT 4 represents youngest 4 subgroups (mean 47.3 years), largest proportion male patients (Figure¬†5.6).\r\nFigure 5.6: Inter-phenotype comparison demographic characteristics. Summaries given means [95% confidence interval] entire population 4 phenotypes. Confidence intervals estimated using nonparametric Basic Bootstrap Sampling¬†[159] 2,000 samples . Kruskal-Wallis test used statistical comparison differences phenotypes continuous features (age), Pearson‚Äôs chi-square test used categorical features (sex). asterisk indicates statistical significance (\\(\\alpha\\) = 0.05). Correction multiple comparisons performed due exploratory nature study. Adapted ¬†[157].\r\nFigure¬†5.7 depicts top-10 features highest average change T1 T0 per cluster.\r\nPT 1 PT 3, BSF_elevatedmood* decreased , namely 0.48 \\(\\pm\\) 0.75 0.65 \\(\\pm\\) 0.85 (\\(Z\\) units), respectively.\r\nPT 2 PT 4, top-ranked features ADSL_depression average difference T1 T0 0.73 \\(\\pm\\) 0.88 0.74 \\(\\pm\\) 0.83, respectively.\r\nSix ten features among top-10 features clusters, including BSF_elevatedmood*, TQ_cogintivedistress, TQ_psychodistress, TQ_emodistress, TQ_distress BSF_fatigue.\r\nFigure 5.7: Cluster-specific top-10 features highest average treatment effect magnitude. Bars depict intra-cluster average differences measurements T1 T0 based standardized values. Lower values represent better treatment effectiveness. symbols right feature names indicate whether feature among top-10 features cluster position . example, character string ‚úó ‚úì ‚úó ‚úì TQ_intrusiveness (ranked 2nd PT 1) means feature among top-10 PT 1 PT 3, PT 2 PT 4.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-clinical-interpretation","chapter":"5 Visual Identification of Informative Features","heading":"5.7 Clinical Value","text":"discussed clinical relevance phenotypes three five tinnitus experts co-authored original publication¬†[157].PT 1 (avoidant group) represents half patient sample.\r\nBesides actual tinnitus symptom, patients subgroup reported affective psychosomatic symptoms.\r\nbiased presentation patients (‚Äúeverything fine weren‚Äôt tinnitus‚Äù), clinicians might easily led believe assessment possible factors contributing individual distress unnecessary.\r\nHowever, clinical experience suggests thorough assessment psychosocial stressors.\r\npsychosocial resourcefulness subgroup enables patients seek help quickly solution-oriented manner.\r\nAdequate tinnitus-specific counseling individualized (online) therapy modules include audiological, psychological, relaxation techniques may represent adequate treatment strategy patient subgroup.PT 2 (psychosomatic group) represents 15% patients high tinnitus distress clinically relevant impairment across affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear interact strongly somatoform expressions distress, including physical complaints somatic symptoms.\r\nPatients subgroup reported greatly reduced quality life coping behaviors, higher pessimism, lower experienced self-efficacy, optimism.\r\nfrequently asked question whether increased tinnitus-related distress contributes increase depression vice versa.\r\ngroup, depressive anxious symptoms may considered crucial underlying factor overall symptom distress, treatment must initially focus improving mood alleviating depression.\r\n, tinnitus-related distress may need viewed broader context medical psychological contributing factors require patient-specific conceptualization.PT 3 (somatic group) appears represent patient subgroup \r\ncharacterized somatopsychic symptom expression, .e., physical symptoms may reflect stress /underlying medical conditions.\r\nmeet needs patient subgroup, multimodal interventions may include proportion body-oriented procedures relaxation exercises physiotherapy, effects interpreted terms direct indirect psychological effects (e.g., increased well-affection others).Patients PT 4 (distress group) reported higher average perceived stress, accompanied physical exhaustion anxious-depressed mood.\r\ngroup likely include younger, employed, male patients reported chronic distress may susceptible burnout syndrome subjectively reduced mental performance (‚Äúhamster wheel‚Äù), used describe life situations even absence tinnitus distress.\r\nMultimodal therapy initially focus stress regulation techniques, including relaxation individually tailored behavior modification approaches.\r\nSimilar PT 2, high psychosomatic burden, patients PT 4 also benefit longer psychotherapeutic multimodal treatment procedures (inpatient rehabilitative).","code":""},{"path":"phenotypes.html","id":"phenotypes-discussion","chapter":"5 Visual Identification of Informative Features","heading":"5.8 Discussion","text":"Although dataset optimal number clusters four, expect number may different different sample tinnitus patients even clustering algorithm.\r\nFigure¬†5.5 shows high variance terms number clusters returned \\(X\\)-means different bootstrap samples.\r\nConsidering slightly lower occurrence 5 6 clusters, clustering result certainly set stone.Nonetheless, comparison clusters showed questionnaires characteristics differed considerably patient phenotypes.\r\nparticular, patient subgroups differed substantially terms coping behaviors, stress, tinnitus burden, perceived pain, discomfort, perception quality life.\r\ncontrast, patients appear differ respect localization noise.\r\nRegarding separability phenotypes, mostly high correlations feature within category suggest phenotyping also possible fewer questionnaires, especially since questionnaires overlap semantically, e.g.¬†PHQK_depression, ISR_depression, ADSL_depression, etc.Without ‚Äúground truth‚Äù different sets available measurements, difficult compare results similar studies.\r\ngreatest strength workflow inclusion wide range self-report questionnaire assessments.\r\nstudies also used audiometry¬†[33], [34] cardiac imaging data¬†[145].\r\nNevertheless, PT 2 (psychosomatic distress group) seems associated ‚Äúconstant distressing tinnitus‚Äù subgroup reported Tyler et al.¬†[34], mean scores tinnitus-related health distress much larger subgroups.\r\nClearly, selection meaningful set characteristics central effectiveness cluster analysis.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nclosest radial bar chart visualization radar chart proposed Schlee et al.¬†[146], whose solution tends overplot two subsets need displayed simultaneously.\r\nTherefore, fill areas spanned connected points color \r\navoid one polygon completely overlaps another.\r\nFurthermore, inferences radar map¬†[146] depend heavily arrangement features, since main criterion comparison shapes polygons.\r\nsolution compute arrangement yields areas achieve maximum mean area difference subgroups minimum area variance within subgroups partially solves problem, since still moderate number 20 features can represented.\r\nchose organize 64 features according expert-determined categories, e.g., quality life, makes easier find features compare similar features.addition, visualization tinnitus-specific can used display compact summary condition subset index symptoms.\r\nWhether visualizations adopted clinicians guide appropriate tinnitus management strategies adopted remains tested.\r\npreliminary user study, clinicians suggested graphical summaries possible patient subtypes ease challenge assigning appropriate treatment strategy specific combinations symptom presentations.\r\nexcluding patients complete questionnaires T0, may selection bias.\r\nPossible reasons noncompletion include unfamiliarity technical equipment used record item responses, loss motivation due relatively large number questionnaires, collisions baseline studies laboratory.\r\nNevertheless, analysis 15 questionnaires led insights \r\ncontributions questionnaires phenotyping, possibly allowing reduction number questionnaires.\r\nresults reflect snapshot patients‚Äô situation baseline, patient may transition one phenotype another later stages life, depending tinnitus management.\r\nTherefore, next step investigate effects treatment phenotypes detail find whether patient phenotypes benefit others.","code":""},{"path":"phenotypes.html","id":"phenotypes-conclusions","chapter":"5 Visual Identification of Informative Features","heading":"5.9 Conclusions","text":"presented workflow find distinct tinnitus phenotypes clustering using algorithm internally determines optimal number clusters.\r\nvisualized characteristics phenotype differences multiple phenotypes using radial bar line graphs.\r\nalso provided web application phenotypes can explored treatment effects.\r\nknowledge, first approach combines clustering tinnitus patients comprehensive visualization subgroups high-dimensional data interactive components exploration patterns treatment effects.","code":""},{"path":"evo.html","id":"evo","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6 Constructing Evolution Features to Capture Study Participant Change over Time","text":"chapter partly based :Uli Niemann, Tommy Hielscher, Myra Spiliopoulou, Henry V√∂lzke, Jens-Peter K√ºhn. ‚ÄúCan classify participants longitudinal epidemiological study previous evolution?‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2015, pp.¬†121-126. DOI: 10.1109/CBMS.2015.12.begin chapter work related construction temporal representations medical data (Section¬†6.1).\r\nSection¬†6.2 present evolution feature framework including full workflow encompasses steps extraction evolution features, dealing class imbalance feature selection. Subsequently, describe evaluation setup (Section¬†6.3) present results (Section¬†6.4).\r\nFinally, Section¬†6.5 conclude chapter give answers aforementioned research questions.","code":""},{"path":"evo.html","id":"brief-chapter-summary-3","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"Brief Chapter Summary","text":"present framework cohort analysis longitudinal cohort studies constructs ‚Äúevolution features‚Äù latent temporal information describing change cohort participants time.\r\nshow exploiting novel features improves generalization performance classification models.\r\nreport results SHIP outcome ‚Äúfatty liver.‚Äù","code":""},{"path":"evo.html","id":"evo-intro","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.1 Motivation and Comparison to Related Work","text":"Epidemiological studies serve basis identification risk factors associated medical condition.\r\nMachine learning still rather little used epidemiology, mainly due hypothesis-driven nature research.\r\nHowever, examples machine learning applications identification health failure subtypes¬†[160] discovery factors (including biomarkers) modulate medical outcome¬†[161], [162].\r\nlongitudinal cohort studies measurements performed multiple study waves, hence researchers obtain access sequences recordings.\r\ncontext machine learning, extracting leveraging inherent temporal information sequences may increase model performance thus, understanding medical condition interest.clinical applications temporal information often exploited, predominantly analysis patient records.\r\nexample, Pechenizkiy et al.¬†[163] analyzed streams recordings predict patient rehospitalization health failure treatment.\r\nSun et al.¬†[164] computed similarity streams patients patient monitoring data.\r\nCombi et al.¬†[165] reported streams life signals, particular temporal analysis timestamped medical records hospital patients.\r\nHowever, participants epidemiological, population-based study hospital patients ‚Äì random sample studied population, often skewed class distribution.\r\nlongitudinal study kind, recordings cohort member made moment.\r\nHielscher et al.¬†[166] presented feature engineering approach extract temporal information multiple, patient recordings longitudinal epidemiological study.\r\nFirst, assessment clusters feature-value sequences associated target variable found.\r\nAfterwards, original sequence features used conjunction classification.\r\nHielscher et al.¬†[166] showed classification performance increases features temporal information incorporated feature space.\r\nInstead modeling individual change measurement values, approach involves deriving multivariate change descriptors.\r\n\r\n\r\n\r\n\r\nPatient evolution clustering studied Siddiqui et al. [167] proposed method predicts evolution patient timestamped data clustering similarity predicting cluster movement multi-dimensional space. However, patient data considered [167] labeled moment.\r\nworkflow combines labeled unlabeled timestamped data longitudinal study improve classification performance skewed data.\r\ntarget variable, study multi-factorial disorder hepatic steatosis (fatty liver) sample participants longitudinal population-based ‚ÄúStudy Health Pomerania‚Äù (SHIP)¬†[6], see Section¬†2.2.1.\r\nSHIP cohort, assessments (interviews, medical tests, etc.) recorded three moments (SHIP-0, SHIP-1 SHIP-2), ca. 5 years apart.\r\nTemporal information often used analyzing patient data hospital, time granularity different.\r\nexample, intensive care unit, timestamped data collected fast pace, .e., every minute even every second.\r\ncontrast, participants longitudinal epidemiological study monitored period months even years.\r\nimplies measurements assessment epidemiological dataset possibly far apart.\r\nlarge time span two consecutive recordings complicates application methods designed data arrive higher frequency.\r\nexample, participant may exhibit alcohol abuse become pregnant, stop smoking start , take antibiotics affect liver, experience lifestyle changes turn medical recordings taken 5 years ago irrelevant learning participant‚Äôs current health state.\r\nAnother patient may changes lifestyle illnesses, past data reflect aging.\r\nchallenge recordings SHIP-2 labeled.\r\nreliable estimate fat accumulation liver computed magnetic resonance tomography images.\r\nSHIP-0, MRT unavailable.\r\nInstead, liver fat accumulation derived ultrasound ‚Äì procedure lower clinical accuracy.\r\nSHIP-1, calculation omitted altogether.\r\nconsequence, given SHIP participant class label available SHIP-2, label SHIP-1 partially reliable indicator SHIP-0.\r\nSince hepatic steatosis reversible disorder, label imputation ‚Äì means growth model¬†[168] ‚Äì possible; participant evolution must learned one moment labeled data.address challenges follows.\r\nFirst, group study participants moment similarity, thus building clusters cohort members similar recordings one three moments.\r\n, connect clusters across time, thus capturing transition cluster one study wave next.\r\ntransitions reflect evolution subgroups, individuals.\r\nHence, next single labeled recording per cohort participant, also exploit earlier, unlabeled recordings, description cluster assigned information clusters evolve time.\r\nshow new, augmented dataset, combining labeled unlabeled data individuals subgroups, improves classification delivers additional insights factors associated hepatic steatosis.","code":""},{"path":"evo.html","id":"evo-concept","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2 Evolution Features","text":"leverage latent temporal information longitudinal cohort study dataset extracting informative features based individual change participants transition respective clusters time.\r\npurpose, exploit similarity among participants moment, surrogate labels available first two moments, assuming similar participants evolve similarly.\r\n\r\ncall new features ‚Äúevolution features‚Äù.\r\napproach illustrated Figure 6.1¬†().\r\nmonitor individual change participants across study waves, trace change clusters separately, extract new features (labeled unlabeled data) augment original data space new descriptors change.\r\ncomplete classification workflow depicted Figure 6.1¬†(b).\r\nFigure 6.1: Concept evolution feature extraction classification performance improvement. () Clustering longitudinal cohort data subsequent generation evolution features change individuals (red) whole clusters (green). (b) Overview classification workflow.\r\nfollowing, describe clustering study participants (Section¬†6.2.1), generation evolution features (Section¬†6.2.2), strategy undersampling majority class balance class distribution (Section¬†6.2.3) feature selection strategy extract subset informative features input classification (Section¬†6.2.4).","code":""},{"path":"evo.html","id":"evo-concept-clustering","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.1 Clustering","text":"clustering, prefer density-based clustering partitional algorithms (like K-means), data contain extreme cases, clusters may arbitrarily shaped different sizes, determine number advance.\r\nmoment \\(t\\), run DBSCAN¬†[169] algorithm cluster set \\(Z(t)\\) recordings cohort members observed \\(t\\).\r\nparticipant \\(x\\), \\(v(x,f,t)\\) denotes value \\(x\\) feature \\(f\\\\) feature-set \\(F\\) \\(t\\), \\(obs(x,F,t)\\) set feature recordings \\(x\\) \\(t\\) (cf.¬†notation Table¬†6.1).\r\nTable 6.1: Symbols basic functions.\r\nDistance function.\r\ndistance participants \\(x,z\\) \\(t\\), use adjusted heterogeneous Euclidean overlap metric¬†[166], [170], weights difference two values \\(x,z\\) feature \\(f\\) feature‚Äôs information gain \\(G(f)\\), scaled largest observed value \\(G^*\\), defined :\r\n\\[\\begin{equation}\r\nd(x,z,t)=\\sqrt{\\sum_{f \\F} \\left(\\frac{G(f)}{G^*}\\cdot \\delta\\left(v(x,f,t),v(z,f,t)\\right)\\right)^2}.\r\n\\tag{6.1}\r\n\\end{equation}\\]\r\ncontinuous features, \\(\\delta(,b)\\) min-max-scaled difference values \\(,b\\), .e., \\((-b)/(\\max(f)-\\min(f))\\).\r\nnominal features, \\(\\delta(,b)\\) 0 \\(=b\\), 1 otherwise.DBSCAN Parameter setting.\r\nDBSCAN relies two parameters: radius \\(eps\\) neighborhood around data point, minimum number \\(minPts\\) neighbors point core point.\r\nuse ‚Äúelbow‚Äù heuristic ¬†[169] determines suitable \\(eps\\) value given \\(minPts\\) value, illustrated Figure¬†6.2.\r\nspecifically, define parameter \\(k\\), compute \\(x\\{}Z(t)\\) distance k-dist\\((x,k)\\) k-th nearest neighbor.\r\nsort distances, draw k-dist\\((x,k)\\) graph g span line l connecting smallest k-dist() value largest one.\r\n, set \\(eps\\) k-dist value maximum distance g l.\r\nFigure 6.2: Setting \\(eps\\) based k-dist graph given \\(minPts\\). k-dist graph g depicts sorted distances points‚Äô k-th next neighbors. suitable \\(eps_{opt}\\) can identified position maximum distance k-dist line l connects first last point g. DBSCAN clustering \\(minPts\\) = k, \\(eps_{opt}\\), points k-dist \\(\\leq\\) \\(eps_{opt}\\) become core points, else border noise points.\r\n","code":""},{"path":"evo.html","id":"evo-concept-evo-features","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.2 Constructing Evolution Features","text":"Table¬†6.2 provides description evolution features.\r\ncohort member \\(x\\) moment \\(t\\), record cluster containing \\(x\\) (feature 1 Table¬†6.2),\r\n(2) distance \\(x\\) cluster‚Äôs centroid,\r\n(3) fraction positively labeled participants among \\(k\\) nearest neighbors \\(x\\),\r\n(4) (graph-based) cohesion¬†[171] (5) Silhouette coefficient¬†[171] \\(x\\), (6) (graph-based) separation¬†[171] \\(x\\) cohort participants outside cluster.\r\ncompute difference cohesion, silhouette separation values \\(t\\) later moments \\(\\{t' \\T|t'>t\\}\\) (7-9), also check much values metrics change \\(x\\) moves \\(c(x,t)\\) \\(c(x,t')\\) (10-12).\r\nrecord whether \\(x\\) outlier, .e., DBSCAN noise point moment (13).\r\n\\(t\\) \\(\\{t' \\T|t'>t\\}\\), compute fraction cohort members cluster \\(x\\) \\(t\\) \\(t'\\) (14), fraction common \\(k\\) nearest neighbors (15), change distance \\(x\\) centroid \\(t\\), \\(t\\) \\(t'\\) (16).\r\nrecord changes sequence values feature, including real (17), absolute (18) relative (19) differences values two moments.\r\nmeasure cluster shrinks/grows \\(t\\) \\(t'\\) (20), much members move (average) closer far apart previous positions (21-23).\r\nway, extract information evolution participants, distinguishing among evolve smoothly switch among clusters.\r\ntransfer information evolution features, thus enriching feature space information unlabeled moments.\r\nTable 6.2: Overview extracted features. first group features () comprises cluster membership aggregated distance information participant moment; feature group II changes participant‚Äôs position (hyperspace) relative cluster closest neighbors; feature group III captures changes values participant‚Äôs recordings; feature group IV refers changes clusters.\r\n","code":""},{"path":"evo.html","id":"evo-concept-undersampling","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.3 Undersampling","text":"imbalanced data, feature selection classification often biased favor majority class¬†[172].\r\navoid problem, prior application CFS undersample majority class generate balanced data set select features informative respect classes.\r\n\r\n\r\n\r\n\r\n\r\n\r\n","code":""},{"path":"evo.html","id":"evo-concept-feature-selection","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.4 Feature Selection","text":"use feature selection method Hielscher et al.¬†[173] follows.\r\nFirst, invoke correlation-based feature selection¬†[175] (CFS), builds feature set iteratively inserting feature adds ‚Äúmerit‚Äù .\r\nmerit \\(M_F\\) feature set \\(F\\) computed calculating information gain pair features \\(F\\) (lower gain corresponds low correlation thus preferred) feature \\(F\\) towards target variable (higher gain better).\r\nContinuous features first discretized entropy-based method Hielscher et al.¬†[134].\r\ndiscretize feature selection; clustering classification use original values.shown Figure¬†6.1, perform feature selection twice.\r\nfirst time, consider features recorded moments.\r\nessential evolution tracing: can compute distances objects clusters located topological space.\r\ngenerating evolution features, build complete set features, also considering recorded moment.\r\nset perform feature selection , discard unpredictive (original evolution) features.\r\nfinal feature set used classification.","code":""},{"path":"evo.html","id":"evo-evaluation","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.3 Evaluation Setup","text":"evaluate workflow 10-fold cross-validation four --shelf classification algorithms: random forest¬†[180] (RF), C4.5 decision tree¬†[181], Na√Øve Bayes¬†(NB) k-nearest neighbor (kNN).\r\nNext, compare generalization performance algorithm used alone (baseline variant) vs.¬†incorporated workflow (worflow-enhanced variant).\r\n, study impact different combinations three workflow components undersampling (U), feature selection (F) incorporation generated evolution features (G).\r\nshown Table¬†6.3, Baseline simply invokes classification algorithm; use classification algorithms achieves highest F-measure scores.\r\nvariant U-G performs undersampling uses generated evolution features classification.\r\nSince undersample feature selection build classification models original dataset, U-G identical --G U-- identical Baseline variant, omit explicityl list U-G U--.main parameter \\(k\\) number neighbors data point: set \\(minPts\\) = \\(k\\) use \\(k\\) derive values DBSCAN parameter \\(eps\\) (cf.¬†Section¬†6.2.1) parameters features same_kNN_\\(k\\)_t_1_t_2 fraction_Of_POS_kNN_\\(k\\)_t (cf.¬†Table¬†6.2).\r\n, number nearest neighbors k-NN classification algorithm also set \\(k\\).\r\nvary \\(k\\) measure impact classification performance.Following findings Chapters¬†3 ¬†4 differences female male participants respect outcome, run experiments whole dataset (Partitionall), partitions female (Partitionf) male (Partitionm) participants.\r\nFinally, list important features found Partitionall two subsets.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 6.3: Workflow variants. UFG complete workflow.\r\n","code":""},{"path":"evo.html","id":"evo-results","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.4 Results","text":"Figure¬†6.3 shows sensitivity (left), specificity (center) F-measure (right) simple classifiers (gray curves) workflow-enhanced counterparts (line style, colored), different \\(k\\).\r\n\r\n\r\nOverall, workflow-enhanced variant outperforms simple counterpart respect sensitivity F-measure, outperforms performs slightly worse respect specificity.\r\nworkflow-enhanced Naive Bayes performs best respect sensitivity \\(k\\) best \\(k=31\\).\r\nDecision trees exhibit highest F-measure, improvements sensitivity F-Measure compared simple variant, albeit specificity sligthly worse; improvements less large \\(k\\).\r\nRandom Forests benefit workflow, absolute improvement F-measure 30% (green vs gray ‚Äú+‚Äù curves right part Figure¬†6.3).\r\nOne explanation rather poor sensitivity simple RF variant large number trees (100) learned data samples containing positive examples, RF trapped many majority class examples.\r\nconsistent specificity curve (almost straight line around 95%) simple RF, F-measure slightly 40%.\r\nworkflow improves RF sensitivity (63%) F-measure (65%), specificity remains high (90%).\r\nOverall, impact \\(k\\) three measures limited algorithms except workflow-enhanced baseline k-NN naturally affected stronger value \\(k\\) algorithm.\r\nTherefore, workflow-enhanced variants outperform simple counterparts terms sensitivity F-measure. algorithms, workflow prevents overfitting negative class.\r\nFigure 6.3: Comparison classification performance workflow baseline. Sensitivity (true positive rate), specificity (false positive rate) F-measure scores different classifiers varying number k neighbors cohort member impacts clustering result. classifier two performance curves shown: colored one workflow-enhanced version gray one baseline counterpart. Higher values better measures. [182].\r\nworkflow component-specific analysis results Figure¬†6.4 show complete workflow UFG variants UF- --G outperform --- sensitivity F-measure.\r\nvariants -F- -FG perform well regarding specificity, suggests feature selection may fruitful without undersampling datasets class imbalance.\r\nFigure 6.4: Comparison classification performance workflow components. Sensitivity, specificity F-measure scores workflow variant baseline using decision tree learning. [182].\r\nImportant features. performance workflow variants include feature selection indicates small number features sufficient class separation.\r\nHereafter, partition report evolution features selected classification among top-15 features according information gain.\r\nFigure¬†6.5 shows PartitionAll 3 15 features generated evolution features.\r\nboxplots () (c) Figure¬†6.5 refer differences values recorded two moments.\r\nfeature separationDelta_g_1_2 measures difference cluster separation participant based cluster assignment moment 1 2, corresponds entry #9 Table¬†6.2.\r\nParticipants positive exhibit higher median separationDelta_g_1_2 participants negative class indicating clusters harboring mostly positive participants cover larger, sparse areas.\r\nfeature relative_Difference_som_huef_g_0_1 (#19) quantifies difference participant‚Äôs hip circumference SHIP-0 SHIP-1, relative value SHIP-0.\r\naverage study participants classes lose weight grow older, negative participants reduce weight compared positive participants (cf.¬†Figure 6.5¬†(c)), general reflects differences life styles.\r\nmosaic chart Figure 6.5¬†(b) feature fraction_of_Positives_kNN_1_g_2 (#3) indicates nearest neighbor participant fatty liver likely also exhibit disorder non- fatty liver participant.\r\nFigure 6.5: Visualization selected evolution features contribute class separation whole dataset PartitionAll.\r\nPartitionF, 5 top-15 features evolution features, cf.¬†Figure¬†6.6.\r\nCompared female participants without disorder, female subjects fatty liver exhibit larger distance centroid cluster SHIP-1 (#2), lower silhouette coefficient SHIP-1 (#5), higher difference waist circumference SHIP-0 SHIP-2 (#19), lower relative difference serum triglycerides concentration SHIP-0 SHIP-1 (#19).\r\nFigure 6.6: Visualization selected evolution features contribute class separation PartitionF.\r\nPartitionM, 2 top-15 features evolution features (Figure¬†6.7), including relative difference waist circumference SHIP-1 SHIP-2 (#19) difference separation SHIP-0 SHIP-1 (#6).\r\nfeatures, patients exhibiting disorder greater values.\r\nFigure 6.7: Visualization selected evolution features contribute class separation PartitionM.\r\n","code":""},{"path":"evo.html","id":"evo-conclusions","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.5 Conclusions from Exploiting Study Participant Evolution","text":"proposed workflow classification longitudinal cohort study data exploits inherent temporal information clustering cohort participants moment, linking clusters tracing participant evolution moments study.\r\nclusters transitions extract evolution features, added feature space subsequently used classification.\r\nworkflow improves generalization performance respect sensitivity F-measure scores.\r\ngenerated evolution features contribute improvement, even used alone without undersampling skewed data.\r\nshow change values somatographic variables cluster quality indices time predictive.","code":""},{"path":"diabfoot.html","id":"diabfoot","chapter":"7 Feature Extraction From Short Temporal Sequences for Clustering","heading":"7 Feature Extraction From Short Temporal Sequences for Clustering","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Fred Samland, Thorsten Szczepanski, Jens Gr√ºtzner, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. ‚ÄúLearning Pressure Patterns Patients Diabetic Foot Syndrome.‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2016, pp.¬†54‚Äì59. DOI: 10.1109/CBMS.2016.31.Uli Niemann, Myra Spiliopoulou, Thorsten Szczepanski, Fred Samland, Jens Gr√ºtzner, Dominik Senk, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. ‚ÄúComparative Clustering Plantar Pressure Distributions Diabetics Polyneuropathy May Applied Reveal Inappropriate Biomechanical Stress.‚Äù : PLOS ONE 11.8 (2016), pp.¬†1‚Äì12. DOI: 10.1371/journal.pone.0161326.Uli Niemann, Myra Spiliopoulou, Jan Malanowski, Juliane Kellersmann, Thorsten Szczepanski, Silke Klose, Eirini Dedonaki, Isabell Walter, Antao Ming, Peter R. Mertens. ‚ÄúPlantar temperatures stance position: comparative study healthy volunteers diabetes patients diagnosed sensoric neuropathy.‚Äù : EBioMedicine 54 (2020), p.¬†102712. DOI: 10.1016/j.ebiom.2020.102712.written","code":""},{"path":"diabfoot.html","id":"brief-chapter-summary-4","chapter":"7 Feature Extraction From Short Temporal Sequences for Clustering","heading":"Brief Chapter Summary","text":"present approach build representations short temporal sequences via clustering example pressure- posture-dependent plantar temperature pressure patients diabetic foot syndrome.","code":""},{"path":"iml.html","id":"iml","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8 Post-Hoc Interpretation of Classification Models","text":"chapter partly based :Uli Niemann, Philipp Berg, Annika Niemann, Oliver Beuing, Bernhard Preim, Myra Spiliopoulou, Sylvia Saalfeld. ‚ÄúRupture Status Classification Intracranial Aneurysms Using Morphological Parameters.‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2018, pp.¬†48-53.\r\nDOI: 10.1109/CBMS.2018.00016.Uli Niemann, Benjamin Boecking, Petra Brueggemann, Wilhelm Mebus, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúTinnitus-related distress multimodal treatment can characterized using key subset baseline variables.‚Äù : PLOS ONE 15.1 (2020), pp.¬†1-18.\r\nDOI: 10.1371/journal.pone.0228037.Uli Niemann, Petra Brueggemann, Benjamin Boecking, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúDevelopment internal validation depression severity prediction model tinnitus patients based questionnaire responses socio-demographics.‚Äù : Scientific Reports 10.1 (2020), p.4664.\r\nDOI: 10.1038/s41598-020-61593-z.medical applications, understanding clearly communicating results machine learning model critical deriving actionable knowledge can ultimately used improve disease prevention, diagnosis, treatment.\r\n\r\n\r\nObtaining results easily understood data scientists medical experts helps formulate new hypotheses regarding relationship potential risk protective factors target; significance relationships can tested follow-studies.\r\nCurrent state---art machine learning algorithms produce models superior performance compared simpler interpretable models, decision trees, rule lists, linear regression fits.\r\nHowever, opaque black boxes involve many complex feature interactions decisions, nonlinear, often difficult explain understandable way.\r\nArising need provide understandable insights otherwise opaque models, interpretability community machine learning gained traction goal resolving dilemma choosing moderately accurate interpretable models highly accurate opaque black-box models.chapter, describe comprehensive data analysis workflow high-dimensional medical data includes classification, feature elimination, post-learning analysis steps addition application-specific preprocessing steps.\r\nvariety learners used classification, simple interpretable models complex black boxes.\r\nbest classifier, explore different post-hoc interpretation methods derive model-, observation-, subpopulation-level insights.\r\nreport results evaluate approach based via experiments three applications.\r\nchapter organized follows.\r\nSection¬†8.1, describe reasons using interpretable machine learning methods provide methodological underpinnings selection pioneering methods.\r\nSubsequently, present components mining workflow Section¬†8.2, includes correlational analysis, image preprocessing, feature selection, classification high-dimensional medical data, hyperparameter tuning, model evaluation approach putting interpretation methods use.\r\nSection¬†8.3, report findings three applications: prediction () tinnitus-related distress (ii) depression treatment tinnitus patients, well (iii) rupture status classification intracranial aneurysms.\r\ndiscuss results Section¬†8.4 conclude chapter Section¬†8.5.","code":""},{"path":"iml.html","id":"brief-chapter-summary-5","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"Brief Chapter Summary","text":"present machine learning workflow combines classification high-dimensional medical data model explanation using post-hoc interpretation methods.\r\nend, use Shapely value explanations (SHAP), LASSO coefficients, partial dependency graphs.\r\napproach provides statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance, help illuminate complex black-box machine learning models.\r\nreport results three applications: () tinnitus-related distress tinnitus patients, (ii) depressivity tinnitus patients, (iii) rupture risk intracranial aneurysms.","code":""},{"path":"iml.html","id":"iml-motivation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.1 Motivation and Methodological Underpinnings","text":"Current state---art machine learning algorithms, gradient boosting¬†[85] tabular data deep learning¬†[183] unstructured data (images, videos, audio recordings), widely used support medical decision-making.\r\nmethods produce models typically achieve better predictive performance simpler models decision trees, rule lists, linear regression fits.\r\nHowever, also complex, making difficult understand prediction made.\r\nThus, practitioners may face dilemma choosing either opaque black-box model high predictive power simple, less accurate model can least explained domain expert.\r\nEspecially high-risk domains healthcare, misconceptions can serious consequences, ability explain reasoning model highly desirable, essential, property decision-support system¬†[184], [185].result, methods explain predictions complex machine learning models attracted increasing attention recent years¬†[186], [187].\r\nExisting methods classified according different criteria¬†[185].\r\nexample, distinction made intrinsically interpretable models post-hoc explanations.\r\nformer often entails limiting model complexity choosing algorithms produce transparent models, decision trees linear regression models.\r\nDecision trees, example, can intuitively visualized node-link diagrams.\r\nFeatures split conditions near tree root generally higher impact predictions features occurring lower tree levels within leaf nodes.\r\nQuantitative measures calculate overall importance feature decrease impurity variance nodes feature occurs compared parent nodes¬†[188].\r\nFurthermore, data partitions created decision tree can described understandable conditions ‚Äúbody mass index > 30,‚Äù decision paths root leaf nodes provide insights feature interactions.\r\naddition, can used contrasting predictions individual instances, e.g., considering alternative feature values effects model prediction (‚Äúpatient body mass index 25 instead 30, difference terms prediction .‚Äù).\r\nDisadvantages decision trees able capture linear, non-axis-parallel relationships predictors response, can unstable respect small changes training data¬†[189].\r\nTherefore, may unsuitable complex learning tasks.sophisticated model trained instead, post-hoc methods can applied examine model training.\r\noutput methods can feature summary statistics, model internals, individual observations, feature summary visualization¬†[185].\r\ngeneral, feature summary statistics individual scores express overall importance feature model prediction strength feature‚Äôs interaction features.\r\nExamples model internals coefficients linear model weight vectors neural network.\r\nIndividual observations can describe representatives (prototypes) observation subgroups model provides consistent predictions subgroup members.\r\nIndividual observations can also used provide counterfactual explanations, e.g., determine minimum change cause model predict different class particular observation interest.Partial dependence plots. Visualizations feature summaries typically depict trends relationship subset features predicted response, often form curves surface plots.\r\npartial dependence plot¬†[85] (PDP) widely used tool visually depicting marginal effect one predictors predicted response model.\r\nexample, PDP Figure 8.1¬†() shows roughly S-shaped relationship values predictor values response estimated model.\r\n\r\nFigure 8.1: Illustrations partial dependence plot (PDP) individual conditional expectation (ICE) plot artificial data. () PDP predictor artificial dataset. Points represent sample predictor distribution. (b) PDP augmented ICE curves. 2 distinct subsets observations PD different upper half predictor distribution.\r\nLet \\(\\zeta\\) classification model (general function returns single real value) let \\(F=Q\\cup R\\) total set features, \\(Q\\) chosen subset features \\(R\\) complement subset.\r\npartial dependence \\(PD\\) model \\(\\zeta\\) \\(Q\\) can represented \\[\\begin{equation}\r\nPD(Q)=\\mathbb{E}_{R}\\left[\\zeta(X)\\right]=\\int\\zeta(Q,R)p_R(R)\\,dR.\r\n\\tag{8.1}\r\n\\end{equation}\\], \\(p_R(R)\\) marginal probability density \\(R\\), .e., \\(p_R(R)=\\int p(X)\\,dQ\\), \\(p(X)\\) joint density dataset \\(X\\).\r\ncomplement marginal density \\(p_R(R)\\) estimated training data, \\(PD\\) can approximated \\[\\begin{equation}\r\nPD(Q)=\\frac{1}{N}\\sum_{=1}^N \\zeta(Q,R_{})\r\n\\tag{8.2}\r\n\\end{equation}\\]\\(R_i\\) actual values complementary features observation \\(\\), \\(N\\) total number observations training data.\r\ncardinality \\(Q\\) usually chosen either equal 1 2.\r\nresults visualized line chart (\\(|Q|=1\\)) contour chart (\\(|Q|=2\\)).\r\npractice, random sample often drawn \\(Q\\) reduce computation time.averaging across observations removes information variability, PD curves can obscure potentially distinct observation subgroups substantially different effects predictors model output.\r\nremedy, Goldstein et al.¬†[86] proposed individual conditional expectation (ICE) plots showing curve observation.\r\nFigure 8.1¬†(b) illustrates example small number observations (black curves) differ rest PD constant second half predictor distribution.\r\nLIME. Another criterion distinguishing model interpretation methods whether explanations global local, .e., whether explanations apply observations one small number selected observations.\r\nLocal Interpretable Model-Agnostic Explanations¬†[190] (LIME) popular local post-hoc interpretation method.\r\nmain assumption LIME complex model linear local scale¬†[190].\r\nThus, explain predictions black-box model particular observation interest \\(\\), LIME generates surrogate model intrinsically interpretable whose predictions similar predictions black-box model proximity \\(\\).\r\nmain ideas LIME shown Figure¬†8.2.\r\nFigure 8.2¬†() shows decision boundary black-box model.\r\nSince form nonlinear decision boundary quite complex, model predictions explained simple terms.\r\nLIME attempts approximate behavior black-box model creating linear surrogate model performs well, especially near user-selected instance interest.\r\nend, perturbed training set created repeatedly randomly changing values instance interest.\r\nFigure 8.2¬†(b) shows instance interest perturbed instances, glyph size represents proximity instance interest.linear surrogate model trained dataset, observation weights proportional distance instance interest.\r\nFigure 8.2¬†(b), decision boundary surrogate model shown dashed line.\r\nFinally, model internals displayed user explanation, coefficients logistic regression model.\r\nFigure 8.2¬†(c) shows feature importance ranking, bar height represents model coefficient feature.\r\nLIME provides intuitive interpretations applicable tabular non-tabular data, several design decisions make hyperparameters tune, including neighborhood kernel width, surrogate model family, feature selection mechanism, number features considered surrogate model, among others.\r\nstability results LIME questioned¬†[191], [192].\r\nFigure 8.2: Illustration LIME‚Äôs main ideas. () data set two-class problem, represented two-dimensional scatterplot simplicity. nonlinear decision boundary black-box model easily explained. (b) LIME aims approximate predictions black-box model vicinity instance interest intrinsically interpretable model, logistic regression model. dashed line shows linear decision boundary surrogate model. (c) feature importance ranking can derived model coefficients.\r\nModel-specific interpretation methods limited specific model families, model-agnostic interpretation methods can applied type model.\r\nModel-specific methods based model internals widely used neural networks¬†[193], e.g.¬†layered relevance propagation¬†[194], explicitly uses layered structure neural network infer explanations.\r\ncontrast, model-agnostic methods decoupled actual learning process access algorithmic internals.\r\nSince consider output models, .e., predictions, model-agnostic methods also post-hoc.\r\nexample, LIME representative model-agnostic, post-hoc interpretability method.SHAP. Closely related LIME Shapley Additive Explanations¬†[195] (SHAP) framework, derives additive feature attributions predictions model.\r\nSHAP based Shapley values¬†[196]‚Äì[198], originally developed game theory.\r\nterm ‚Äúadditive‚Äù denotes given observation, model output equal sum attributions feature.\r\nspecifically, observation \\(x\\), model output \\(\\zeta(x)\\) \r\n\\[\\begin{equation}\r\n\\zeta(x)=\\phi(\\zeta,x)_0 + \\sum_{j=1}^M \\phi(\\zeta,x)_j\r\n\\tag{8.3}\r\n\\end{equation}\\]\\(\\phi(\\zeta,x)_0=E(\\zeta(x))\\) expected value model training data, \\(\\phi(\\zeta,x)_j\\) attribution feature \\(j\\) \\(x\\), \\(M\\) total number features.\r\n, combination feature \\(j\\) observation \\(x\\), Shapely value \\(\\phi\\) represents impact predictor added, aggregated weighted average possible feature subsets \\(S\\subseteq S_{}\\):\\[\\begin{equation}\r\n\\phi_{j}(x)=\\sum_{S\\subseteq S_{}\\setminus\\left\\{j\\right\\}}\\frac{|S|!(M-|S|-1)!}{M!}\\left(\\zeta_{S\\cup{j}}(x)-\\zeta_S(x)\\right).\r\n\\tag{8.4}\r\n\\end{equation}\\]SHAP feature importance estimates offer several practical properties.\r\nFirst, sum feature attributions observation equal difference average prediction model actual prediction observation (local accuracy).\r\nSecond, feature important one model another, regardless features also present, importance attributed feature also higher (symmetry / monotonicity).\r\nThird, feature value missing, associated feature importance 0 (missingness).\r\nSeveral approaches proposed reduce complexity Shapley value estimation exponential polynomial time, including KernelSHAP¬†[195], works model type, TreeShap¬†[199] tree-based models.Feature Selection.\r\ncontext predictive modeling, feature selection (FS) methods generally aim reduce number predictor features either () maximize model performance (b) affect model performance little possible.\r\nmodeling families sensitive predictors irrelevant target feature, support vector machines¬†[202] neural networks¬†[183], [204].\r\nOthers, linear logistic regression models, susceptible correlated predictors.\r\nOften domain experts require intrinsically interpretable models, requires eliminating predictors contribute substantially model performance.Traditionally, FS methods broadly classified three categories: embedded, filter, wrapper¬†[206].\r\nEmbedded FS refers internal mechanisms modeling algorithms evaluate usefulness features.\r\nExamples algorithms include tree- rule-based models¬†[208], [209], regularization methods Least Absolute Shrinkage Selection Operator¬†[212] (LASSO) Ridge¬†[214] regression.\r\nFiltering methods rank predictors based measure importance, e.g., correlation target feature.\r\n\r\nPopular examples include correlation-based feature selection¬†[215] Relief¬†[216].\r\nWrapper methods rank refine candidate feature subsets iterative search driven model performance.\r\nExamples include sequential forward search, recursive backward elimination, genetic search¬†[217].","code":""},{"path":"iml.html","id":"iml-workflow","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2 Overview of the Mining Workflow","text":"section, describe components mining workflow (Figure¬†8.3).\r\napply workflow three classification tasks: prediction () tinnitus-related distress (ii) depression treatment tinnitus patients, well (iii) rupture status classification intracranial aneurysms.\r\nrefer tasks CHA-Tinnitus, CHA-Depression AneurD, hereafter.CHA dataset, selected patients complete data two classification tasks. AneurD, segmented aneurysms raw image data, performed automated centerline neck curve extraction, generated morphological features. performed correlation analysis identify relevant correlations predictors, correlations predictors response, significant differences correlation predictors response T0 T1. embedded model training iterative feature elimination wrapper retained predictors identified important model. selected best overall model based AUC used post-hoc interpretation methods identify predictors highest attribution model prediction global, subpopulation observation level.\r\nFigure 8.3: mining workflow.\r\n","code":""},{"path":"iml.html","id":"preprocessing-of-raw-image-data","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.1 Preprocessing of Raw Image Data","text":"AneurD, necessary extract morphological features first.Segmentation neck curve extraction.\r\nAneurysms vessels segmented using threshold-based approach¬†[218] digital subtraction data reconstructed 3D rotational angiography images.\r\nSubsequently, centerline vessel extracted using Vascular Modeling Toolkit (VMTK, vmtk.org)¬†[219].\r\nSubsequently, plane separating aneurysm parent vessel determined using automatic ostium detection Saalfeld et al.¬†[220].Morphological feature extraction.\r\n3D surface mesh, obtained neck curve, dome point \\(D\\), two base points \\(B_1\\) \\(B_2\\).\r\ndescribed ¬†[220], \\(B_1\\) \\(B_2\\) approximated points centerline largest distance rays \\(B_1\\) \\(B_2\\) \\(D\\) intersect surface mesh.\r\nFigure¬†8.4 illustrates extracted parameters, \\(H_{max}\\), \\(W_{max}\\), \\(H_{ortho}\\), \\(W_{ortho}\\), \\(D_{max}\\) (see Figure¬†()) describe aneurysm shape¬†[20], [221].\r\nangle parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) (Figure 8.4¬†(b)) extracted based \\(B_1\\), \\(B_2\\), \\(D\\), respectively.\r\nabsolute difference \\(\\alpha\\) \\(\\beta\\) denoted \\(\\Delta_{\\alpha\\beta}\\).\r\nseparating aneurysm parent vessel neck curve, able derive surface area \\(A_A\\) volume \\(V_A\\) aneurysm (Figure 8.4¬†(c)).\r\nprovide two variants surface area ostium, \\(A_{O1}\\) \\(A_{O2}\\) (see Figure 8.4¬†(d)).\r\n\\(A_{O1}\\) area ostium, .e., area triangulated ostium surface resulting connection neck curve points centroid \\(C_{NC}\\), \\(A_{O2}\\) denotes area neck curve projected plane (cf.¬†[220]).\r\nTherefore, \\(A_{O2}\\) extracted parameter comparable studies often use cutting plane determine ostium.\r\nhighly lobulated aneurysms, method achieves local optimum considers one many dome points.\r\nAlthough estimated positions \\(B_1\\) \\(B_2\\) may vary slightly, neck curve detection still performed morphological parameters calculated. Table¬†(tab:09-morphological-features) provides overview extracted morphological features.\r\nFigure 8.4: Illustration extracted morphological features. () Features describe aneurysm width, height, diameter. (b) angles \\(\\alpha\\), \\(\\beta\\) \\(\\gamma\\) extracted base points \\(B_1\\), \\(B_2\\) dome point \\(D\\). (c) separating aneurysm parent vessel via neck curve, area \\(A_A\\) volume \\(V_A\\) computed. (d) area ostium \\(A_{O1}\\) area projected ostium \\(A_{O2}\\) extracted estimating center neck curve \\(C_{NC}\\).\r\n\r\nTable 8.1: Overview morphological features extracted AneurD.\r\n","code":""},{"path":"iml.html","id":"correlational-analysis","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.2 Correlational Analysis","text":"CHA tinnitus, calculate pairwise Spearman correlation predictors part exploratory data analysis.\r\nUsing agglomerative hierarchical clustering complete linkage, arrange predictors correlation heat map potential subgroups predictors similar intra-group correlations similar inter-group correlations can visually identified.\r\nFurthermore, calculate median correlation response predictors questionnaire T0 T1 obtain potential candidate predictors important later modeling step.\r\naddition, identify predictors highest absolute correlation respect response T0 T1, respectively.\r\nFinally, examined predictors whose correlation TQ distress score differed T0 T1.","code":""},{"path":"iml.html","id":"classification-algorithms","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.3 Classification Algorithms","text":"order limited particular classification algorithm, create model highest possible predictive power, examined total eleven classifiers:Least absolute shrinkage selection operator¬†[212] (LASSO) Ridge¬†[214] extensions ordinary least squares (OLS) regression perform feature selection regularization improve predictive performance interpretability.\r\ndataset \\(n\\) observations, \\(p\\) predictor features target \\(y\\), objective LASSO Ridge solve\r\n\\[\\begin{equation}\r\n\\underset{\\beta}{\\text{argmin}} \\underbrace{\\sum_{=1}^n \\left( y_i - \\left( \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j \\right) \\right)^2}_{\\text{Residual Sum Squares}} + \\alpha \\lambda \\underbrace{\\sum_{j=1}^p |\\beta_j|}_{\\text{L1 Penalty}} + (1-\\alpha) \\lambda \\underbrace{\\sum_{j=1}^p \\beta_j^2}_{\\text{L2 Penalty}}\r\n\\tag{8.5}\r\n\\end{equation}\\]\r\n\\(\\beta\\) determined model coefficients, \\(\\lambda\\) tuning hyperparameter controls amount regularization.\r\nLASSO uses L1 norm penalty term, .e., \\(\\alpha\\) = 1, shrinks absolute values coefficients, often forcing exactly equal 0.\r\nRidge uses L2 norm penalty term, .e., \\(\\alpha = 0\\), shrinks coefficient magnitudes.\r\ngeneral, LASSO performs better Ridge relatively small number predictors substantial coefficients remaining predictors coefficients close equal zero.\r\nRidge performs better settings response depends many predictors, approximately equal importance.\r\nperspective interpretability, LASSO advantage producing sparser models reducing values predictors‚Äô coefficients exactly zero.Partial least squares another derivative OLS regression first performs projection extract latent variables capture much variability among predictors possible modeling response well.\r\nlinear regression fit preferably small number latent features projection.\r\nuse generalized partial least squares (GPLS) implementation Ding Gentleman¬†[224].\r\nsupport vector machine (SVM)¬†[202] learns linear nonlinear decision boundaries feature space separate classes.\r\ndecision boundary represented training observations difficult classify, .e., support vectors.\r\ngoal find maximum margin hyperplane separating hyperplane maximum margin support vectors.\r\ncase linear decision boundary exist, nonlinear SVM approaches can used apply called kernel trick transform original feature space new, higher-dimensional space linear hyperplane can found separate classes.artifical neural network (NNET) consists structure nodes connected directed edges.\r\nnode performs basic unit computation.\r\nNodes supplied data values passed via incoming edges nodes.\r\nedge holds weight controls impact node forwards values .\r\nmain goal NNET adjust weights edges relationship predictors response underlying data represented.\r\nNeural networks extract new useful features original predictors relevant classification.\r\ncombining interconnected nodes complex predictive features, NNETs capable extracting classification-relevant feature sets compared expert-driven feature engineering dimension reduction techniques.\r\nNNETs undergone widespread adoption last decade led various success stories computer vision natural language processing¬†[183]. used feed-forward NNET one intermediary layer (hidden unit)¬†[227].Weighted k-nearest neighbor¬†[229] (WKNN) variant KNN classification.\r\nclassify observation unknown response value, k nearest training observations identified modus response values taken prediction.\r\nProximity observations quantified distance measure Euclidean distance.\r\nWhereas ordinary KNN neighbors equal influence prediction, weighted KNN takes account actual distance magnitudes.\r\nresult, WKNN assigns weights training observations inversely proportional distance observation classified.Na√Øve Bayes classifier (NB) uses Bayes‚Äô theorem calculate class membership probabilities.\r\n\r\nnaive property refers assumption class-conditional independence among predictors, employed reduce computational complexity obtain reliable class-conditional probability estimates.Classification regression trees¬†[232] (CART),\r\nC5.0¬†[208],\r\nrandom forests¬†[180] (RF) \r\ngradient boosted trees (GBT)¬†[85] tree-based models.\r\nAlgorithm model family partition predictor space set non-overlapping hyperrectangles based combinations predictor-value conditions, ‚Äúage \\(>\\) 52 & body-mass index \\(<\\) 25.‚Äù\r\nnew observation classified based majority class training data associated hyperrectangle belongs.\r\nRandom forests gradient boosted trees ensembles several different decision trees, tree casting vote final prediction.\r\nrandom forest, base trees created independently.\r\ngradient boosted model, base trees constructed added composite model way new tree reduces error current set trees.","code":""},{"path":"iml.html","id":"classifier-evaluation-and-hyperparameter-tuning","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.4 Classifier Evaluation and Hyperparameter Tuning","text":"use 10-fold stratified cross-validation (CV) classifier evaluation.\r\nk-fold CV, observations split k disjunct partitions.\r\npartition serves test set model trained remainder partitions.\r\nk performance estimates aggregated obtain overall performance score.\r\nperformed grid search hyperparameter selection (cf.¬†Table¬†8.2).\r\nthree applications dichotomous responses different skew, accuracy might inappropriate estimate generalization performance.\r\nInstead, used area receiver operating characteristic curve (AUC) performance measure.\r\nreceiver operating characteristic curve (ROC) shows relationship sensitivity (true positive rate (TPR)) false positive rate (FPR) binary classifier.\r\narea ROC curve (AUC) takes values 0 (0% TPR, 100% FPR) 1 (100% TPR, 0%FPR).\r\nhigher AUC suggests classifier better separating classes.Table 8.2: Overview hyperparameter tuning grid. classifiers implemented statistical programming language R [234] using package mlr [237], provides uniform interface listed machine learning algorithms R packages. grid search used tune hyperparameters using area ROC curve (AUC) evaluation measure. table provides overview classifier, including R package used, tuned hyperparameters value ranges. hyperparameters set default values. * = {linear, polynomial, radial, sigmoid}","code":""},{"path":"iml.html","id":"iterative-feature-elimination","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.5 Iterative Feature Elimination","text":"developed feature selection wrapper successively eliminates subset predictors positively contribute performance model.\r\ncontribution predictor computed using model reliance¬†[255], generalization random forest permutation feature importance¬†[180].\r\nModel reliance estimates merit predictor \\(f\\) toward model \\(\\zeta\\) comparing classification error \\(\\zeta\\) original training set \\(\\mathbf{X}_{orig}\\) classification error \\(\\zeta\\) modified version training set \\(\\mathbf{X}_{perm}\\) values \\(f\\) randomly permuted.\r\nparticular, model reliance \\(MR\\) model \\(\\zeta\\) predictor \\(f\\F\\) calculated \\[\\begin{equation}\r\nMR(f,\\zeta) = \\frac{CE(y,\\zeta(\\mathbf{X}_{perm}))}{CE(y,\\zeta(\\mathbf{X}_{orig}))}\r\n\\tag{8.6}\r\n\\end{equation}\\]\\(CE\\) classification error function takes true class labels \\(y\\) vector predicted class labels, returns fraction incorrectly classified observations.\r\nhigh MR score represents high dependence model \\(f\\), since shuffling values \\(f\\) increases classification error.\r\nConversely, \\(MR\\) score smaller 1 suggests \\(f\\) potentially adversarial model performance, removal increase model performance. Thus, feature elimination wrapper starts training model full set predictors, followed iterative step subset adversarial predictors according model reliance removed, new model remaining predictors trained.\r\nfirst iteration \\(=1\\), initial model \\(\\zeta_1\\) calculated full set predictors \\(F_1 = F\\).\r\npredictor \\(f \\F_i\\), model reliance \\(MR(f,\\zeta_i)\\) calculated.\r\nPredictors \\(f\\F_i:MR(f,\\zeta_i)>1\\) kept iteration \\(+1\\) remaining predictors removed.\r\nprocedure repeated either \\(MR\\) smaller equal 1, .e., \\(\\forall f \\F_i: MR(f,\\zeta_i) \\leq 1\\), \\(F_{+1}=F_i\\).\r\nrandom feature permutation introduces statistical variability, compute mean \\(MR\\) 10 runs obtain stable estimate.","code":""},{"path":"iml.html","id":"post-hoc-interpretation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.6 Post-Hoc Interpretation","text":"SHAP. facilitate model interpretation, model-agnostic post-hoc framework SHAP¬†[195], [199] used assess feature importance CHA data.\r\nBriefly, SHAP value \\(\\phi_f(\\zeta,x)\\) expresses estimated importance feature \\(f\\) prediction model \\(\\zeta\\) instance \\(x\\) change expected value prediction \\(f\\) feature vector \\(x\\) observed instead random.\r\nSHAP framework composes model prediction sum SHAP values feature, .e., \\(\\zeta(x)=\\phi_0(\\zeta,x)+\\sum_{=1}^M \\phi_i(\\zeta,x)\\), \\(\\phi_0(\\zeta,x)\\) expected value model (bias) \\(M\\) number features.SHAP values calculated best model \\(\\zeta_{opt}\\) according AUC.\r\nranking T0 feature attribution towards \\(\\zeta_{opt}\\) determined calculating average SHAP value magnitude instances, .e., \\((j)=\\sum_{=1}^N |\\phi_j(\\zeta_{opt},x)|\\),\r\n\\((j)\\) attribution \\(j\\)-th feature.\r\n\\(N\\times M\\) SHAP matrix clustered agglomerative hierarchical clustering identify subgroups patients similar SHAP values.PDP feature importance. derive global feature importance PD predictor.\r\nassumption predictors high PD variability important.\r\nexample, consider two PD curves Figure 8.1¬†(c): predicted response changes considerably different predictor values blue PD curve whereas green PD curve basically flat line.\r\nTherefore, predictor blue PD curve higher importance score predictor green PD curve.\r\ndefine partial dependence importance \\(\\) predictor \\(f\\) average magnitude differences consecutive values along distribution \\(f\\), .e.,\\[\\begin{equation}\r\nI_f = \\frac{1}{k-1}\\sum_{}^{k-1} |PD(Q=s_i) - PD(Q=s_{+1})|\r\n\\tag{8.7}\r\n\\end{equation}\\]\\(k\\) number (sampled) values distribution \\(f\\).\r\n\r\nFigure 8.5: Illustration partial dependence importance. Partial dependence importance \\(I_f\\) 2 toy PD curves.\r\n","code":""},{"path":"iml.html","id":"iml-results","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3 Results","text":"section, report results three classification tasks, .e., regarding CHA-Tinnitus (Section¬†8.3.1),\r\nCHA-Depression (Section¬†8.3.2) \r\nAneurD (Section¬†8.3.3).","code":""},{"path":"iml.html","id":"iml-results-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.1 Results for CHA-Tinnitus","text":"Correlational analysis. Figure 8.6¬†() shows pairwise correlations among predictors T0.\r\nidentified two major subgroups moderate high intra-group correlations low negative inter-group correlations.\r\nlarger group (cf.¬†upper black square Figure 8.6¬†()) comprises 114 predictors (ca. 55.6%) representing negatively worded items scores higher values represent higher disease burden, e.g.¬†ADSL_depression BI_overallcomplaints.\r\nConsequently, smaller group (cf.¬†lower black square Figure 8.6¬†()) contains 47 predictors (ca. 22.9%) positive wording, e.g.¬†SF8 mental health score (SF8_mental) BSF elevated mood score (BSF_mood).\r\nPredictors one two subgroups exhibit moderate high negative correlation predictors subgroup.\r\n\r\n\r\nFigure 8.6¬†(b) compares correlation predictors TQ_distress (x-axis) treatment (y-axis).\r\nOverall, low moderate bivariate correlations observed, values -0.6 +0.6.\r\naverage change absolute correlation T0 T1 0.031.\r\nchange absolute correlation smaller 0.067 ca. 95% predictors (compare distance points diagonal line Figure 8.6¬†(b)).\r\n137 205 predictors (66.8%), absolute correlation decreased T0 T1.\r\nMedian target-correlation questionnaires ADSL, BSF BI (SF8) greater (smaller) +0.3 (-0.3) moments, respectively, thus greater remaining questionnaires.\r\nFigures 8.6¬†(c) (d) reveal predictors ADSL, BSF, BI, SF8, TINSKAL PSQ among top-20 predictors ranked absolute correlation TQ_distress T0 T1.\r\ngeneral depression score ADSL_depression shows largest correlation magnitude (\\(\\rho\\) = 0.630) treatment (\\(\\rho\\) = 0.564).\r\nFigure 8.6¬†(e) shows 10 predictors largest differences correlation magnitudes T0 T1.\r\nCorrelation treatment larger predictors.\r\nFigure 8.6: Spearman correlation among predictors correlation predictors TQ_distress T0 T1. () heatmap depicts correlation coefficients pairs predictors T0. Predictors arranged result agglomerative hierarchical clustering complete linkage. two black squares depict two major subgroups correlated predictors. (b) relationship predictor TQ_distress T0 (x-axis) T1 (y-axis). diamond symbol represents median correlation predictors questionnaire. (c) Top-20 predictors exhibit highest absolute correlation TQ_distress T0. (d) Top-20 predictors exhibit highest absolute correlation TQ_distress T1. (e) Top-10 predictors highest change absolute correlation TQ_distress T0 T1.\r\nPredictive performance classification models. performances 11 classifiers across feature elimination iterations shown Figure¬†8.7.\r\ngradient boosted trees model (GBT) yields highest AUC (iteration = 7, AUC = 0.890 \\(\\pm\\) 0.04; mean\\(\\pm\\)SD), using 26 predictors (ca. 13%).\r\n\r\nRIDGE classifier achievs second-best performance (=2, AUC: 0.876 \\(\\pm\\) 0.05), relying 127 features, followed random forest model (=3, AUC: 0.872 \\(\\pm\\) 0.05) using 77 features.\r\nClassification using best model (GBT, =7) based probability threshold 0.5 resulted accuracy 0.86, true positive rate (sensitivity) 0.72, true negative rate (specificity) 0.88, precision 0.48 negative predictive value 0.95.\r\nFigure 8.7: Classification results CHA-Tinnitus. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier‚Äôs run maximum AUC. Classifiers ordered maximum AUC left right.\r\ntrained using smaller feature space, classifier produce least one model similar even improved performance compared respective model learned whole set predictors.\r\nfact, exception WKNN, classification methods benefit feature elimination produce best model predictor subset (cf.¬†Figure¬†8.7).\r\nGBT, gain AUC 185 features 26 features (= 11) 0.01.\r\nmodel achieves maximum AUC good trade-high predictive performance low model complexity, thus decided investigate model.Feature importance. best model, attributions 26 selected features shown Figure 8.8¬†().\r\nAmong 26 features 6 scores, 12 single items, 4 demographic features (number visited doctors, university-level education, lower secondary education, tinnitus duration) 4 features measuring average time spent completing item.\r\nTINSKAL tinnitus impairment score (TINSKAL_impairment) represents predictor highest model attribution exhibits highest average absolute SHAP value (change log odds) 0.448.\r\nADSL depressivity score (ADSL_depression) single question ADSL (ADSL_adsl11: ‚Äúpast week sleep restless.‚Äù) ranked second third important, respectively.\r\nRemarkably, 9 questionnaires least 1 feature selected.\r\nFigure 8.8¬†(b) shows patient-individual SHAP values predictor point color depicts predictor value magnitude.\r\nhigh attribution TINSKAL_impairment highlighted wide spread SHAP value distribution.\r\npredictor, high values generally correspond increased predicted probability tinnitus decompensation.\r\nHowever, trend non-linear, since small values (light green yellow) associated SHAP value just slightly smaller equal 0.\r\nMoreover, large spread SHAP value ca. 0.7 1.2 patients high TINSKAL_impairment values opposed somewhat dense bulk points representing patients SHAP values ca. -0.7 -0.4.\r\nindicate patients report high tinnitus impairment difficult classify.\r\n, may suggest visual analog scales robust enough quantify tinnitus-related distress.\r\ninference supported SHAP feature dependence plot Figure 8.9¬†(1) juxtaposes actual values predictor corresponding SHAP values patients reveals J-shaped relationship .\r\nspecifically, predicted tinnitus-related distress decreasing 0 2.5, remains plateau 2.5 4 increasing 4 maximum value 10.\r\nBesides TINSKAL_impairment, features ADSL_depression, TINSKAL_loudness, BI_overallcomplaints, BSF_timestamp SWOP_pessimism also showed non-linear relationship respect SHAP values.\r\nFigure 8.8: SHAP analysis results best model (GBT, feature elimination iteration = 7). () Global feature importance based mean absolute SHAP magnitude observations. Values depict absolute change log odds higher values indicate higher feature attribution towards model. (b) Patient-individual SHAP values. Points represents SHAP value predictor (y-axis) individual patient. afar point vertical 0-baseline, larger attribution corresponding predictor value model prediction. Vertically offset points depict regions high density (similar violin plot), .e., greater number patients similar SHAP values. Actual predictor values mapped point color. (c) Stacked patient-individual SHAP values 6 predictors highest mean absolute SHAP values. Patients ordered according hierarchical clustering Ward linkage. Black horizontal lines depict average sum SHAP values cluster members k = 5 clusters. inset plot shows Bayesian information criterion minimal number clusters.\r\n\r\nFigure 8.9: SHAP feature dependence. relationship actual values predictor (x-axis) corresponding SHAP values (y-axis) shown points representing patient locally weighted scatterplot smoothing (LOWESS) curves indicating overall trend. Predictors ordered mean absolute SHAP value (see Figure¬†8.8¬†()). Gray histograms bar charts depict distributions predictors.\r\nEven though several predictors exhibit low moderate global importance, high attribution towards model prediction specific subgroups.\r\nexample, considering SOZK_lowersec, patients lower secondary education average SHAP value +0.5, whereas patient different education levels average SHAP value -0.1 hence close population average (cf.¬†Figure 8.8¬†(b), Figure 8.9¬†(13)).\r\nfeatures show monotonic relationship actual values SHAP values.\r\nexample, increasing values SF8 physical component score (SF8_physicalcomp) exhibit decreasing likelihood predicted decompensated tinnitus increasing physical health (cf.¬†Figure 8.8¬†(b), Figure 8.9¬†(14)).investigate whether subgroups patients similar respect model prediction can explained , clustered patients based SHAP values.\r\nFigure 8.8¬†(c) shows stacked patient-individual SHAP values six predictors highest average absolute SHAP values remaining predictors combined.\r\nAccording Bayesian information criterion (cf.¬†insetted plot Figure 8.8¬†(c)), optimal number patients clusters similar SHAP value patterns 5.\r\nClusters 1 5 comprise subgroups sum SHAP values predictors positive, see horizontal lines Figure 8.8¬†(c).\r\nHence, patients likely predicted decompensated tinnitus.\r\ncomparison subgroups, patients clusters 1 5 reported higher degrees tinnitus impairment, depressivity, anxiety, tinnitus loudness, sleeplessness, pessimism, psychosomatic complaints well perceived levels stress social isolation.\r\ngeneral, patients cluster 1 slightly higher values across predictors patients cluster 2.\r\naddition, cluster 1 contains higher fraction patients lower secondary education (‚ÄúHauptschule‚Äù), report frequently occuring headaches, higher levels fears future longer tinnitus duration.\r\nCluster 3 largest subgroup comprising 39.6% patients.\r\nTogether cluster 2, subgroups lowest predicted probability tinnitus decomposition.\r\nPatients cluster 2 3 report highest physical health levels determination.\r\nCluster 4 somewhat close prediction average positive negative SHAP values nearly even .\r\nrespect average patient-sum SHAP values, cluster 3 lies cluster 2 cluster 4.\r\n","code":""},{"path":"iml.html","id":"iml-results-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.2 Results for CHA-Depression","text":"Predictive performance classification models.\r\nFigure¬†8.10 depicts performance classification methods across iterations.\r\nLASSO classifier achieved maximum AUC classification algorithms (iteration = 1, AUC = 0.867 \\(\\pm\\) 0.037; mean \\(\\pm\\) SD), followed Ridge (= 1, AUC = 0.864 \\(\\pm\\) 0.040) GBT (= 1, AUC = 0.862 \\(\\pm\\) 0.038).\r\nconsidering best model per classifier, models similar performance, ranging AUC 0.809 (C5.0) 0.867 (LASSO).\r\nFigure 8.10: Classification results CHA-ADSL_depression. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier‚Äôs run maximum AUC. Classifiers ordered maximum AUC left right.\r\nbest model (Lasso, = 1) achieves accuracy 79%, true positive rate (sensitivity) 61%, true negative rate (specificity) 88%, precision 72% negative predictive value 82% based probability threshold 0.5.\r\nfinal model includes 40 predictors non-zero coefficients.\r\nFigure¬†8.11 shows median model coefficient features across 10 cross-validation folds.\r\nADSL questionnaire alone, 16 single items included final model, including indicators depressivity (ADSL_adsl09, ADSL_adsl18, ADSL_adsl12) perceived antipathy received people (ADSL_adsl19), sleeplessness (ADSL_adsl11), dejectedness (ADSL_adsl03), lack appetite (ADSL_adsl02), confusion (ADSL_adsl05), anxiety (ADSL_adsl10, ADSL_adsl08), absence self-respect (ADSL_adsl04, ADSL_adsl09), lack vitality (ADSL_adsl09, ADSL_adsl09), taciturnity (ADSL_adsl13) irritability (ADSL_adsl01).\r\nThus, questionnaire contributed highest number predictors model. tinnitus-distress-oriented TQ, 5 predictors selected.\r\n, model used 5 predictors socio-demographics questionnaire (SOZK), including German nationality (SOZK_nationality) highest absolute model coefficient, university level graduation (SOZK_graduate), tinnitus duration (SOZK_tinnitusdur), employment status (SOZK_job), marital status (SOZK_unmarried) partnership status (SOZK_partnership).\r\nFigure 8.11: Coefficients LASSO model. Cross-validation (CV) median (points) \\(\\pm\\) median absolute deviation (line ranges) coefficients best LASSO model (= 1). frequency non-zero coefficients 10-fold CV given parentheses right predictor name. 185 features total, 40 features exhibit non-zero model coefficient least one CV fold.\r\nTable¬†8.3 provides description predictors Figure¬†8.11.Effect feature elimination classification performance. classifiers SVM show high stability performance smaller feature subsets.\r\nFigure¬†8.10, see LASSO difference AUC trained 185 features (= 1) vs.¬†trained 6 features (= 7) -0.017.\r\nSeveral classifiers benefited feature selection terms predictive performance.\r\nGPLS, NNET, CART, C5.0 RF, max. AUC achieved feature subset.\r\ndecision tree variants CART C5.0 gain performance feature removal, since respective max. AUC obtained smallest predictor subset, cardinality 22 10, respectively.\r\nTable 8.3: important features LASSO model. Predictors highest absolute coefficient final LASSO model (iteration = 1). 185 predictors total, 40 predictors exhibit non-zero model coefficient least one ten cross-validation folds.\r\n","code":""},{"path":"iml.html","id":"iml-results-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.3 Results for AneurD","text":"Figure¬†8.12 shows classification results data subset.\r\nGBT achieves maximum AUC (cross-validation average 67.2% \\(\\pm\\) 1.8% standard deviation), followed C5.0 (AUC 64.6% \\(\\pm\\) 1.9%) GPLS (AUC 63.3% \\(\\pm\\) 1.2%).\r\nsubset SW, SVM comes best 75.2% \\(\\pm\\) 5.7% AUC, slightly superior GPLS (AUC 73.6% \\(\\pm\\) 4.4%) NNET (AUC 71.6% \\(\\pm\\) 5.5%).\r\nBF, WKNN yields best (AUC 64.0% \\(\\pm\\) 1.1%) model, GPLS (AUC 62.9% \\(\\pm\\) 2.6%) RF (AUC 62.7% \\(\\pm\\) 2.3%) similar yet slightly inferior generalization performances.\r\nresults indicate classifiers yield better performance subset sidewall aneurysms.\r\nOverall, none classification algorithms outperforms others across three subsets.\r\nFigure 8.12: Classification results AneurD. combination data subset classification algorithm, performance run preprocessing transformation achieves highest AUC shown. SW = sidewall; BF = bifurcation.\r\nrespect PD importance, Figure¬†8.13 illustrates high attribution angle parameter \\(\\gamma\\) towards rupture status classification, feature ranked first third best models BF.\r\nSVM model trained SW subset, ellipticity index (EI) important.\r\nFigure 8.13: Relative PD importance (AneurD). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\nFigure¬†8.14 shows PDP ICE curves important predictors according \\(I_f\\) best models data subset.\r\nICE curves GBT model SVM model (Figure 8.14¬†() ¬†(b)) exhibit nearly identical trend different intercept.\r\ncontrast, ICE curves WKNN (Figure8.14¬†(c)) appear jittery.\r\nApparently, plots summarize major characteristics different model families.\r\nspecifically, GBT model (Figure 8.14¬†()) produces jagged curves distinct vertical cuts, representing splits base decision trees tree ensemble.\r\nexample, plot \\(\\gamma\\) shows 4 splits {16.54, 49.26, 54.42, 64.35}.\r\nSVM classifier (Figure 8.14¬†(b)) linear model, hence ICE PD curves just lines fixed slope.\r\nmarginal model posterior aneurysm rupture increases higher values ellipticity index, max. width aneurysm body, aspect ratio \\(H_{ortho}/N_{avg}\\) max. aneurysm diameter, whereas area ostium (variant 2), lower values indicative high rupture likelihood.\r\nWKNN, PDP better able clearly show marginal posteriors individual predictors ICE curves.\r\ndue property WKNN ‚Äúlazy‚Äù learner produce actual model makes predictions based observation-individual similarity.\r\nFigure 8.14: Relative PD importance (AneurD). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\n","code":""},{"path":"iml.html","id":"iml-discussion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4 Discussion","text":"section, discuss findings respect three classification tasks, .e., regarding\r\nCHA-Depression (Section¬†8.4.1), CHA-Tinnitus (Section¬†8.4.2) \r\nAneurD (Section¬†8.4.3).","code":""},{"path":"iml.html","id":"iml-discussion-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.1 CHA-Depression","text":"Machine learning used build predictive models depression severity based structured patient interviews¬†[257], [259].\r\nrefrain quantitative comparison studies due differences population characteristics measurements.\r\ngood best models actually?\r\nreasonable baseline classifier simply predicts depression status time T0, yields 79% accuracy.\r\nmodels outperform baseline, although likely provide good fit sample, patient subgroups centers yet studied.\r\nHowever, models promising first step supporting timely prediction depression severity selection appropriate treatment questionnaire items.Consistent previous studies¬†[261], found strong association tinnitus distress depression severity.\r\nFurthermore, predictors measuring perceived stress demands found significant contributor depression tinnitus patients¬†[263].\r\nfact predictors selected different questionnaires confirms multifactoriality depression, whose assessment requires inclusion different measurements.\r\nTherefore, concomitant emotional symptoms comorbidities must taken account meet patient-specific needs.\r\nprevious study¬†[265], high sensitivity detecting depression achieved using two-item questionnaire.\r\nOne two items ‚Äúpast month, often bothered feeling , depressed, hopeless.‚Äù;[265] similar ADSL_adsl06 (‚Äúpast week, felt depressed.‚Äù), second largest absolute coefficient best LASSO model.Generally, care must taken interpreting model coefficients:\r\nexample, identified strong relationship non-German citizenship depression severity (cf.¬†Figure¬†8.11 Table¬†8.3).\r\nAlthough studies reported ethnic differences depression¬†[267], [269], occurrence item tends suggest higher perceived social stress patients predominantly Turkish origin, due higher unemployment rates, larger families, poorer housing conditions, etc. demographic group.\r\n5.0% cohort population non-German citizens, results also effect overfitting.\r\nSince associated predictor first iteration feature elimination model reliance score less 1.0, consequently omitted sparser models.Regarding stability models smaller feature sets, results show simpler models slightly inferior predictive model.\r\nspecifically, classification methods show improvement AUC number predictors decreases.\r\nfact, 5 11 classifiers improved even feature selection, .e., AUC second later iteration superior AUC first iteration (205 predictors used).\r\nexample, two decision tree variants achieved highest performance smallest feature subset case.\r\nRegarding LASSO classifier, showed best performance, encouraging 6 predictors 4 questionnaires showed similar performance (AUC = 0.850) compared best overall model (AUC = 0.867).\r\nnoteworthy neither predictors tinnitus localization quality sociodemographic predictors included model.\r\nfinding used reduce number questions entire questionnaires patients must answer treatment.\r\npsychological physical stress subject undergoing examination (e.g., painful biopsy vs.¬†blood test).\r\nexample, Yu et al.¬†[270] perform feature selection budget, cost feature acquisition derived suggestions medical experts based total financial burden, patient privacy, patient inconvenience.\r\nKachuee et al.¬†[271] derive feature costs based convenience answering questions, performing medical exams, blood urine tests.terms clinical relevance, results first step guide clinicians making treatment decisions regarding clinical depression patients chronic tinnitus.\r\nmodels used design appropriate treatment pathway.\r\nHowever, using models practice, one must aware trained cross-sectional data, .e., models separate subclinical clinical depression based questionnaire responses sociodemographic data treatment.\r\nAlso, one must keep mind treatment 7-day treatment response depression status treatment.also limitations approach.\r\nFirst, models might subject selection bias patients complete seven questionnaires admission treatment excluded analyses.\r\nHowever, consider data ‚Äúmissing values‚Äù lead problematic suggestion using imputation methods.\r\nuse imputation () proportion patients complete entire questionnaire (rather individual items) (ii) know whether data missing random.\r\nHowever, number patients large, believe results sufficiently robust.\r\nfuture work, investigate possible systematic differences included excluded patients.\r\nexclusion patients dropped completing questionnaires prematurely, partly gradual loss motivation, technical unfamiliarity computer, possible interruptions staff complete baseline assessments, lead selection bias.\r\npatient population one hospital, future work involves external validation models data different populations hospitals.\r\nuse cross-sectional data limits interpretation prediction depression severity beyond end therapy, future work need validate models longitudinal data.Another potential limitation greedy process iterative feature selection wrapper, can miss global optima result.\r\niteration, predictors prevent model classifying correctly removed feature set.\r\npredictor eliminated, included subsequent iteration.\r\nHowever, possible including predictor removed early iteration lead better model later iteration.\r\npossible solution mechanism backtrack revisit earlier iterations turns removed predictors actually contributed positively model performance.\r\nAlternatively, \\(MR\\) cutoff value discarding features (set 1 experiments) subjected hyperparameter tuning.\r\nFuture work therefore includes comparison feature selection algorithms.","code":""},{"path":"iml.html","id":"iml-discussion-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.2 CHA-Tinnitus","text":"trained classification models predict tinnitus-related distress multimodal treatment (T1) patients chronic tinnitus based self-report questionnaires data acquired treatment (T0).\r\ngradient boosted trees model uses 26 (12.7%) total 205 predictors separates patients ‚Äúcompensated‚Äù vs.¬†‚Äúdecompensated‚Äù tinnitus best AUC.\r\n\r\nAmong features measurements describe variety psychological psychosomatic patient characteristics well socio-demographics therefore confirming multi-factorial nature tinnitus-related distress.\r\ncharacteristics used phenotyping Chapter¬†5.\r\nAdditionally, predictors can investigated followup studies characteristics influence treatment success.\r\n\r\nexpected, predictors directly linked tinnitus quality show high model attribution, degree perceived tinnitus impairment loudness.\r\ntime, depression, attitudinal factors (self-efficacy, pessimism, complaint tendency), sleep problems, educational level, tinnitus location duration emerged highly important model prediction well.Quantitative predictors, tinnitus impairment loudness, show non-monotonic relationships respect predicted outcome.\r\nNotably, low self-reported impairment loudness measured visual analogue scales generally indicate low tinnitus-related distress measured TQ.\r\nOne explanation simple measurements like TINSKAL_impairment TINSKAL_loudness less robust show higher variability compound scale combines multiple single questionnaire items.\r\nfindings investigated , e.g., whether relationship towards subgroup patients fatigued thus less thoroughly filling large number questionnaires.results confirm intricate interplay depression tinnitus-related distress elucidated numerous previous studies¬†[273], [275], [277], [280], [282].\r\nbest model, ADSL score 20 associated increased predicted risk tinnitus decompensation (cf.¬†Figure 8.9¬†(2)) close cutoff clinical relevance depression¬†[48].context parsimonious learning, general strategy determine set predictors small possible inclusion predictor yield considerable improvement performance.\r\nmany predictors really necessary accurate tinnitus distress prediction?\r\nFigure 8.15¬†() illustrates change performance GBT classifier predictors iteratively added feature space order SHAP values respect best model.\r\nmodel uses TINSKAL_impairment achieves AUC = 0.79 \\(\\pm\\) 0.06.\r\nAdding ADSL_depression leads improvement AUC 0.06.\r\nHowever, none remaining 24 predictors results improvement 0.01, respectively.\r\nMoreover, 3 predictors necessary model AUC = 0.85, 8 predictors model AUC = 0.87 15 predictors model AUC = 0.89 (cf.¬†Figure 8.15¬†()).\r\nFigure 8.15: Cumulative feature contribution & correlation network. () Cross-validation AUC (average \\(\\pm\\) standard deviation) GBT model trained feature subset comprising predictors denoted y-axis iteration. ordering features according mean absolute SHAP value (cf.¬†Figure 8.8~()). (b) Network illustrating 3 groups features among 26 selected predictors best model high intra-group correlation (\\(|\\rho| \\geq\\) 0.5). 8 predictors (predominantly SOZK) without moderate high pairwise correlation shown.\r\nOne potential explanation multicollinearity among groups predictors.\r\nFigure 8.15¬†(b) shows network 3 predictor groups among 26 features best model.\r\nexample, features TINSKAL_impairment TINSKAL_loudness moderately correlated (Spearman correlation \\(\\rho\\) = 0.69), raises question whether one two predictors removed without considerable loss AUC.\r\nlargest subgroup spanning 14 features involves descriptors depression, perceived stress reported physical health.\r\nfuture work, investigation possible interaction effects among moderately strongly correlated features investigated, better understand selected determine whether removed achieve better trade-model accuracy complexity.workflow leverages potential machine learning identifying key predictors variety features collected treatment post-treatment tinnitus compensation, ensuring every potential predictor included analysis, internal validation classification models using cross-validation hyperparameter tuning.\r\nFurthermore, selecting variety classification algorithm families, linear nonlinear relationships feature outcome identified.\r\nlimitation hypothesis-free approach learned models contain features quantify similar patient characteristics.\r\nexample, best model study included two highly correlated features ADSL_depression BSF_anx_depression (anxious depressiveness score).\r\ninclusion features contributed model performance, medical perspective, predictive model certain features might beneficial. Preselecting features avoid multicollinearity direction future work.Finally, exclusion 2,701 4,117 patients (65.6%) complete 10 questionnaires resulted selection bias.\r\nMany patients spent one hour completing questionnaire dedicated minicomputer therefore likely drop completion process.\r\n\r\nCompleters slightly younger non-completers (mean age 49.8 \\(\\pm\\) 12.2 vs.¬†51.7 \\(\\pm\\) 13.8), likely highest German school degree ‚ÄúAbitur‚Äù (48.2% vs.¬†42.0%) suffering tinnitus longer (\\(>\\) 5 years: 33.3% vs.¬†25.1%).\r\n\r\n\r\n\r\nfuture work intend investigate extent insights completers can used subsamples non-completers.\r\nTherefore, can use DIVA framework Hielscher et al.¬†[283].\r\nHowever, psychological treatment approaches likely benefit report psychological problems prior tinnitus perception association tinnitus perception.","code":""},{"path":"iml.html","id":"iml-discussion-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.3 AneurD","text":"classification results promising, morphological parameters alone can provide models moderate power.\r\nprevious studies found hemodynamic parameters also predictive¬†[284], [285], future work includes exploring potential combining morphologic hemodynamic features classification rupture status.\r\naddition, focus now quantifying merit morphologic parameters, ignored demographic characteristics, age sex, also correlate strongly aneurysm rupture¬†[286].\r\nexpect adding patient characteristics improve classification performance.\r\nPDP analysis showed best model (gradient boosted trees), parameters angle dome point \\(\\gamma\\), ellipticity index \\(EI\\), maximum aneurysm width \\(W_{max}\\), nonsphericity index \\(NSI\\), aneurysm area \\(A_{O2}\\) highest attribution (see Figure 8.13¬†()).\r\ndiffered found two subsets sidewall bifurcation aneurysms, respectively.\r\nFigure¬†8.14 shows none features appear among top 5 predictors sidewall aneurysms, bifurcation aneurysms, overall data set.\r\nalso partly due fact family best model different subsets.\r\nConsequently, Figure¬†8.16 shows PDP curves top 5 features differ substantially.\r\nTherefore, argue PDPs appropriate intra-model comparisons feature attributions.\r\nFigure 8.16: PDP curves top-5 predictors - GBT best models data subset.\r\nobserved classification performance consistently higher subset sidewall aneurysms vs.¬†bifurcation aneurysms, different parameters found high model attribution.\r\npartially due rather small sample size already mentioned differences model families.\r\nHowever, Baharoglu et al.¬†[22] identified significant differences sidewall bifurcation aneurysms respect morphological parameters, parameters can predict rupture status.findings also suggest form higher-level interactions groups features.\r\nexample, ellipticity index (\\(EI\\))found second important - GBT important SW - SVM, although differences \\(EI\\) unruptured ruptured aneurysms significant (p = 0.323, Wilcoxon rank sum test, \\(\\alpha\\) = 0.01).limitations analysis.\r\nsmall sample size, especially subset sidewall aneurysms (N=24), lead overfitting.\r\nfuture work, like retrain models larger number datasets, incorporate wider variety predictors, hemodynamic demographic features, mentioned .\r\nlimitation concerns validity class label.\r\nSamples labeled unruptured ruptured later moment.\r\n, like investigate samples high classification error detail.\r\n, goal derive descriptions aneurysms subgroups hard classify, order better understand reasons misclassification, signalize medical expert manual diagnosis necessary.","code":""},{"path":"iml.html","id":"iml-conclusion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.5 Conclusion","text":"medical applications, black-box models becoming increasingly popular due high predictive power.\r\nHowever, due opacity, post-modeling step required extract actionable insights .\r\npresented machine learning workflow classification post-hoc interpretation alongside dataset-specific steps three medical applications.\r\nvariety classification algorithms created identical datasets, chose interpretation method based opacity model family achieves maximum performance, dimensionality.\r\nCHA-tinnitus, gradient-boosted trees performed best, used Shapely value explanations obtain feature importance values model, subpopulation, observation levels.\r\nCHA depression, LASSO found achieve best generalization performance, used intrinsically interpretable model coefficients.\r\nAneurD, used PD importance instead SHAP values, although gradient-boosted trees provide best model number observations small produce reliable importance scores subpopulation observation level.TODO: discussion robustness","code":""},{"path":"gender.html","id":"gender","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","text":"chapter partly based :Uli Niemann, Benjamin Boecking, Petra Brueggemann, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúGender-Specific Differences Patients Chronic Tinnitus ‚Äì Baseline Characteristics Treatment Effects.‚Äù : Frontiers Neuroscience 14 (2020), p.¬†487. DOI: 10.3389/fnins.2020.00487.","code":""},{"path":"gender.html","id":"brief-chapter-summary-6","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"Brief Chapter Summary","text":"present workflow examine subpopulations differ respect predictive characteristics temporal data. , derive post-hoc interpretation measure assess difference association predictors two subpopulations. report results CHA gender differences (subpopulations female male patients) two outcomes tinnitus-related distress depression, effect treatment outcomes.","code":""},{"path":"gender.html","id":"gender-intro","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.1 Motivation and Comparison to Related Work","text":"written","code":""},{"path":"gender.html","id":"gender-measure","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.2 Comparing Differences in Feature Importance between Two Subpopulations","text":"written","code":""},{"path":"gender.html","id":"gender-workflow","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3 Workflow","text":"","code":""},{"path":"gender.html","id":"learning-tasks","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3.1 Learning Tasks","text":"written","code":""},{"path":"gender.html","id":"model-evaluation-and-hyperparameter-tuning","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3.2 Model Evaluation and Hyperparameter Tuning","text":"","code":""},{"path":"gender.html","id":"gender-results","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.4 Results","text":"written","code":""},{"path":"gender.html","id":"gender-conclusions","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.5 Conclusions on Subpopulation-Specific Differences in Feature Importance","text":"written","code":""},{"path":"summary.html","id":"summary","chapter":"10 Conclusion and Future Work","heading":"10 Conclusion and Future Work","text":"written","code":""},{"path":"summary.html","id":"summary-results","chapter":"10 Conclusion and Future Work","heading":"10.1 Research Results for Medical Expert-Guided Knowledge Discovery","text":"written","code":""},{"path":"summary.html","id":"summary-future-work","chapter":"10 Conclusion and Future Work","heading":"10.2 Future Work","text":"written","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"appx-pheno.html","id":"appx-pheno","chapter":"A Variables selected for phenotyping","heading":"A Variables selected for phenotyping","text":"ACSA_qualityoflife*: Quality life last 2 weeksADSL_depression: Depressive disorder sum scoreBI_abdominalsymptoms: Abdominal symptoms scoreBI_fatigue: Fatigue scoreBI_heartsymptoms: Heart symptoms scoreBI_limbpain: Limb pain scoreBI_overallcomplaints: Overall complaints sum scoreBSF_anger: Anger scoreBSF_anxdepression: Anxious depression scoreBSF_apathy: Apathy scoreBSF_elevatedmood*: Elevated mood scoreBSF_fatigue: Fatigue scoreBSF_mindset*: Positive mindset scoreISR_additionalitems: Additional items scoreISR_anxiety: Anxiety scoreISR_compulsivesyn: Obsessive-compulsive syndrome scoreISR_depression: Depression scoreISR_eatingdisorder: Eating disorder scoreISR_somatosyn: Somatoform syndrome scoreISR_totalpsychiatricsyn: Total psychiatric syndrome scorePHQK_depression: Presence depressionPHQK_panicsyn: Presence panic syndromePSQ_demand: Demand scorePSQ_joy*: Joy scorePSQ_stress: Total perceived stress sum scorePSQ_tension: Tension scorePSQ_worries: Worries scoreSES_affectivepain: Affective painSES_sensoricpain: Sensoric painSF8_bodilyhealth*: Bodily health scoreSF8_mentalcomp*: Mental component summary scoreSF8_mentalhealth*: Mental health scoreSF8_overallhealth*: Overall health scoreSF8_physicalcomp*: Physical component summary scoreSF8_physicalfunct*: Physical functioning scoreSF8_roleemotional*: Role emotional scoreSF8_rolephysical*: Role physical scoreSF8_socialfunct*: Social functioning scoreSF8_vitality*: Vitality scoreSSKAL_painfrequency: Visual analog scale pain frequencySSKAL_painimpairment: Visual analog scale pain impairmentSSKAL_painseverity: Visual analog scale pain severitySWOP_optimism*: Optimism scoreSWOP_pessimism: Pessimism scoreSWOP_selfefficacy*: Self-efficacy scoreTINSKAL_frequency: Tinnitus frequencyTINSKAL_impairment: Tinnitus impairmentTINSKAL_loudness: Tinnitus loudnessTLQ_01_bothears: Tinnitus location: earsTLQ_01_entirehead: Tinnitus location: entire headTLQ_01_leftear: Tinnitus location: left earTLQ_01_rightear: Tinnitus location: right earTLQ_02_hissing: Tinnitus noise: hissingTLQ_02_ringing: Tinnitus noise: ringingTLQ_02_rustling: Tinnitus noise: rustlingTLQ_02_whistling: Tinnitus noise: whistlingTQ_auditoryperceptdiff: Auditory perceptual difficulties scoreTQ_cognitivedistress: Cognitive distress scoreTQ_distress: Total tinnitus distress scoreTQ_emodistress: Emotional distress scoreTQ_intrusiveness: Intrusiveness scoreTQ_psychodistress: Psychological distress scoreTQ_sleepdisturbances: Sleep disturbances scoreTQ_somacomplaints: Somatic complaints score","code":""}]

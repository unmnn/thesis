[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"HTML version PhD thesis titled “Intelligent Assistance Expert-Driven Subpopulation Discovery High-Dimensional Timestamped Medical Data”, submitted Otto von Guericke University Magdeburg Uli Niemann March 23, 2021.submitted PDF document can downloaded accompanying GitHub repository.","code":""},{"path":"index.html","id":"abstract","chapter":"Welcome!","heading":"Abstract","text":"Subpopulation discovery essential objective data analysis medical research contributes prevention treatment adverse medical conditions.\r\nCharacteristic subpopulations detected, example, identifying long-term determinants diseases revealing patient subgroups differential responses treatment.Traditional medical data analysis mostly hypothesis-driven.\r\nincreasing volume heterogeneity medical data, workflows becoming impractical, important relationships variables may go undetected.\r\nBesides, medical studies often involve measurements collected repeatedly time.\r\nInvestigating hidden temporal information can potentially lead new insights.\r\nmachine learning potential automatically detecting previously unknown subpopulations, results complex black-box models must made understandable.\r\nTherefore, medical expert must equipped tools understand, explore, visualize models, breaking individual patterns extract actionable insights.thesis proposes machine learning-based solutions expert-driven subpopulation discovery high-dimensional timestamped medical data.first part presents workflows detect comprehensible distinct subpopulations described classification rules clusters.\r\npresent novel visualizations interactive tools inspect juxtapose high-dimensional subpopulations compare change time.second part covers workflows exploit temporal information.\r\npresent framework extract evolution features characterize subpopulations’ change time.\r\nFurthermore, provide method build representations short temporal sequences.third part addresses topic post-hoc interpretation complex black-box models.\r\npropose end--end data analysis workflow includes steps data augmentation, modeling, nesting model training feature elimination, post-hoc analysis trained models.\r\nworkflow returns statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance machine learning model type.\r\nBesides, provide solution visualizing differences two priori defined subpopulations.proposed methods evaluated datasets four medical studies:longitudinal population study,observational therapy study data self-report questionnaire responses tinnitus patients,clinical experiment timestamped plantar pressure temperature recordings diabetes patients healthy volunteers, anda retrospective clinical study image data intracranial aneurysms.","code":""},{"path":"index.html","id":"zusammenfassung","chapter":"Welcome!","heading":"Zusammenfassung","text":"Die Entdeckung von Subpopulationen stellt ein wesentliches Ziel der Datenanalyse der medizinischen Forschung dar und trägt zur Vorbeugung und Behandlung von Erkrankungen bei.\r\nCharakteristische Subpopulationen werden beispielsweise durch die Identifizierung von Langzeitdeterminanten von Krankheiten oder durch die Bestimmung von Patientensubgruppen mit differenziellem Ansprechen auf eine Behandlung entdeckt.Die traditionelle medizinische Datenanalyse war bisher überwiegend hypothesengetrieben.\r\nMit der zunehmenden Menge und Heterogenität medizinischer Daten werden diese Workflows zunehmend ungeeignet, da wichtige Beziehungen zwischen Variablen unentdeckt bleiben können.\r\nAußerdem beinhalten medizinische Studien oft Messungen, die im Laufe der Zeit wiederholt erhoben werden.\r\nDas Extrahieren verborgener zeitlicher Informationen kann potenziell zu neuen Erkenntnissen führen.\r\nWährend maschinelles Lernen das Potenzial hat, bisher unbekannte Subpopulationen automatisch zu erkennen, müssen die Ergebnisse komplexer Black-Box-Modelle verständlich gemacht werden.\r\nDies erfordert, medizinische Expertinnen und Experten mit Werkzeugen auszustatten, die es ihnen ermöglichen, die Modelle zu interpretieren, zu explorieren und zu visualisieren, um individuelle Muster aufzuschlüsseln und daraus handlungsrelevante Erkenntnisse zu gewinnen.dieser Arbeit werden auf maschinellem Lernen basierende Lösungen für die expertengesteuerte Entdeckung von Subpopulationen hochdimensionalen, zeitgestempelten medizinischen Daten vorgeschlagen.Der erste Teil stellt Workflows vor, um verständliche und unterscheidbare Subpopulationen zu erkennen, die durch Klassifikationsregeln und Cluster beschrieben werden.\r\nWir stellen neuartige Visualisierungen und interaktive Werkzeuge vor, um die hochdimensionalen Subpopulationen zu inspizieren und gegenüberzustellen sowie ihre Veränderung über die Zeit zu vergleichen.Der zweite Teil befasst sich mit Workflows zur Modellierung zeitlicher Informationen.\r\nWir stellen ein Framework zur Extrahierung von Evolutionsvariablen vor, die die zeitliche Veränderung der Subpopulationen beschreiben.\r\nAußerdem wird ein Verfahren zur Erstellung von Repräsentationen aus kurzen zeitlichen Sequenzen vorgestellt.Der dritte Teil befasst sich mit dem Thema der Post-hoc-Interpretation von komplexen Black-Box-Modellen.\r\nWir stellen einen Ende-zu-Ende-Datenanalyse-Workflow vor, der Schritte zur Datenanreicherung, Modellierung, Verzahnung von Modelltraining mit Variablen-Eliminierung und Post-hoc-Analyse der trainierten Modelle umfasst.\r\nDieser Workflow liefert Kenngrößen und Visualisierungen, die die globale, instanz-individuelle und subpopulationsspezifische Variablenbedeutsamkeit für ein maschinelles Lernmodell jedweden Typs darstellen.\r\nAußerdem wird eine Visualisierung von Unterschieden zwischen zwei apriorisch definierten Subpopulationen präsentiert.Die vorgeschlagenen Methoden werden anhand von Datensätzen aus vier medizinischen Studien evaluiert:eine longitudinale Bevölkerungsstudie,eine beobachtende Therapiestudie mit Daten zu Selbstbeurteilungsfragebögen von Tinnitus-Patienten,ein klinisches Experiment mit zeitgestempelten Plantardruck- und Temperaturaufzeichnungen von Diabetes-Patienten und gesunden Probanden, undeine retrospektive klinische Studie mit Bilddaten zu intrakraniellen Aneurysmen.","code":""},{"path":"index.html","id":"chapter-progress","chapter":"Welcome!","heading":"Chapter Progress","text":"1 🏁🏁 Introduction\r\n2 🏁🏁 Medical Background DatasetsPART SUBPOPULATION DISCOVERY HIGH-DIMENSIONAL DATA3 🏁🏁 Interactive Discovery Inspection Subpopulations\r\n4 🏁🏁 Identification Distinct Subpopulations\r\n5 🏁🏁 Visual Identification Informative FeaturesPART II EXPLOITING DYNAMICS6 🏁🏁 Extraction Evolution Features Capture Change Time\r\n7 🏁🏁 Extraction Features Short Temporal SequencesPART III POST-MINING INTERPRETATION8 🏁🏁 Post-Hoc Interpretation Classification Models\r\n9 🏁🏁 Subpopulation-Specific Learning Post-Hoc Model InterpretationPART IV CONCLUSION10 🏁🏁 Summary Future WorkLegend:\r\n🏁🏁 = submission-ready\r\n🏁 = feedback reviewers incorporated\r\n🟢 = (preliminary) draft ready\r\n🔵 = maturing\r\n🔴 = unwritten","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"motivation-and-objectives","chapter":"1 Introduction","heading":"1.1 Motivation and Objectives","text":"Subpopulation discovery essential objective data analysis medical research [1], [2].\r\nKnowledge characteristic subpopulations can improve prevention (public health) treatment (clinical medicine) adverse medical conditions.\r\nSubpopulations detected \r\n() identifying long-term determinants protective factors medical condition interest [3]–[5],\r\n(ii) revealing subcohorts increased disease prevalence different treatment response [6]–[8], \r\n(iii) generating robust statistical models can explain relationships one independent variables outcome [9]–[11].\r\nexample, epidemiologists attempt discover associations specific features (e.g., demographics descriptors lifestyle) target variable (e.g., obesity) cohort studies collecting analyzing extensive participant data obtained questionnaires, medical examinations, laboratory analyses, imaging [12]–[15].\r\nstudies longitudinal design, measurements collected repeatedly time contain hidden temporal information, investigation can potentially lead new insights.find associations variables, medical researchers usually first carefully derive hypotheses clinical practice, experimental studies, extensive literature reviews test formally statistical significance [16].\r\nHowever, ever-increasing volume heterogeneity medical data [17], traditional hypothesis-driven workflows becoming increasingly impractical, , reason, critical inherent associations variables may go undetected [18].\r\nMachine learning can improve medical research discovering understandable descriptions patient study participant subpopulations similar terms target variable can thus used derive new hypotheses [19], [20].proliferation medical machine learning applications motivated, among others, desire make automated use plethora information collected study subjects, sometimes also ubiquity deep learning success stories media [21].\r\nHowever, ease creating complex data-driven models guarantee insights can effortlessly derived [22].\r\nstate---art machine learning algorithms deep neural networks [23] gradient boosting machines [24] generate -called black-box models multiple layers complexity involve many multivariate, nonlinear interactions variables difficult represent intuitively [25], [26].critical application expert, practitioner scientist working clinical epidemiological setting, equipped tools understand, explore, visualize models [27], [28] can drill specific individual patterns gain actionable insights ultimately contribute prevention, diagnosis, treatment clinical practice.\r\nmedical data come wide variety sources, key characteristics collected datasets vary, requiring adaptation methods application scenario [29].work proposes methods serve intelligent assistance medical researchers analyze high-dimensional, timestamped medical data.\r\nHence, core research question thesis :RQ: derive accurate yet understandable patterns subpopulation discovery high-dimensional timestamped medical data?, , generation machine learning models, several challenges must overcome medical expert derive actionable knowledge.\r\ntranslate challenges following three goals:\r\nComprehensibility distinctiveness subpopulations:\r\nextracted models, including clusters, rules, patterns, must made understandable; preferably, model generation process must also comprehensible. Furthermore, must minimize redundancy, negatively affects perceived quality model. task extract, process, display relevant patterns expert-driven model exploration.\r\n\r\nExploitation time:\r\nHidden temporal information must exploited. Medical scholars search long-term determinants severe diseases. Finding patterns subject “evolution” can contribute goal.\r\n\r\nPost-hoc interpretation complex black-box models:\r\ndiscovered patterns intrinsically interpretable, methods required extract relevant subpopulations can presented application expert.\r\n","code":""},{"path":"intro.html","id":"structure-and-contributions-of-this-thesis","chapter":"1 Introduction","heading":"1.2 Structure and Contributions of This Thesis","text":"thesis presents solutions support medical researchers expert-driven subpopulation discovery high-dimensional, timestamped medical data.\r\nDesign decisions developments partly inspired suggestions respective domain experts cooperation partners, including three tinnitus experts, epidemiologist statistical expertise, diabetes expert.thesis organized three parts ten chapters tackling research question challenges mentioned .\r\nPart covers methods subpopulation discovery high-dimensional data.\r\nPart II focuses specifically temporal aspects medical datasets provides approaches extract informative representations timestamped data.\r\nPart III addresses post-hoc analysis machine learning models includes solutions derive model-, observation-, subpopulation-level insights otherwise “opaque” black-box models.Chapter 2 (Medical Background Datasets) presents medical background relevant thesis, brief comparison medical study types, overview medical studies used validate proposed methods.Chapter 3 (Interactive Discovery Inspection Subpopulations) presents workflow interactive data-driven analysis population-based cohort data using hepatic steatosis example. includes steps () detect subpopulations different distributions respect target variable, (ii) classify subpopulation taking class imbalance account, (iii) detect variables associated outcome.Chapter 4 (Identifying Distinct Subpopulations) refines analysis previous chapter examining redundancy large rule sets describing subpopulations. present workflow extracts smaller number “representative” rules, .e., rules avoid instance overlap much possible, thus covering different subpopulations.Chapter 5 (Visual Identification Informative Features) introduces parameter-free clustering approach deriving phenotypes, phenotype exploration, visual juxtaposition phenotypes high-dimensional feature space.Chapter 6 (Constructing Evolution Features Capture Change Time) presents solution cohort analysis longitudinal cohort study data construct “evolution features” latent temporal information describing cohort participants’ change time.Chapter 7 (Feature Extraction Short Temporal Sequences Clustering) complements previous solution presenting approach create representations short temporal sequences via clustering experimental data.Chapter 8 (Post-Hoc Interpretation Classification Models) builds upon insights previous chapters role features subpopulation understanding. propose method makes already learned, complex classification models understandable domain experts. combine classification high-dimensional medical data model explanation using post-hoc interpretation methods.\r\nend, use Shapely value explanations (SHAP), LASSO coefficients, partial dependency plots.\r\napproach delivers statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance, help illuminate complex black-box machine learning models.Chapter 9 (Subpopulation-Specific Learning Post-Hoc Model Interpretation) addresses issue visualizing differences two subpopulations temporal data. purpose, derive post-hoc interpretation measure assess difference predictors’ association target variable two subpopulations.Chapter 10 (Summary Future Work) concludes thesis summarizing contributions providing detailed outlook presented work.validation proposed methods, used datasets following epidemiological clinical studies:SHIP: longitudinal population “Study Health Pomerania” [12],CHA: observational therapy study involving data self-report questionnaire responses tinnitus patients [30],DIAB: clinical experiment yielding timestamped plantar pressure temperature recordings diabetes patients non-diabetic volunteers [31], andANEUR: retrospective clinical study involving image data intracranial aneurysms [32].datasets used method validation follows:","code":""},{"path":"background.html","id":"background","chapter":"2 Medical Background and Datasets","heading":"2 Medical Background and Datasets","text":"Medical research data clinical epidemiological studies lays foundation decisions diagnosing treating multifactorial conditions diseases disorders.\r\nMajor goals identify long-term determinants protective factors outcome interest [3]–[5], discover subpopulations increased disease prevalence [6]–[8], study intervention effects generating statistical models explaining cause--effect relationships [9]–[11].\r\nTraditional medical data analysis pipelines usually structured hypothesis-driven way follows [16]:medical scientist formulates hypothesis based observations clinical practice current research. Possible examples include: “risk factor alcohol abuse affect prevalence particular outcome?” “effect novel therapy patients depressive symptoms?”small set relevant variables can controlled confounders selected test hypothesis. Variable selection may include controlling confounders. data necessary test hypothesis collected.strength associations selected variables outcome assessed using regression models statistical methods.Based results, inferential statistical calculations performed, conclusions drawn may support implementation new preventive interventions use appropriate treatments high-risk patients.However, advent big data [33] various fields, including medicine, data volume heterogeneity increasing dramatically, making traditional hypothesis-driven workflows increasingly inadequate important relationships variables may go undetected [18].\r\nthesis, present methods deal different aspects high-dimensional timestamped medical data.\r\nvalidate methods variety datasets diverse study types.chapter divided two parts.\r\nSection 2.1 provides brief comparison medical study types.\r\nSection 2.2, present studies data samples developed methods proposed thesis.","code":""},{"path":"background.html","id":"background-med-research","chapter":"2 Medical Background and Datasets","heading":"2.1 Brief Comparison of Medical Study Types","text":"Primary medical research can divided basic, clinical, epidemiological studies [34].\r\nfollowing comparison study types based reviews Thiese [35] Röhrig et al. [34] indicated otherwise.Basic research.\r\nBasic medical research (experimental research) aims improve understanding cellular, molecular, physiological mechanisms human health disease conducting cellular molecular investigations, animal studies, drug material property studies tightly controlled laboratory environments.\r\nstudy effects one variables interest outcome, variables usually held constant, variables interest varied.\r\ncarefully standardized experimental conditions basic medical studies ensure high internal validity, conditions often easily transferred clinical practice without compromising results’ generalizability.Clinical studies. Clinical studies generally classified interventional (experimental) studies non-interventional (observational) studies.\r\ngeneral objective intervention study compare different treatments within patient population whose members differ little possible except treatment arm.\r\ncommon example pharmaceutical study aims validate efficacy safety investigating establishing drug’s main side effects, absorption, metabolism, excretion.\r\nSelection bias can avoided appropriate measures, particular randomly assigning patients groups.\r\nTreatment may medication, surgery, therapeutic use medical device (e.g., stent), physical therapy, acupuncture, psychosocial intervention, rehabilitation, training form, diet.\r\nrandomized controlled trial (RCT) considered gold standard study design [36].\r\nSelection bias minimized () randomly assigning patients treatment control groups (b) ensuring equal distribution known unknown influencing variables (confounders), risk factors, comorbidities, genetic variability.\r\nRCTs thus suitable obtaining unambiguous answer clear question concerning (causal) efficacy treatment.Non-interventional clinical trials patient-based observational studies patients either receive individually defined treatment, patients receive treatment.\r\nexample non-interventional design study investigating regular use drugs therapies.\r\n, treatment, diagnosis, monitoring follow predefined study protocol rather medical practice alone.\r\nData analysis often retrospective.\r\nWhether study design prospective retrospective depends sequence hypothesis generation data collection.\r\nprospective studies, hypothesis generation comes data collection.\r\nFirst, hypotheses tested defined, e.g., regarding new treatment procedure.\r\n, data collected specifically hypothesis testing.\r\nfirst formulating testable hypotheses, possible ensure research questions can actually answered measured data.\r\nretrospective study design means data collection took place study began.Epidemiological studies.\r\nEpidemiological studies usually interested distribution change time incidence diseases causes general population subpopulations.\r\nCohort studies examine individuals, health outcomes interest beginning observation period assess exposure status various health-related conditions [37].\r\nincluded subjects followed time longitudinal studies (opposed cross-sectional studies), outcomes interest recorded multiple waves.\r\ndata, researchers can establish subgroups subjects exposure status, sort exposure, compare incidence prevalence disease among exposure categories.\r\nLongitudinal studies categorized trend panel design.\r\ntrend study, wave can involve different participant sample, .e., individual participant followed time.\r\ncontrast, panel study investigates population multiple points time allows also measure intra-individual temporal changes.","code":""},{"path":"background.html","id":"background-data","chapter":"2 Medical Background and Datasets","heading":"2.2 Datasets Investigated in This Thesis","text":"section, present datasets associated studies investigated thesis, namelythe Study Health Pomerania Section 2.2.1,observational therapy study health tinnitus patients baseline therapy Section 2.2.2,clinical experiment study diabetic foot syndrome Section 2.2.3, anda retrospective clinical study image data intracranial aneurysms Section 2.2.4.","code":""},{"path":"background.html","id":"background-data-ship","chapter":"2 Medical Background and Datasets","heading":"2.2.1 The Study of Health in Pomerania (SHIP)","text":"reunification Germany, found life expectancy significantly lower East West [38].\r\nFurthermore, regional differences within new states, lowest life expectancy found Northeast [38], [39].\r\nfind causal risk factors mortality conditions northeastern German population, Community Medicine Research Center Greifswald established Study Health Pomerania (SHIP) [12], longitudinal epidemiological study two independent cohorts northeastern Germany.\r\nSHIP seeks describe broad spectrum health conditions rather focusing specific target disease [12].\r\nparticular, major study objectives include investigations prevalence common diseases risk factors, correlation interaction risk factors diseases, progression subclinical manifest diseases, identification subpopulations increased health risk, prediction concomitant diseases, well usage costs medical service.Cohort inclusion criteria age 20 79 years, main residency study region, German nationality.\r\nParticipants SHIP underwent extensive, recurring (ca. every 5 6 years) examination program encompasses personal interviews, body measurements, exercise electrocardiogram, laboratory analysis, ultrasound examinations, whole-body magnetic resonance tomography (MRT).\r\nBaseline examinations first cohort performed 1997 2001 (SHIP-0, N = 4308).\r\nFollow-examinations carried 2002-2006 (SHIP-1, n = 3300), 2008-2012 (SHIP-2, n = 2333), 2014 - 2016 (SHIP-3, n = 1718) since 2019 (SHIP-4).\r\nFigure 2.1 illustrates participant response age distribution show-ups across study waves.\r\nBaseline information second, independent cohort (SHIP-Trend-0, N = 4420) collected 2008 2012, follow-conducted 2016 2019.\r\nMajor strengths SHIP high level quality assurance, standardized examination protocols, high cohort representativeness.\r\nFigure 2.1: Participation response age distribution show-ups across SHIP study waves. () Change number show-ups non-respondents relative cohort size across waves. (b) Age distribution show-ups wave.\r\nexamination program changed across waves.\r\nexample, MRT performed SHIP-2; liver ultrasound performed SHIP-0 SHIP-2 SHIP-1; dermatologic examinations performed SHIP-1 SHIP-2 SHIP-0.analyses focus disorder hepatic steatosis, also known “fatty liver,” characterized high accumulation fat liver, occurring approximately 30% adults [12], [40].\r\nRisk factors include alcohol abuse, obesity, metabolic syndrome, diabetes [41].\r\nLiver biopsy considered diagnostic gold standard [42] associated intermediate risk patient.\r\nNon-invasive diagnostic techniques include MRI, CT, ultrasound.\r\nhepatic steatosis usually asymptomatic, often goes undetected, can develop serious diseases, steatohepatitis, cirrhosis, hepatocellular carcinoma, even liver failure [42].used SHIP data subsets validate methods presented Chapters 3, 4, 6,  9.","code":""},{"path":"background.html","id":"background-data-cha","chapter":"2 Medical Background and Datasets","heading":"2.2.2 The Charité Tinnitus Patients Observational Therapy Study Dataset (CHA)","text":"Tinnitus perception phantom sound absence external sound source.\r\ncomplex, multifactorially caused maintained phenomenon estimated affect 10% 15% adult population [43].\r\nassociated annual economic burden 19.4 billion USD \r\nUnited States [44] 6.8 billion EUR Netherlands alone [45].\r\nClinical evaluation tinnitus challenging due patient heterogeneity tinnitus perception (laterality, pitch, noise characteristics, frequency, duration, chronicity), risk factors (including hearing loss, temporomandibular joint disorder, aging), comorbidities (including hyperacusis, depression, sleep disorders), perceived distress, treatment response [46].\r\ndifferences complicate identification appropriate effective treatment modality.\r\nCurrently, treatment gold standard: sound therapy (masking), informational counseling (minimal contact education), cognitive behavioral therapy, tinnitus retraining shown effective patients, also evidence patients benefit equally forms treatment [47]–[51].\r\nDue heterogeneous nature tinnitus symptom unclear evidence base regarding treatment management, identification patient subgroups critical stratify individual pathophysiology treatment pathways [52]–[54].“Charité tinnitus patients observational therapy study dataset” (CHA) includes self-report data 4103 tinnitus patients treated Tinnitus Center Charité Universitätsmedizin Berlin, Germany, January 2011 October 2015.\r\npatients 18 years age older suffered tinnitus least 3 months.\r\nExclusion criteria presence acute psychotic illness addiction, deafness, insufficient knowledge German language.\r\nTreatment included 7-day multimodal program intensive daily informational counseling, detailed ear-nose-throat psychological diagnosis, cognitive behavioral therapy interventions, hearing exercises, progressive muscle relaxation, physical therapy.\r\nbaseline (T0; start therapy) treatment (T1), patients asked complete several self-report questionnaires.\r\nquestionnaires selected obtain comprehensive assessment tinnitus, including tinnitus-related distress psychosomatic background tinnitus anxiety, depression, general quality life, experienced physical impairments.Table 2.1: Description questionnaires form basis CHA. |F|: total number items, subscales total scales.\r\nFigure 2.2: Patient demographics (CHA). Overview patient demographics degree tinnitus distress measured therapy commencement.\r\nTable 2.1 provides overview questionnaires used analyses.\r\nquestionnaires contain multiple-choice items answers Likert scale. example, “Tinnitus Questionnaire” [68] (TQ) contains 52 statements, “unable enjoy listening music noises.” respondents can give 3 possible answers: “true” (coded 0), “partly true” (1), “true” (2).\r\nquestionnaires also include aggregate variables called “subscales” “total scores.”\r\nexample, TQ total score (TQ_distress) calculated sum 40 item values, 2 items used twice [68], resulting range values 0 84, higher values representing higher tinnitus-related distress.\r\ncutoff value 46 [68] used distinguish compensated (0-46) decompensated (47-84) tinnitus.\r\nFurthermore, average time answer item recorded questionnaire.\r\nFigure 2.2 provides graphical representation demographic data 3803 (92.7%) patients complete data socio-demographics questionnaire [65] (SOZK) TQ score.used CHA dataset validate methods developed Chapters 5, 8,  9.","code":""},{"path":"background.html","id":"background-data-diab","chapter":"2 Medical Background and Datasets","heading":"2.2.3 The Diabetic Foot Clinical Experiment Study (DIAB)","text":"Diabetic foot syndrome umbrella term foot-related problems diabetic patients.\r\none four diabetes patients develop foot ulcer lifetime [69], many risk amputations next four years [70].\r\n85% foot amputations due foot ulcers [71], [72].\r\nrate foot amputations diabetes patients estimated 17-40 times higher general population [73].\r\nDFS patients predisposed peripheral sensory neuropathy, results, example, patients unaware temperature feet pressure applied .\r\nAffected individuals may even injure without realizing .\r\nExcessive plantar pressures can exacerbate tissue destruction increase lifetime risk foot ulceration [74].\r\nHowever, understanding pathomechanisms underlying tissue destruction absence trauma limited.university hospital Magdeburg, Germany, experimental study 31 healthy volunteers 30 diabetes patients diagnosed severe polyneuropathy conducted quantify pressure- posture-dependent changes plantar temperatures surrogate tissue perfusion.\r\npurpose, plantar pressure temperature changes feet recorded extended episodes standing.\r\nCustom-made shoe insoles [75] equipped eight temperature sensors eight pressure sensors preselected positions used data acquisition (Figure 2.3 ()).\r\ninsoles positioned closed protective shoes specifically developed diabetes patients.\r\nWithin shoes, temperature increases time due exchange person’s body temperature also affected environmental temperature.\r\nclosely monitor -shoe temperature changes, one sensor placed bottom insole without contact feet, denoted “ambient temperature sensor.”\r\nFigure 2.3: Positions pressure- temperature sensors insole temporal, pressure-dependent temperature change. () Sensor positioning insole relation foot placement. (b) Thermographic infrared images showing healthy subject seated position pressure applied feet () placement 20 kg weight thighs. measured temperature ranged 29°C (blue) 34°C (red). time-dependent temperature decrease observed predominantly forefoot region, visualized yellow color pressure application. rapid temperature increase noted within 1 min pressure relief. MTB: metatarsal bone. figure adapted [31].\r\nData collection began immediately shoes put .\r\nParticipants asked follow predefined sequence actions, .e., alternating standing (stance episode) sitting (pause).\r\nsession consisted 6 stance episodes lasting 5, 10, 20, 5, 10, 20 minutes, respectively, separated pause episodes lasting 5 minutes .\r\nParticipants instructed apply equal pressure feet standing.\r\nParticipants receive immediate feedback actual application pressure sessions, verbally encouraged study nurses maintain pressure standing without releasing .\r\nseated position, participants instructed release pressure 5 minutes maintaining contact insole.\r\nParticipants explicitly asked adhere instructions, .e., release pressure standing episode temporarily.\r\nstudy protocol included measurements performed twice, room temperature approximately 22°C outdoors ambient temperature approximately 16°C.\r\ntwo measurements performed two independent days.thermographic images Figure 2.3 (b) visualize exemplary changes plantar temperature healthy subject sitting position pressure application (1), placing 20 kg weight front thigh (2-6), removing additional weight (7-8).\r\npressure application, gradual temporal decrease temperature noted predominantly forefoot.\r\npressure relief, rapid temperature increase observed within 1 min.used DIAB dataset validate methods developed Chapter 7.","code":""},{"path":"background.html","id":"background-data-aneur","chapter":"2 Medical Background and Datasets","heading":"2.2.4 The Intracranial Aneurysm Angiography Image Dataset (ANEUR)","text":"Intracranial aneurysms pathologic dilations intracranial vessel wall, often form dilation. \r\nbear risk rupture, leading subarachnoidal hemorrhages often fatal consequences patient.\r\nSince treatment can also cause severe complications, extensive studies conducted assess patient-individual rupture risk based various parameters, including aneurysm symptomatology, size, location, patient age sex [76].\r\nstudies identified parameters, aspect ratio, undulation index, nonsphericity index, statistically significant respect aneurysm rupture status [77], [78].\r\nHowever, although studies allow retrospective analysis, clinician needs guidance asymptomatic aneurysm (accidental finding) detected rupture risk determined.developed methods retrospective “Intracranial Aneurysm Angiography Image Dataset” (ANEUR) comprising 3D rotational angiography data 74 patients (age: 33-85 years, 17 male 57 female patients) university hospital Magdeburg, Germany, adding total 100 intracranial aneurysms.\r\nidentified two primary goals dataset: () build models can accurately predict rupture status based morphological parameters , (ii) assess importance parameters models optimal accuracy.Motivated results Baharoglu et al. [79], found differences sidewall bifurcation aneurysms (cf. Figure 2.4) terms relationship several morphological parameters rupture status, learn different models subset sidewall aneurysms (9 (37.5%) 24 ruptured) subset bifurcation aneurysms (29 (46.8%) 62 ruptured).\r\nAdditionally, run experiments combined group (43 100 ruptured) containing 14 additional samples clearly determined either sidewall bifurcation aneurysms.\r\nFigure 2.4: Sidewall bifurcation aneurysms. Illustration aneurysm side parent vessel wall (left) aneurysm vessel bifurcation (right). figure adapted [32].\r\nused ANEUR dataset validate methods developed Chapter 8.","code":""},{"path":"imm.html","id":"imm","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3 Interactive Discovery and Inspection of Subpopulations","text":"chapter partly based :Uli Niemann, Henry Völzke, Jens-Peter Kühn, Myra Spiliopoulou. “Learning inspecting classification rules longitudinal epidemiological data identify predictive features hepatic steatosis.” : Expert Systems Applications 41.11 (2014), pp. 5405-5415. DOI: 10.1016/j.eswa.2014.02.040.Uli Niemann, Myra Spiliopoulou, Henry Völzke, Jens-Peter Kühn. “Interactive Medical Miner: Interactively exploring subpopulations epidemiological datasets.” : ECML PKDD 2014, Part III, LNCS 8726. Springer, 2014, pp. 460-463.\r\nDOI: 10.1007/978-3-662-44845-8_35.chapter organized follows.\r\nSection 3.1, motivate classification interactive subpopulation discovery epidemiological cohort studies review related work.\r\npresent workflow interactive assistant Section 3.2.\r\nSection 3.3, report results main findings.\r\nchapter closes summary discussion main contributions Section 3.4.","code":""},{"path":"imm.html","id":"brief-chapter-summary","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"Brief Chapter Summary","text":"Analysis population-based cohort data mostly hypothesis-driven.\r\npresent workflow interactive application data-driven analysis population-based cohort data using hepatic steatosis example.\r\nmining workflow includes stepsto discover subpopulations different distributions respect target variable,classify subpopulation taking class imbalance account, andto identify variables associated target variable.show workflow suited () build subpopulations classification reduce class imbalance (b) drill-derived models identify predictive variables subpopulations worthy investigation.","code":""},{"path":"imm.html","id":"imm-intro","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1 Motivation and Comparison to Related Work","text":"Medical decisions diagnosis treatment multifactorial conditions diseases disorders based clinical epidemiological studies [80].\r\nlatter contain information participants without disease allow learning discriminatory models , longitudinal designs, understanding disease progression.\r\nexample, several studies identified risk factors (obesity alcohol consumption) comorbidities (cardiovascular disease) associated hepatic steatosis [81]–[85].\r\nHowever, studies identified risk factors associated outcomes relate entire population.\r\nwork arose need identify factors subpopulations promote personalized diagnosis treatment, expected personalized medicine [86], [87].","code":""},{"path":"imm.html","id":"role-of-subpopulations-in-classifier-learning-for-cohorts","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1.1 Role of Subpopulations in Classifier Learning for Cohorts","text":"Classification subpopulations studied Zhanga Kodell [88], pointed classifier performance whole dataset may low entire population heterogeneous.\r\nTherefore, first trained ensemble classifiers used ensemble member’s predictions create new feature space.\r\nperformed hierarchical clustering partition instances three subpopulations: one prediction accuracy high, one intermediate range, one low.\r\nUsing approach, Zhanga Kodell partition original data set subpopulations easy hard classify.\r\nmethod seems appealing general, appears inappropriate three-class problem SHIP data, highly skewed distribution, clear low classification accuracy caused (part) class imbalance.\r\nTherefore, exploratively examined data set classification identify less skewed subpopulations classification determine – within subpopulation – variables strongly associated target.Pinheiro et al. performed association rule discovery patients liver cancer [89].\r\nauthors pointed early detection liver cancer reduces mortality rate.\r\nEarly detection still difficult patients often show symptoms early stages liver cancer [89].\r\nPinheiro et al. used association rule algorithm FP-growth [90] discover high-confidence association rules high-confidence classification rules related liver cancer mortality.\r\nalso considered association rules promising medical data analysis easy compute produce results understandable humans.\r\nTherefore, used association rules primary method, epidemiological data classification rather mortality prediction.\r\nuse association rules classification, specified rule’s consequence target variable.","code":""},{"path":"imm.html","id":"workflows-for-expert-machine-interaction-for-cohort-construction-and-analysis","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1.2 Workflows for Expert-Machine Interaction for Cohort Construction and Analysis","text":"Zhang et al. [91] addressed increasing technical challenges medical expert-driven subpopulation discovery due increasingly large complex medical data, often including information hundreds variables thousands patients form tables, images, text.\r\npast, sufficient physician basic knowledge statistics spreadsheet software Microsoft Excel analyze small table patient data.\r\nToday, effective efficient approaches managing, analyzing, summarizing extensive medical data available [91].\r\nHowever, domain experts typically rely technical experts help perform tasks.\r\nback--forth often slow, tedious, expensive.\r\nTherefore, better provide domain expert technical tool allows perform exploratory analysis quickly.\r\nZhang et al. [91] presented CAVA, system includes various subgroup visualizations (called “views”) analytical components (called “analytics”) subgroup comparison.\r\nmain panel Figure 3.1 shows one views: flowchart [92] patient subgroups sequence symptoms.\r\nuser can obtain additional summaries interacting visualization, example, dragging dropping one boxes flowchart onto one entries analysis panel.\r\nuser can also expand selected cohort tool search patients strictly meet current inclusion criteria somewhat similar selected patient subpopulation interest [93].\r\nFigure 3.1: CAVA’s graphical user interface. flowchart visualizes subgroups cardiac patients organized common occurrence symptoms. Arc color represents hospitalization risk. user can switch graphical representations data processing methods dragging dropping. upper right panel contains detailed information currently selected patients. lower right panel contains provenance graph allows user undo operations revisit previous interaction steps. figure taken [91].\r\nKrause et al. [94] argued model selection based global performance metrics accuracy, statistics contribute better understanding model’s reasoning.\r\nMoreover, complex highly accurate model automatically guarantee actionable insights.\r\nKrause et al. propose Prospector [94], system provides diagnostic components complex classification models based concepts partial dependence (PD) plots [24].\r\nPD plots popular tool visualizing marginal effects features predicted probability target.\r\nBriefly, point PD plot represents model’s average prediction observations, assuming observations fixed value feature interest.\r\nfeature whose PD curve exhibits high range high variability considered influential model prediction feature flat PD curve.\r\nClosely related PD plots individual conditional expectation (ICE) plots, [95] display curve observation, helping reveal contrasting subpopulations might “average ” PD plot.\r\nProspector combines PD ICE curves show relationship feature model prediction (global) model level (local) patient-individual level.\r\nBesides, custom color bar provided compact alternative ICE curves (Figure 3.2 ()).\r\nstacked bar graph shows distribution predicted risk scores study group (Figure 3.2 (b)).\r\nuser can click specific decile obtain list individual patients exact predicted score label.\r\nway, patients whose prediction scores close decision threshold can investigated.\r\nfeature, authors calculate “impactful feature change”: given patient’s current feature values, identify near-counterfactual value leads large change predicted risk score minimizing difference original feature value maximizing predicted risk score.\r\ntop 5 -called “suggested changes” displayed – separately increasing decreasing disease risk - table (cf. Figure 3.2 (c)) integrated interactive elements IC color bars (cf. Figure 3.2 (d)).\r\n\r\nFigure 3.2: Selected model diagnostics Prospector. () upper plot shows two curves characteristic “age”: gray partial dependence (PD) curve represents marginal prediction model patients, black individual conditional expectation (ICE) curve illustrates effect counterfactual ages predicted risk diabetes example patient. histogram shows age distribution. color bar compact representation ICE curve ; circled value represents selected patient’s feature value. (b) Stacked bars show distribution predicted risk scores study group. Clicking one bars opens table showing ID, predicted risk, true label patients belonging selected decile predicted risk. (c) Summary table “impactful feature changes” decreasing (upper group) increasing (lower group) predicted risk: row shows actual feature value “suggested change,” .e., similar counterfactual value lead significant change predicted risk. (d) Multiple PD color bars augmented suggested changes (labels outlined white). figure adapted [94].\r\nPahins et al. [96] presented COVIZ, system cohort construction large spatiotemporal datasets.\r\nCOVIZ includes components exploratory data analysis treatment pathways event trajectories, visual cohort comparison, visual querying.\r\nOne design goals COVIZ fast, e.g., using efficient data structures Quantile Data Structure [97] ensure low latency computational operations thus suitability large data sets.\r\n\r\nBernard et al. [98] proposed system cohort construction temporal prostate cancer cohort data included visualizations subpopulations individual patients.\r\nguide users exploration, visual markers indicate interesting relationships attributes derived statistical tests.\r\nRecently, Corvo et al. [29] presented comprehensive visual analytics system pathological high-throughput data, encompasses major steps typical data analysis pipeline, preprocessing raw histopathology images interactive segmentation, components exploratory data analysis, interactive cohort construction high-dimensional feature space, feature engineering includes extraction potentially predictive biomarker features, modeling, well visualization summarization modeling results.\r\nPreim Lawonn provided comprehensive reviews visual analytics methods applications public health [27] epidemiology [99] particular.","code":""},{"path":"imm.html","id":"previous-work-on-subpopulation-discovery-with-the-ship-data","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1.3 Previous Work on Subpopulation Discovery with the SHIP Data","text":"Since carry proof--concept workflow SHIP data, list major scientific preparatory work hereafter.\r\nPreim et al. [100] provided overview research developed data mining visual analytics methods gain insights SHIP data.\r\nAmong “3D Regression Cube” Klemm et al. [101], system allows interactive exploration feature correlations epidemiological datasets.\r\nsystem generates many multiple linear regression models different combinations one dependent three independent variables displays goodness fit three-dimensional heat map.\r\nsystem allows user modify regression equation, example, changing number independent variables, specifying wild cards interaction terms, fixing one variables reduce computational complexity, focusing specifically variable interest.\r\napproach also able identify variables strongly associated target variable.\r\nHowever, search subpopulation-specific relationships rather generating global model entire dataset, additionally provide predictive value ranges.\r\nKlemm et al. [16] presented system combines visual representations non-image image data.\r\nidentify clusters back pain patients SHIP data.\r\nSince specify hepatic steatosis target variable, instead build supervised models classification rules directly capture relationships predictors target variable.\r\n\r\nAlemzadeh et al. [102] presented S-ADVIsED, system interactive exploration subspace clusters incorporates various visualization types donut diagrams, correlation heatmaps, scatterplot matrices, mosaic diagrams, error bar graphs.\r\nS-ADVIsED requires user input mining results obtained advance outside system, tool enables expert-driven interactive subpopulation discovery instead expert-driven interactive result exploration.\r\n\r\nHielscher et al. [103] developed semi-supervised constrained-based subspace clustering algorithm find diverse sets interesting feature subsets using SHIP data.\r\nguide search predictive feature subsets, expert can provide domain knowledge form small number instance-level constraints, forcing pairs instances (.e., study participants) assigned either different cluster.\r\nHielscher et al. [104] extended work introduced mechanism validate subpopulations independent cohorts.","code":""},{"path":"imm.html","id":"imm-workflow","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2 Subpopulation Discovery Workflow and Interactive Mining Assistant","text":"section, present subpopulation discovery workflow.\r\nbuild classification models whole dataset different partitions, described Section 3.2.1.\r\nSection 3.2.2, introduce relevant underpinnings classification rule discovery, followed description primarily used HotSpot [105] algorithm Section 3.2.3.\r\npresent interactive mining assistant Section 3.2.4.\r\ndataset used population partitioning class separation target variable hepatic steatosis comes “Study Health Pomerania” (SHIP), recall Section 2.2.1.\r\nSection 3.2.5, describe origin availability target variable.\r\nSection 3.2.6, motivation data partitioning partitioning steps presented.","code":""},{"path":"imm.html","id":"imm-workflow-classification","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.1 Classification","text":"classification cohort participants, focus algorithms provide interpretable models, aim identify predictive conditions, .e., variables values/ranges models.\r\nTherefore, consider decision trees, classification rules, regression trees.\r\nuse J4.8 decision tree classification algorithm (equivalent C4.5 algorithm [106]) Waikato Environment Knowledge Analysis (Weka) Workbench [107].\r\nalgorithm builds tree successively partitioning node (subset dataset) variable maximizes information gain within node.\r\noriginal algorithm works variables take categorical values, creating one child node per value.\r\nHowever, Weka implementation also provides option forces algorithm always create exactly two child nodes: one best separating value one values.\r\nuse option experiments yields better quality trees.\r\nWeka algorithm also supports variables take numeric values:\r\nnode split two child nodes partitioning variable’s range values two intervals.deal skewed distribution, consider following classification variants:Naive: problem imbalanced data ignored.InfoGain: keep top 30 66 variables sorting variables information gain towards target variable.Oversampling: use SMOTE [108] resample dataset minority-oversampling: class B, 100% new instances generated; class C, 300% new instances generated, resulting following distribution : 438, B: 216, C: 128.CostMatrix: prefer misclassify negative case rather detecting positive case, penalize false negatives (FN) false positives (FP).\r\nuse cost matrix depicted Table 3.1.\r\nTable 3.1: Cost matrix. Cost matrix penalize misclassification class imbalance.\r\n","code":""},{"path":"imm.html","id":"imm-workflow-rule-discovery","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.2 Classification Rule Discovery","text":"Classification rules can reveal interesting relationships one features target variable [109], [110].\r\nCompared model families deep neural networks, support vector machines random forests, classification rules usually achieve lower accuracy.\r\nHowever, easier interpret infer therefore suitable interactive subpopulation discovery.\r\nepidemiological research, interesting subpopulations subsequently used formulate validate small set hypotheses investigate associations risk factors particular target variable.\r\nsubpopulation interest formulated follows: “sample study, prevalence goiter 32%, whereas probability subpopulation described thyroid-stimulating hormone less equal 1.63 mU/l body mass index greater 32.5 kg/m2 49%.”Classification rule algorithms induce descriptions interesting subpopulations interestingness quantified quality function.\r\nclassification rule association rule whose consequent fixed specific class value.\r\nConsider exemplary classification rule \\(r_1\\):\r\n\\[\\begin{equation}\r\nr_1: \\underbrace{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}_{\\text{Antecedent}} \\longrightarrow \\underbrace{\\vphantom{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}hepatic\\_steatosis = pos}_{\\text{Consequent}}\r\n\\tag{3.1}\r\n\\end{equation}\\]Classification rules expressed form \\(r: \\text{antecedent} \\longrightarrow T=v\\).\r\nconjunction conditions (.e., feature - feature value pairs) left arrow constitutes rule’s \\(\\text{antecedent}\\) (left-hand side).\r\n\\(\\text{consequent}\\) (right-hand side), \\(v\\) requested value target variable \\(T\\).define \\(s(r)\\) subpopulation cover set \\(r\\), .e., set instances satisfy antecedent \\(r\\).\r\ncoverage \\(r\\), fraction instances covered \\(r\\), defined \\(Cov(r)=|s(r)|/N\\), \\(N\\) total number instances.\r\nsupport \\(r\\) quantifies percentage instances covered \\(r\\) additionally \\(T=v\\), calculated \\(Sup(r)=|s(r)_{T=v}|/N\\).\r\nconfidence \\(r\\) (also referred precision accuracy) defined \\(Conf(r)= |s(r)_{T=v}|/|s(r)|\\) expresses relative frequency instances satisfying complete rule (.e., antecedent consequent) among satisfying antecedent.\r\nrecall sensitivity \\(r\\) respect \\(T=v\\) defined \\(Recall(r)=Sensitivity(r)=\\frac{|s(r)_{T=v}|}{n_{T=v}}\\).\r\nWeighted Relative Accuracy rule interestingness measure balances coverage confidence gain often used internal criterion candidate generation [110].\r\ndefined \\[\\begin{equation}\r\nWRA(r) = Cov(r)\\cdot \\left(Conf(r)-\\frac{n_{T=v}}{N} \\right).\r\n\\tag{3.2}\r\n\\end{equation}\\]odds ratio \\(r\\) respect \\(T=v\\) defined \r\n\\[\\begin{equation}\r\n(r) = \\frac{ |s(r)_{T=v}| }{|s(r)_{T\\neq v}|} / \\frac{n_{T=v} -  |s(r)_{T=v}| }{ n_{T\\neq v} -  |s(r)_{T\\neq v}|}.\r\n\\tag{3.3}\r\n\\end{equation}\\]example, Figure 3.3 illustrates exemplary rule \\(r_2\\) dataset 10 instances binary target, circles cyan color represent instances negative class red circles positive instances.\r\ncover set \\(r_2\\) contains instances 7, 8, 9 10, hence \\(Cov(r_2)\\) = 0.40.\r\n, \\(Sup(r_2)\\) = 0.30, \\(Conf(r_2)\\) = 0.75, \\(WRA(r_2)\\) = 0.40 \\(\\cdot\\) (0.75 - 0.40) = 0.14 \\((r_2)\\) = (3/1) / (1/5) = 15.\r\nFigure 3.3: Exemplary classification rule. gray area represents data space covered instances.\r\n","code":""},{"path":"imm.html","id":"imm-workflow-hotspot","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.3 HotSpot","text":"classification rule discovery, use HotSpot [105] algorithm provided Weka [107].\r\nHotSpot beamwidth search algorithm implements general--specific approach rule extraction.\r\nsingle rule constructed successively adding condition antecedent locally maximizes confidence.\r\nUnlike general hill-climbing, considers best rule candidate iteration, HotSpot’s beam search retains b highest-ranked candidates refines later steps.\r\nConsequently, HotSpot reduces “myopia” [109] Hill-Climbing search typically suffers.\r\nBriefly, hill-climbing approaches consider locally optimal candidate iteration.\r\nresult, globally optimal rule found locally optimal iteration.\r\nalso desirable generate one rule application perspective since alternative descriptions subpopulations can facilitate hypothesis generation.\r\nbeamwidth can specified maximum branching factor, .e., maximum number conditions can added candidate rule.\r\niteration, rule candidates must satisfy minimum value count, sensitivity threshold.\r\navoid adding condition leads marginal improvement confidence, parameter minimum improvement, .e., minimum relative improvement confidence adding another condition, can specified.\r\nrule search’s computational complexity can reduced specifying maximum rule length, .e., number conditions antecedent.\r\nexperiments set parameters follows: maximum branching factor = 20, maximum value count = 1/3, minimum improvement = 0.1, maximum rule length = 3.","code":""},{"path":"imm.html","id":"imm-workflow-imm","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.4 Interactive Medical Miner","text":"Classification rules can provide valuable insights potentially prevalent conditions different subpopulations cohort study.\r\nHowever, number rules created large, usually case large epidemiological data, rules’ conditions overlap.\r\nHence, conditions present classes target variable.\r\nTherefore, medical expert needs inspection tools decide rules informative features investigated .\r\nInteractive Medical Miner (IMM) allows expert todiscover classification rules,inspect frequency rules () class (b) unlabeled subset cohort, andexamine statistics rule values selected variables.describe functionalities , referring screenshot Figure 3.4.\r\nFigure 3.4: user interface Interactive Medical Miner. Classification rules discovered class B shown bottom left panel. selected rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, distribution participants covered rule among three classes shown absolute values (top middle panel) histogram (bottom right panel) respect age (top right panel).\r\nuser interface consists six panels.\r\n“Settings” panel (top left), medical expert can set parameters rule induction pressing “Build Rules” button.\r\npanel, discovered rules displayed.\r\n“Sorting preference” panel, expert can specify whether rules sorted confidence, coverage, rather alphabetically better overview overlapping rules.rule generation, user can specify sub-cohort dataset.\r\nclicking button Select Subpopulation, popup window appears, multiple filter queries form <variable> <operator> <value> can added, e.g., som_bmi_s2 >= 30.\r\ndefined constraints displayed table can undone.\r\nFurthermore, user can select variables model creation, e.g., exclude variable already known highly correlated another variable already considered model learning.Mining criteria include dataset (choose whole dataset one partitions), class rules generated (drop-list “Class”), constraints related class, .e., “Minimum number values” (can also specified relative value), “Maximum rule length,” “Maximum branching factor” “Minimum improvement.”\r\nexample parameters affect rule search, consider selected rule Figure 3.4, som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, coverage 0.12 confidence 0.56.\r\nsensitivity 38/108 = 0.352 satisfies minimum value count threshold 0.33.\r\nApriori property, evident two conditions antecedent rule, namely som_huef_s2 > 109 crea_u_s2 > 5.38, must also exceed threshold.\r\nposition condition within antecedent indicates refinement step condition added rule candidate.\r\nexample, first condition som_huef_s2 > 109 confidence 44/107 = 0.41 extended second condition crea_u_s2 > 5.38 confidence gain exceeds minimum improvement threshold, .e., 38/68 - 44/107 = 0.15 > 0.05.\r\nHowever, rule extended maximum rule length set 2.\r\nmaximum branching factor conservatively set 1000 prevent potentially interesting rules generated due small beamwidth.\r\nexpert can lower parameter interactively number rules found high rule induction takes long.output list execution run (area “Settings”) scrollable interactive.\r\nexpert clicks rule, upper-middle area “Summary Statistics” updated.\r\nfirst row shows distribution cohort participants across classes entire dataset.\r\nsecond row shows participants covered rule (column “Total” second row) distributed across classes.\r\nThus, expert can specify discovery classification rules one classes examine often rule’s antecedent occurs among participants classes.\r\nexample, rule covers participants selected class (class B Figure 3.4) necessarily interesting also covers high number participants classes.\r\nrule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B covers total 68 participants, 38 class B.\r\nreduce number covered participants classes, .e., increase confidence, user can decrease minimum value count threshold allow generating rules lower sensitivity higher homogeneity respect selected class.data may incomplete.\r\nexample, participants cohort underwent liver MRI.\r\nTherefore, also interest know distribution unlabeled participants support given rule’s antecedent.\r\npurpose, “Histogram” panel can used: expert selects another feature interactive “Variable selection” area upper right panel can see values variable distributed among study participants – labeled unlabeled; latter marked “Missing” color legend.\r\nplotting histograms, use free Java chart library JFreeChart [111].\r\nNumerical variables discretized using “Scott’s rule” [112] follows:\r\nlet \\(X_{s(r)}\\) set values numeric variable \\(X\\) respect cover set \\(s(r)\\).\r\nbin width \\(h\\) calculated \\(h(X_{s(r)})=\\frac{\\max{X_{s(r)}}-\\min{X_{s(r)}}}{3.49\\sigma_{s(r)}}\\cdot |s(r)|^{\\frac{1}{3}}\\).expert select variable, target variable used default, distribution labeled participants visible.\r\nhistogram Figure 3.4 shows age distribution labeled unlabeled participants covered example rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B.\r\ndistribution values among labeled participants indicates age may risk factor indicated subpopulation, probability class B increases age.\r\nvisual finding suggests adding condition age_ship_s2 > 56.8 antecedent rule.\r\nIndeed, confidence specific rule increases 38/68 = 0.56 27/40 = 0.675.\r\nHowever, sensitivity decreases 38/108 = 0.352 27/108 = 0.250, minimum value count threshold longer met.\r\nThus, visualizing participant statistics selected rules can provide clues subpopulations monitored closely clues modify algorithm parameters subsequent runs, example, decrease minimum value count 0.25 increase maximum rule length 3.","code":""},{"path":"imm.html","id":"imm-workflow-target","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.5 The Target Variable","text":"target variable derived participants’ liver fat concentration calculated magnetic resonance imaging (MRI).\r\ntime writing original manuscript, MRI results available 578 (total 2333; ca. 24.7%) SHIP-2 participants.\r\nuse data participants classifier learning, Interactive Medical Miner also contrasts data data remaining 1755 participants MRI scans made available.discussions domain experts, decided assign participants liver fat concentration 10% less class (“negative” class, .e., absence disorder); values greater 10% less 25% represent class B (increased liver fat/fatty liver tendency) values greater 25% class C (high liver fat).\r\nconsider classes B C “positive.”\r\ncutoff value 10% intentionally higher value 5% proposed Kühn et al. [113] separate subjects without hepatic steatosis primary interest medical perspective identify predictive variables subjects likely ill.\r\nSelecting high cutoff value exacerbates class imbalance makes data analysis difficult.\r\nFigure 3.5 depicts class distribution stratified sex.\r\n578 participants, 438 belong class (approximately 76%), 108 B (19%), 32 C (6%).\r\nMen likely elevated high liver fat concentration women (30.7% vs. 18.8% classes B C).\r\nFigure 3.5: Sex-specific distribution target variable. boxes’ relative sizes depict number female male participants classes.\r\naddition target variable, data set contains 66 variables extracted participants’ questionnaire responses medical tests (cf. [12]).\r\nvariables socio-demographics (e.g., sex age),\r\nself-reported lifestyle indicators (e.g., alcohol cigarette consumption), single-nucleotide polymorphism (SNP), laboratory measurements (e.g., serum concentrations), liver ultrasound.\r\ntwo available variables liver ultrasound stea_s2 stea_alt75_s2.\r\ntake symbolic values reflecting probability participant fatty liver; latter combination former participant’s alanine transaminase (ALAT) concentration; details caption Table 3.3 [12].\r\nAlmost variables mentioned suffix _s2 indicating SHIP-2 follow-measurements, contrast SHIP-0 (_s0) SHIP-1 (_s1).\r\nExceptions sex, highest school degree, 10 SNP variables.","code":""},{"path":"imm.html","id":"imm-workflow-partitioning","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.6 Partitioning the Dataset into Subpopulations","text":"dataset imbalanced respect sex (314 females, 264 males), decided partition dataset classification.\r\nFirst, examined class distributions sex.\r\nobserved distributions different, especially class B (see Figure 3.5).\r\nSecond, examined class distribution sex age.\r\nfound age associated female subpopulation, male subpopulation.\r\nThird, identified cutoff point age introducing heuristic determines age value minimizes target variable’s standard deviation.\r\nperformed supervised learning separately partitions female male participants, referred PartitionF PartitionM hereafter.\r\nalso created additional learner subpopulation older female participants aged cutoff point 52 (Partition F:age>52).understand age affects class distribution, introduced heuristic determines cutoff age value PartitionF splits two bins standard deviations liver fat concentration bin minimized.\r\nLet \\(splitAge\\) denote cutoff value \\(X_y=\\{x\\\\mathtt{PartitionF}|\\text{age } x \\leq splitAge\\}\\), \\(X_z=\\{x\\\\mathtt{PartitionF}|\\text{age } x > splitAge\\}\\) denote bins.\r\n, let \\(n\\) cardinality \\(X_y\\cup{}X_z\\), .e., PartitionF.\r\n, define Sum Weighted Standard Deviations (\\(SWSD\\)) \\[\\begin{equation}\r\nSwSD\\left(X_y,X_z\\right) = \\frac{|X_y|}{n}\\sigma({X_y})+\\frac{|X_z|}{n}\\sigma({X_z})\r\n\\tag{3.4}\r\n\\end{equation}\\]\\(|X_i|\\) cardinality \\(X_i\\) \\(\\sigma(X_i)\\) standard deviation original liver fat values.\r\nheuristic selects \\(\\mathsf{splitAge}\\) \\(SwSD\\) minimal.\r\nPartitionF, minimum value 7.44 age 52, .e., close onset menopause.\r\nFigure 3.6: Distribution liver fat concentration partition. Distribution liver fat concentration male participants (PartitionM), females younger older 52 years. horizontal axis shows liver fat concentration bins 5%, vertical axis indicates number participants bin.\r\nhistograms Figure 3.6 depict differences liver fat concentration distributions age cutoff value 52.\r\nNext PartitionM (n=264), show subpartitions \\(\\mathsf{F:age\\leq{}52}\\) (n=131) F:age>52 (n=183) PartitionF.\r\nfemale participants \\(\\mathsf{F:age\\leq{}52}\\) 5% liver fat concentration, ca. 95% 10%, .e., belong negative class .\r\ncontrast, ca. 28% F:age>52 liver fat concentration 10%; belong positive classes B C.","code":""},{"path":"imm.html","id":"imm-experiments","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3 Experiments and Findings","text":"","code":""},{"path":"imm.html","id":"imm-experiments-trees","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.1 Results of Decision Tree Classifiers","text":"evaluation decision tree classifiers, consider accuracy, .e., ratio correctly classified participants, sensitivity specificity, F-measure, .e., harmonic mean precision recall.\r\nconsider two classes B C together positive class specificity, precision, recall.Oversampling achieved best performance accuracy 80% F-measure score 62%.\r\nfound best decision trees F:age>52, followed PartitionF, PartitionM.\r\nlarge discrepancy accuracy F-measure scores also appears partitions’ models, suggesting accuracy scores unreliable skewed distribution. Therefore, report accuracy .partition F:age>52, overall best decision tree achieved oversampling variant.\r\nlarger PartitionF, best performance achieved decision tree created InfoGain variant.\r\ncontrast, best decision tree PartitionM created CostMatrix variant.\r\nsensitivity specificity values trees given Table 3.2, trees shown Figures 3.7 - 3.9 discussed Section 3.3.3.\r\nTable 3.2: Best decision trees three partitions. Best separation achieved F:age>52; PartitionM heterogeneous one, performance values lowest.\r\nTable 3.2 indicates decision tree variants perform differently different partitions.\r\nOversampling beneficial F:age>52 partially compensates class imbalance problem.\r\nPartitionM heterogeneous class distribution partitions, variants perform relatively poorly .\r\nHence, expected insights decision trees F:age>52 PartitionF, better separation achieved.\r\nFigure 3.7: Best decision tree F:age>52, achieved variant Oversampling.\r\n\r\nFigure 3.8: Best decision tree PartitionF, achieved variant InfoGain.\r\n\r\nFigure 3.9: Best decision tree PartitionM, achieved variant CostMatrix.\r\n","code":""},{"path":"imm.html","id":"imm-experiments-rules","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.2 Discovered Classification Rules","text":"classification rules found HotSpot whole dataset conclusive class positive classes B C, omit report rules useful diagnostic purposes.\r\nclassification rules found partitions informative.\r\nHowever, classification rules one feature antecedent low confidence.\r\nensure high confidence, restricted output rules least two features antecedent.\r\nensure still high coverage, allowed three features.\r\nselection high confidence high coverage rules partition class shown Tables 3.3 - 3.5, respectively.\r\ndescribe important features antecedent rules next subsection, together important features best decision trees.\r\nTable 3.3: Classification rules (PartitionF). Best HotSpot classification rules (maxLength = 3) PartitionF (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; blt_beg_s2: time blood sampling; ggt_s_s2: serum Gamma-glutamyltransferase (GGT; \\(\\mu\\)mol/sl); gluc_s_s2: serum glucose (mmol/l); gx_rs11597390: genetic marker; hrs_s_s2: serum uric acid concentration (µmol/l); ldl_s_s2: serum low-density lipoprotein (LDL; mmol/l); sleeph_s2: sleep hours; sleepp_s2: sleep problems; som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) alanine aminotransferase (ALAT) concentration \\(\\geq\\) 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis); tg_s_s2: serum triglycerides (mmol/l); tsh_s2: thyroid-stimulating hormone (TSH; mu/l).\r\n\r\nTable 3.4: Classification rules (F:age>52). Best HotSpot classification rules (maxLength = 3) F:age>52 (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; crea_u_s2: urine creatinine (mmol/l); fib_cl_s2: fibrinogen (Clauss) (g/l); gluc_s_s2: serum glucose (mmol/l); ggt_s_s2: serum Gamma-glutamyltransferase (GGT; \\(\\mu\\)mol/sl); gx_rs11597390: genetic marker; hdl_s_s2: high-density lipoprotein (mmol/l); hrs_s_s2: serum uric acid concentration (µmol/l); som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) alanine aminotransferase (ALAT) concentration \\(\\geq\\) 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis).\r\n\r\nTable 3.5: Classification rules (PartitionM). Best HotSpot classification rules (maxLength = 3) PartitionM (excerpt). Cov: coverage; Sup: support; Conf: confidence. age_ship_s2: age; ATC_C09AA02_s2: enalapril intake; chol_s_s2: serum cholesterol (mmol/l); crea_u_s2: urine creatinine (mmol/l); crea_s_s2: serum creatinine (µmol/l); fig_cl_s2: Fibrinogen (Clauss) (g/l); ggt_s_s2: serum Gamma-glutamyltransferase (GGT; \\(\\mu\\)mol/sl); gout_s2: treated gout (self-report); hdl_s_s2: high-density lipoprotein (mmol/l); hgb_s2: haemoglobin (g/l); hrs_s_s2: serum uric acid concentration (µmol/l); jodid_u_s2: urine iodide (µg/dl); quick_s2: thromboplastin time Quick test (%); sleeph_s2: sleep hours; stea_alt75_s2: hepatic steatosis (ultrasound diagnosis) alanine aminotransferase (ALAT) concentration \\(\\geq\\) 0.55 µmol/sl – 0 = normal, 1 = hypoechogenic, 2 = hyperechogenic, 3 = questionable; stea_s2: hepatic steatosis (ultrasound diagnosis); som_bmi_s0: body mass index; som_huef_s0: hip circumference (cm); som_waist_s2: waist circumference (cm); tg_s_s2: serum triglycerides (mmol/l).\r\n","code":""},{"path":"imm.html","id":"imm-experiments-important-features","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.3 Important Features for Each Subpopulation","text":"important features decision trees Figures 3.7 - 3.9 closer root.\r\nreadability, tree nodes figures contain short descriptions instead original variable names.\r\nthree decision trees, root node ultrasound diagnosis variable stea_s2.\r\nnegative ultrasound diagnosis points negative class , positive ultrasound diagnosis directly lead positive classes B C.\r\ndecision trees three partitions differ nodes placed near root.Important features PartitionF. best decision tree PartitionF (cf. Figure 3.8), can observed ultrasound report positive HbA1C concentration 6.8%, class C.\r\nclassification rules high coverage confidence Table 3.3) point interesting features:\r\nwaist circumference 80 cm, BMI 24.82 kg/m2, hip circumference 97.8 cm less characterize participants negative class.\r\n6 participants serum glucose concentration greater 7 mmol/l TSH concentration greater 0.996 mu/l belong class C.\r\n, severe obesity (BMI value 38.42 kg/m2 points class C high confidence – combination variables.Important features F:age>52 contrast best tree PartitionF, best decision tree subpartition F:age>52 (cf. Figure 3.7) also contains nodes SNPs, indicating potentially genetic associations fatty liver participants.\r\nClassification rules high coverage confidence class B also contain SNPs, shown Table 3.4.\r\nSimilar PartitionF, high BMI values point positive class combined features.\r\nTable 3.4 shows four participants stea_alt75_s2 = 3 (.e., positive ultrasound diagnosis combined critical ALAT value) BMI larger 38.42 kg/m2 belong class C.\r\nsimilar association holds stea_alt75_s2 = 3 combined high waist circumference (> 124 cm).\r\n19 20 participants class B positive ultrasound diagnosis, genetic marker gx_rs11597390 = 1, high-density lipoprotein (HDL) serum concentration 1.53 mmol/l.Important features PartitionM. role ultrasound report predicting negative class PartitionM (cf. Figure 3.9 PartitionF).\r\nbest tree F:age>52, best tree PartitionM contains nodes SNPs serum Gamma-glutamyltransferase (GGT) value ranges.\r\nfeatures also antecedent top Hotspot rules (cf. Table 3.5): Serum GGT concentration 1.9 \\(\\mu\\)mol/sl combination creatinine concentration 90 mmol/l thromboplastin time ratio (quick_s2) 59% points class C.\r\nSimilarly, positive ultrasound diagnosis serum HDL concentration exceeding 0.84 mmol/l point class C.decision trees classification rules provide insights features appear diagnostically important.\r\nHowever, medical expert needs additional information decide whether feature worth investigation.\r\nDecision trees highlight importance feature context subtree found; subtree describes typically small subpopulation.\r\ncontrast, classification rules provide information larger subpopulations.\r\nHowever, subpopulations may overlap; example, first four rules class C PartitionM (cf. Table 3.5) may refer 6 participants.Furthermore, unless classification rule confidence value close 100%, participants classes may also support .\r\nTherefore, decide whether features antecedent rule deserve investigation, expert also needs knowledge statistics rule classes.\r\nassist expert task, proposed Interactive Medical Miner.\r\ntool discovers classification rules class provides information statistics rules classes.","code":""},{"path":"imm.html","id":"imm-conclusion","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.4 Conclusion","text":"date, analysis population-based cohort data mostly hypothesis-driven.\r\npresented workflow interactive application data-driven analysis population-based cohort data using hepatic steatosis example.\r\nmining workflow includes stepsto discover subpopulations different distributions respect target variable,classify subpopulation taking class imbalance account, andto identify variables associated target variable.workflow shown appropriate () build subpopulations classification reduce class imbalance (b) drill-derived models identify important variables subpopulations worthy investigation.assist domain expert latter objective (b), developed Interactive Medical Miner, interactive application allows user explore classification rules understand cohort participants supporting rule distributed across three classes.\r\nexploration step essential identifying -yet-known associations variables target.\r\nvariables must investigated – hypothesis-driven studies.\r\nTherefore, workflow Interactive Medical Miner carry potential data-driven analysis provide insights multifactorial disease generate hypotheses hypothesis-driven studies.\r\nInteractive Medical Miner extended Schleicher et al. [114], added panels include tables showing additional rule statistics lift p-value.\r\nBesides, mosaic plot contrasts class distributions subpopulation “complements,” .e., subsets participants meet one conditions length-2 rule describing subpopulation.terms multifactorial disorder interest, results confirm potential data-driven approach variables top positions decision trees classification rules previously shown associated hepatic steatosis independent studies.\r\nparticular, indices fat accumulation body (BMI, waist circumference) liver enzyme GGT proposed Bedogni et al. [115] reliable “Fatty Liver Index.”\r\nAccording Yuan et al. [116], SNPs rs11597390, rs2143571, rs11597086 among “Independent SNPs Associated Liver-Enzyme Levels Genome-wide Significance Combined GWAS Analysis Discovery Replication Data Sets.”\r\nRegarding effects alcohol consumption, toxic effects alcohol liver well established ascribe even significant role obesity heavy alcohol consumption concerning fat accumulation liver [117], [118].\r\nIndeed, variable related alcohol consumption appears decision tree F:age>52 (see Figure 3.7) among top classification rules, tend see variables associated person’s weight obesity (cf. variables som_bmi_s2, som_huef_s2, som_waist_s2 figures tables Section 3.3).\r\nsubpopulation F:age>52 identified without prior knowledge subpopulation’s semantics.\r\nStill, noteworthy age 52 years close onset menopause – Völzke et al. [119] showed menopausal status associated hepatic steatosis.\r\nresults also verify another fact known medical experts independent observation: sonographic variables (cf. stea_s2, stea_alt75_s2 figures tables Section 3.3) associated liver fat concentration found MRI, ultrasound alone predict hepatic steatosis [115], [118].algorithms provide variables associated target also identify value intervals related specific class, see, example, value intervals BMI associated class B PartitionM (Table 3.5) classes C F:age>52 (Table 3.4).\r\nintervals imply person BMI within specific interval actually belongs corresponding class can serve starting point hypothesis-driven analyses.approach allows us study subpopulations two points time.\r\nmodeling step, identify subpopulations different class distributions.\r\nmodeling step, Interactive Medical Miner highlights subpopulation supports classification rule; overlapping subpopulations.\r\nOverlapping subpopulations necessarily disadvantage, especially small subpopulations.\r\nHowever, working overlapping data sets can unintuitive tedious domain expert.\r\nChapter 4, explore potential clustering identify reorganize overlapping rules reduce user’s cognitive load displaying semantically unique representative rules.Although workflow’s main application longitudinal cohort study, exploit temporal characteristics data.\r\nHowever, indicators lifestyle change potentially predictive later occurrence disease.\r\nChapter 6 presents follow-method expands feature space extracting temporal variables describe study participant changes time derive new informative variables hypothesis generation.","code":""},{"path":"sdclu.html","id":"sdclu","chapter":"4 Identifying Distinct Subpopulations","heading":"4 Identifying Distinct Subpopulations","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Bernhard Preim, Till Ittermann, \r\nHenry Völzke. “Combining Subgroup Discovery Clustering Identify\r\nDiverse Subpopulations Cohort Study Data.” : Computer-Based Medical Systems (CBMS). 2017, pp. 582-587. DOI: 10.1109/CBMS.2017.15.Epidemiologists search significant relationships risk factors target variable large heterogeneous datasets encompass participant health information gathered questionnaires medical examinations [80].\r\nprevious chapter describes expert-driven workflow can help epidemiologists automatically detect relationships form classification rules, descriptions risk factors predictive value ranges specific subpopulation target variable interest.\r\nHowever, rule induction algorithms often produce large overlapping rule sets requiring expert manually pick informative rules remove less informative redundant ones.\r\npost-filtering step time-consuming tedious.chapter presents clustering-based algorithm hierarchically reorganizes large rule sets summarizes essential subpopulations maintaining distinctiveness clusters.\r\ncluster, representative rule shown expert can drill cluster members.\r\nevaluate algorithm two subsets SHIP target variables hepatic steatosis goiter serve target variables, respectively.\r\n, report effectiveness algorithm, present selected subpopulations.propose SD-Clu, approach combines subgroup discovery clustering return \\(k\\) representative classification rules.\r\nBuilding upon set potentially highly overlapping rules generated SD algorithm, leverage hierarchical agglomerative clustering find groups rules cover different sets instances.\r\ncluster, nominate one rule group’s representative exhibits best tradeoff rule confidence coverage towards target variable.\r\ndefine similarity pair subgroups based fraction mutually covered instances individually covered instances.\r\nRules covering (almost) instances likely condensed cluster thus likely represented proxy rule.\r\nSection 4.1 serves motivation related field subgroup discovery redundancy classification rules.\r\n\r\nSection 4.2 presents SD-Clu algorithm generates distinct rules.\r\nSection 4.3 describes experimental setup.\r\nSection 4.4, report evaluation results.\r\nSection 4.5, discuss findings regarding hepatic steatosis goiter SHIP data.\r\nSection 4.6, summarize contributions.\r\n","code":""},{"path":"sdclu.html","id":"brief-chapter-summary-1","chapter":"4 Identifying Distinct Subpopulations","heading":"Brief Chapter Summary","text":"Subgroup discovery algorithms often generate redundant descriptions subpopulations.\r\npresent workflow extract small number representative rules large set classification rules.\r\n-called proxy rules minimize instance overlaps across rule groups, thus covering different subpopulations.\r\nevaluate workflow SHIP data samples hepatic steatosis goiter target variables, respectively.","code":""},{"path":"sdclu.html","id":"sdclu-intro","chapter":"4 Identifying Distinct Subpopulations","heading":"4.1 Motivation and Comparison to Related Work","text":"Subgroup discovery (SD) algorithms aim uncover “interesting” relationships one conditions (variables value ranges) target variable form classification rules [110], [120].\r\nCompared accurate predominantly opaque black-box models neural networks, support vector machines, random forests, SD algorithms yield interpretable results, making highly suitable domain expert-guided subpopulation discovery.\r\nSD algorithms used several medical studies descriptive knowledge needed inferred, e.g., extract potential drug targets multi-relational data sources treatment dementia [121], identify predictive auditory-perceptual, speech-acoustic, articulatory-kinematic features preschool children speech sound disorders [122], discover discriminative features patient subpopulations different admission times psychiatric emergency departments [123].However, SD methods often yield large sets rules domain experts willing tediously go manually separate interesting irrelevant redundant ones.\r\ncommon observation groups rules cover almost set instances, shown Figure 4.1.\r\nInstead presenting rules found SD algorithm , propose organize rule sets hierarchically domain expert can explore compact set different subpopulations, equipped mechanisms drill specific rules interest.\r\nFigure 4.1: Example redundant rules. \\(r_2\\) \\(r_3\\) cover instances 7,8,9, 10; \\(r_3\\) additionally covers instance 2. cover set overlap due high correlation #Teeth Age; BMI Waist circumference. rules describe subpopulation, .e., elderly overweight people higher risk developing disease.\r\nSimilar HotSpot algorithm described Section 3.2.3, popular SD algorithms SubgroupMiner [124],\r\nSD [125], CN2-SD [126] use fixed beamwidth [109] limit number expanded subgroup candidates iteration.\r\npost-pruning step can applied reduce size rule set – e.g., return top k rules – using quality criterion Weighted Relative Accuracy [127] p-value statistical test.\r\nEven beamwidth search top k pruning applied, result often still contains redundant rules.\r\ndue correlation (non-target) variables, leads large number variations given finding, cf. Figure 4.2 illustrative example.\r\nparticular, top k pruning leads different variations subpopulation (.e., high instance overlap among rules) [128].\r\nFigure 4.2: Illustration rule redundancy. Graphical representation 1115 HotSpot rules found SHIP data 886 labeled instances. gray cell indicates instance x-position covered rule associated y-position. Instances rules sorted agglomerative hierarchical clustering. partitioning 10 clusters based covered instances, cluster’s rules describe similar subpopulations.\r\nUnlike SD algorithms generate multiple rules overlap terms coverage sets, predictive rule learning algorithms CN2 [129] RIPPER [130] designed generate rules capture different subpopulations.\r\nwork iteratively according divide--conquer strategy [109] follows:\r\nFirst, rule maximizes algorithm’s quality function generated set instances yet covered.\r\nSecond, covered instances removed training set.\r\nprocess rule induction removal instances training set repeated instances covered least one rule.\r\noutput algorithm often decision list.\r\nclassify instance, prediction first rule covers instance used.\r\nalgorithms avoid rule redundancy, important subpopulations may remain undiscovered order dependencies.\r\nexample, algorithm might induce rule includes instances high BMI, might find slightly weaker association income instances immediately removed instance candidate space.\r\nAlso, coverage rules generally decreases iteration.\r\nRules low coverage negligible epidemiological studies may represent spurious correlations study sample.Instead merely eliminating covered instances rule induction subsequent iterations, weighted covering approaches consider many times instances covered far rule candidate expansion step [126].\r\nleniency removing instances allows larger number rules, new hyperparameter introduced control tradeoff reusing already covered instances minimum rule confidence.\r\nparameter unintuitive difficult set, especially domain experts.\r\nMoreover, traditional predictive rule learning algorithms weighted covering extensions introduce order dependencies rules: rule depends previous rules rule list instances target variable covers.\r\nmay make sense interpret single rule.","code":""},{"path":"sdclu.html","id":"sdclu-method","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2 Finding Distinct Classification Rules","text":"section presents algorithm SD-Clu, combines subgroup discovery clustering return set \\(k\\) distinct classification rules.\r\nalgorithm consists three main steps.\r\nFirst, SD algorithm generates set (potentially highly overlapping) rules.\r\nUsing hierarchical agglomerative clustering, set rules grouped distinct rule clusters covering different sets instances.\r\ncluster, rule best tradeoff confidence coverage target variable appointed representative group.\r\nRules covering instances grouped cluster therefore represented proxy rule.","code":""},{"path":"sdclu.html","id":"sdclu-method-proxy-rules","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.1 Rule Clustering and the Concept of “Proxy Rules”","text":"use notation classification rule discovery Section 3.2.1.\r\nAgglomerative hierarchical clustering iteratively merges clusters similar instances bottom-way.\r\n, instances rules.\r\nHence alternative definition distance measure required.\r\norder merging two clusters depends linkage strategy.\r\ncomplete linkage, distance two clusters defined maximum distance two instances.\r\npair clusters minimizing maximum distance selected merging.\r\n\r\ndefine rule similarity clustering basis mutually covered instances adaption Sørensen–DICE coefficient [131].\r\ndistance two rules \\(r_1\\), \\(r_2\\) corresponding subpopulations (cover sets) \\(s(r_1)\\), \\(s(r_2)\\) given \r\n\\[\\begin{equation}\r\n\\text{dist}(r_1,r_2) = 1 - \\frac{2\\cdot\\left|s(r_1)\\cap s(r_2)\\right|}{\\left|s(r_1)\\right| + \\left|s(r_2)\\right|}.\r\n\\tag{4.1}\r\n\\end{equation}\\]propose two ways completing hierarchical rule clustering process:Specification maximum number clusters \\(k\\)Discovery optimal \\(k\\) based internal clustering index Silhouette coefficient.\r\n\r\nclustering \\(\\xi\\) set rules \\(R\\), Silhouette coefficient calculated \r\n\\[\\begin{equation}\r\n\\text{Silh}(R,\\xi) = \\frac{1}{|R|}\\sum_{r\\R}{\\frac{b(r)-(r)}{\\max\\left\\{(r), b(r)\\right\\}}}\r\n\\tag{4.2}\r\n\\end{equation}\\]\r\n\r\n\\[\\begin{equation}\r\n(r)=\\frac{\\sum_{y\\{}Y}\\text{dist}(r,y)}{|Y|-1}\r\n\\tag{4.3}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Y\\\\xi\\) contains \\(r\\), \r\n\\[\\begin{equation}\r\nb(r)=\\frac{\\sum_{y\\{}Z}\\text{dist}(r,y)}{|Z|}\r\n\\tag{4.4}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Z\\\\xi\\) closest cluster \\(Y\\) containing \\(r\\).\r\nBasically, cluster quality function used.\r\nchoose Silhouette coefficient understandable interpretation.\r\nvalue silhouette coefficient can vary -1 1.\r\nnegative value undesirable represents case average distance rules cluster greater minimum average distance rules another cluster.\r\nprefer positive silhouette coefficient, .e., value close maximum 1., traverse dendrogram bottom-, compute Silhouette set clusters \\(\\xi\\), select \\(\\xi_{opt}\\) set clusters best Silhouette value.\r\noptimal number clusters cardinality \\(|\\xi_{opt}|\\).\r\nFinally, map cluster \\(Y\\\\xi_{opt}\\) representative rule.\r\n, invoke rule interestingness measure Weighted Relative Accuracy [127] (WRA hereafter), balances coverage confidence gain defined \r\n\\[\\begin{equation}\r\nWRA(r)=Cov(r)\\cdot\\left(Conf(r)-\\frac{n_{T=v}}{N}\\right)\r\n\\tag{4.5}\r\n\\end{equation}\\]\r\n\\(N\\) total number instances dataset \\(n_{T=v}\\) number instances target variable value interest.\r\ncompute WRA rule \\(r\\{Y}\\) select cluster proxy \\(cp(Y)\\) rule WRA maximum.","code":""},{"path":"sdclu.html","id":"representativeness-of-a-set-of-proxy-rules","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.2 Representativeness of a Set of Proxy Rules","text":"proxy rules good representation total rule set.\r\nThus, instance covered equally often cluster proxies compared total rule set.\r\nHence, define representativeness difference average fraction proxy rules instances covered total average fraction rules instances covered .\r\ndifference two ratios small, proxy rules’ representativeness high.Typically, set rule clusters \\(\\zeta\\) set rules \\(R\\) optimal set clusters, described previous subsection, can set clusters chosen user, long contains rules \\(R\\).\r\n\\(\\zeta\\), let \\(R_{\\zeta}=\\{cp(Y)|Y\\\\zeta\\}\\) denote set proxy rules.\r\nquantify representative set rules , proceed follows.\r\nFirst, let \\(U\\subseteq{}R\\) arbitrary subset complete set rules, let \\(x\\) data instance.\r\ncoverage rate \\(x\\) towards \\(U\\) calculated \r\n\\[\\begin{equation}\r\ncovRate(x,U) = \\frac{\\sum_{r\\{}U}isCovered(x,r)}{|U|}\r\n\\tag{4.6}\r\n\\end{equation}\\]\r\n\\(isCovered(x,r)\\) equal 1, \\(r\\) covers \\(x\\), .e., \\(x\\s(r)\\), \\(0\\) otherwise.\r\nobserve set rules \\(U\\), instance \\(x\\) covered \\(|U|\\) rules.\r\nLet \\(R_x\\) set rules cover instance \\(x\\), .e., \\(R_x=\\{r\\{}U | isCovered(x,r)=1 \\}\\).\r\nwhole set instances \\(X\\), create bins:\r\n\\[\\begin{equation}\r\nbin_i(U)= \\{x \\{}X | |R_x|=\\}.\r\n\\tag{4.7}\r\n\\end{equation}\\]\r\n, let \\(bin_0(U)= \\{x \\{}X | \\forall r\\U : isCovered(x,r)=0 \\}\\).\r\ninstance \\(x\\) can covered \\(0, 1, \\ldots, |U|\\) rules, .e., \\(covRate(x,U)\\) can take one \\(|U|+1\\) values.\r\ncontrast, \\(covRate(x,R)\\) can take one \\(|R|+1\\) values, usually much larger number.\r\nTherefore, map possible values \\(covRate(x,R)\\) much smaller set possible values rounding, computing:\r\n\\[\\begin{equation}\r\nadjCovRate(x,U, R)=\\frac{\\lfloor covRate(x,R)\\cdot|U| \\rceil}{|U|}\r\n\\tag{4.8}\r\n\\end{equation}\\]\r\n\\(\\lfloor\\rceil\\) rounding operator.\r\n, complete set instances \\(X\\), set induced rules \\(R\\), clustering \\(\\zeta\\) \\(R\\) set proxy rules \\(R_{\\zeta}\\), \\(representativeness\\) \\(R_{\\zeta}\\) defined \r\n\\[\\begin{equation}\r\nrepresentativeness(R_{\\zeta},R)=1-\r\n\\frac{1}{|X|}\\sum_{x\\{}X} |adjCovRate(x,U,R) - covRate(x,R_{\\zeta})|.\r\n\\tag{4.9}\r\n\\end{equation}\\]investigate \\(representativeness\\) changes increasing number proxy rules \\(k\\) different criteria select cluster’s proxy rule.\r\nIdeally, \\(representativeness\\) high \\(k\\) low.\r\nwords, number proxy rules small possible, rules cover important subpopulations within data.\r\nfixed \\(k\\), compare strategy selecting cluster’s rule highest WRA proxy rule three baselines: top \\(k\\) rules according () WRA, (ii) odds ratio, (iii) coverage.","code":""},{"path":"sdclu.html","id":"sdclu-experiments","chapter":"4 Identifying Distinct Subpopulations","heading":"4.3 Experimental Setup","text":"Datasets.\r\nevaluate method, used data SHIP study.\r\ndescription SHIP, see Section 2.2.1.\r\nconsidered hepatic steatosis (see Section 3.2.5) goiter target variables.\r\nsample HepStea, derived binary target variable discretizing liver fat concentration obtained MRI report, study participants concentration greater 10% assigned negative class, values greater 10% assigned positive class indicating presence disease.\r\n886 participants MRI report SHIP-2 available time, 694 (78.3%) negative, 192 (21.7%) positive.\r\nconsidered 99 variables selected exclusively SHIP-0 assess long-term effects expressed ten years later SHIP-2.\r\nGoiter sample, target variable derived thyroid ultrasound.\r\npresence goiter defined thyroid volume greater 18 ml women 25 ml men [132].\r\n4400 participants target variable available TREND-0, 3010 belong negative class (68.4%) 1390 (31.6%) positive class.\r\nApart target variable, use total 182 variables pre-selected medical expert potential risk factors.SD algorithms.\r\nsubgroup discovery, use approach Chapter 3, algorithm HotSpot [133] (Section 3.2.3).\r\ntry SD-Map [134], exhaustive algorithm adapts popular FP-Growth association rule learning method [90].\r\nRules fall minimum coverage threshold pruned.\r\ndepth-first search performed candidate generation.\r\nRules ranked according user-defined quality function.\r\nuse implementation VIKAMINE framework [135].\r\nimplementation SD-Map supports categorical variables.\r\nTherefore, numeric variable discretized using minimum description length based approach Fayyad Irani [136].SD-Map, set minimum coverage threshold 0.05 avoid overfitting, small rules.\r\nuse WRA quality function define minimum threshold 0.025.\r\nHotSpot, set support threshold rule 0.05.\r\nbeamwidth set 500.\r\nFurthermore, avoid rather meaningless literals, restrict extension rule body another literal relative confidence gain least 0.3.\r\navoid many overly specific rules cover small number study participants, limit length rule body, .e., number literals 3.\r\ndetermine optimal number proxy rules \\(k\\), compute Silhouette coefficient every possible number clusters.","code":""},{"path":"sdclu.html","id":"sdclu-results","chapter":"4 Identifying Distinct Subpopulations","heading":"4.4 Results","text":"Figure 4.3 shows optimal number clusters study sample combination SD algorithm.\r\nTable 4.1 lists optimal \\(k\\) ratio proxy rules vs. total number rules.\r\nexample, clustering optimal Silhouette coefficient algorithm HotSpot Goiter 76 clusters thus 76 proxy rules (cf. Table 4.1), 21.3% total number rules.\r\nThus, cluster proxies initially displayed expert, time needed check rules reduced 78.7%.\r\nFigure 4.3: Silhouette coefficients (\\(Silh\\)) SD-Clu using complete linkage combination dataset algorithm. number clusters \\(k\\) highest \\(Silh\\) score (\\(|\\zeta_{opt}|\\)) indicated dashed vertical line.\r\n\r\nTable 4.1: Statistics best runs per dataset algorithm. Number rules \\(|R|\\), optimal Silhouette coefficient \\(Silh(\\zeta_{opt})\\), corresponding number rule proxies \\(k\\) optimal clustering \\(|\\zeta_{opt}|\\) percentage rule proxies relative total number rules every combination data sample SD algorithm.\r\noptimal number clusters \\(k\\) \\(|\\zeta_{opt}|\\) shown Figure 4.3 used suggestion, expert free specify number rules wish obtain.\r\nexample, expert considers \\(|\\zeta_{opt}|\\) = 100 large HotSpot HepStea, diagram show reduction \\(|\\zeta_{opt}|\\) = 58 possible, reducing \\(Silh\\) slightly 0.48 0.37.\r\nAlso direction: \\(|\\zeta_{opt}|\\) relatively low, diagram shows small increase change \\(Silh\\) much; therefore, added rules may also important.\r\nexpert even analyze diagram derive range instead single value, e.g., range \\(Silh\\) 90% maximum.assess representativeness cluster proxies, compare three baseline criteria return top \\(k\\) rules according odds ratio (baseline 1), coverage (baseline 2), WRA (baseline 3).\r\nFigure 4.4 juxtaposes representativeness SD-Clu three baselines different numbers rules \\(k\\) returned expert HotSpot algorithm sample HepStea.\r\nplot matrix’ panels arranged rule selection method (rows), number representative rules \\(k\\) returned expert (columns).\r\ngraph shows \\(adjCovRate\\) (y-axis) instances (x-axis) \\(R\\) (solid black curve).\r\ninstances sorted number rules \\(R\\) covered , respective \\(covRate\\) shown dots.\r\ndotted curve represents locally weighted scatterplot smoothing (LOWESS) [137] points.\r\nIdeally, curves close , meaning instances covered cluster proxy rules approximately proportion rules cover .\r\nFigure 4.4: Evaluation representativeness HepStea using HotSpot algorithm. \\(representativeness\\) SD-Clu three baseline approaches different numbers clusters \\(k\\) HepStea sample using HotSpot. Points depict instance’s \\(adjCovRate\\) set representative rules approach (row) \\(k\\) (column). Instances sorted \\(covRate\\) respect \\(R\\), .e., set rules) descending order, shown solid black curve. dotted curve depicts LOWESS regression fit points. similarity solid dotted curves illustrates \\(representativeness\\) top \\(k\\) rules respective approach, illustrated dark gray area -solid curve dotted curves. Smaller areas better reflect higher \\(representativeness\\) values.\r\napproaches, \\(representativeness\\) improves \\(k\\) increases.\r\nexample, increase \\(k\\) 10 50, \\(representativeness\\) increases 0.87 0.96 SD-Clu, means absolute difference \\(adjCovRate\\) \\(\\zeta\\) \\(covRate\\) \\(R\\) instances successively decreases.\r\n, given \\(k\\), representative rules baselines less representative SD-Clu’s proxy rules, e.g., 0.91, 0.92, 0.91 vs. 0.96 \\(k=50\\), respectively (cf. 5th column plot matrix Figure 4.4).","code":""},{"path":"sdclu.html","id":"sdclu-discussion","chapter":"4 Identifying Distinct Subpopulations","heading":"4.5 Discussion of Findings","text":"Tables 4.2  4.3 show antecedent, support, confidence proxy rules found two algorithms HepStea Goiter \\(k\\)=5.\r\nprevalence hepatic steatosis goiter significantly higher subpopulations described rules corresponding overall population.\r\nsubpopulations characterized known risk factors hepatic steatosis, large waist circumference BMI, blood pressure hypertension, advanced age, high values medical tests (ALAT LDL).\r\nFurthermore, apolipoprotein A1 (ApoA1), major protein component high-density lipoprotein (HDL) particles plasma, associated target variable elderly patients (see fourth hotspot rule Table 4.2).\r\nLipoprotein metabolism considered main process contributing development fatty liver [138].\r\nBesides, Poynard et al. [139] found patients hepatic steatosis higher levels ApoA1 patients hepatic fibrosis, turn higher levels patients cirrhosis.\r\nfifth HotSpot rule describes subpopulation elevated levels liver high-sensitivity C-reactive protein (CRP) (approximately 0.80-quantile) elevated levels uric acid (approximately 0.36-quantile).\r\nLizardi-Cervera et al. [140] reported increased ultra-sensitive CRP levels subjects hepatic steatosis independent metabolic states.\r\nSimilarly, Keenan et al. [141] found elevated uric acid levels patients hepatic steatosis independent metabolic syndrome.\r\nTable 4.2: Representative rules (HepStea). Proxy rules \\(k\\) = 5 HepStea sample positive value target variable.\r\nSimilarly, identified subpopulations goiter (see Table 4.3) characterized common risk factors, increased weight, body mass index angiotensin II receptor blocker intake (see second HotSpot rule).\r\nFurthermore, first HotSpot rule describes participants intima-media thickness greater 0.73 mm (approximately 0.80-quantile).\r\nPrevious studies found associations intima-media thickness thyroid-stimulating hormone [142] well subclinical hypothyroidism [143], [144].\r\ncondition third HotSpot rule describes duration ECG phase.\r\nJabbar et al. [145] summarized pathological thyroid hormone levels increase risk cardiovascular disease.\r\nassociation appears especially true elderly [146].\r\nfourth rule suggests certain thrombocyte levels indicate increased thyroid volume, confirms Erikci et al. [147], found hypothyroid patients higher platelet volume platelet distribution width control group.\r\nshows approach delivers relevant results application field; two chronic reversible conditions.\r\nTable 4.3: Representative rules (Goiter). Proxy rules \\(k\\)=5 Goiter sample positive value target variable.\r\n","code":""},{"path":"sdclu.html","id":"sdclu-conclusion","chapter":"4 Identifying Distinct Subpopulations","heading":"4.6 Conclusion","text":"proposed SD-Clu, algorithm rule clustering, identifying cluster-representative rules (“proxy rules”), minimizing proxy rules shown domain expert.\r\nalgorithm tackles problem high instance overlap sets rules generated subgroup discovery algorithms.\r\nlimiting number rules, time spent rule inspection reduced.\r\nSD-Clu nominates representative rule hierarchical clustering large set rules thus returns rules express distinct subpopulations, .e., rules cover different sets instances.\r\nintroduced representativeness measure assesses whether instances similarly often covered representatives total rule set.\r\nSD-Clu evaluated two samples epidemiological study optimal set proxy rules selected () contains considerably fewer rules total rule set (ii) representative compared baseline approaches, respectively.\r\nSD-Clu can applied subpopulation discovery epidemiological data high-dimensional medical datasets general.\r\nabsence concrete target variable, unsupervised methods detecting subpopulations (phenotypes) needed.\r\nFurthermore, sophisticated visualizations needed subpopulations characterized many variables rather just .\r\npresent solution challenges following chapter.","code":""},{"path":"phenotypes.html","id":"phenotypes","chapter":"5 Visual Identification of Informative Features","heading":"5 Visual Identification of Informative Features","text":"chapter partly based :Uli Niemann, Petra Brueggemann, Benjamin Boecking, Matthias Rose, Myra Spiliopoulou, Birgit Mazurek. “Phenotyping chronic tinnitus patients using self-report questionnaire data: cluster analysis visual comparison.” : Scientific Reports 10 (2020), pp. 1-10. DOI: 10.1038/s41598-020-73402-8.supervised methods subpopulation discovery described previous chapters show great potential applications one small number well-defined target variables.\r\nHowever, medical conditions multifaceted well understood yet.\r\nexample, chronic tinnitus complex, multifactorial heterogeneous disorder [46].\r\nClinical assessment selection suitable treatment difficult patients benefit equally type intervention [53], [148], [149].\r\nDue large number heterogeneity available assessment tools, phenotyping appears promising () describe subpopulations patients seeking treatment (b) understand response treatment within subpopulation [52], [53], [150].\r\nClinical acceptance empirical results can strengthened comprehensive visualizations intuitively illustrate phenotype’s major characteristics differences multiple phenotypes.chapter describes workflow todetermine distinct phenotypes medical conditions parameter-free clustering algorithm high-dimensional data,visualize phenotypes explore compare essential subpopulation characteristics, andinspect interactive web application.workflow’s effectiveness demonstrated CHA dataset (Section 2.2.2) exploring phenotypes tinnitus patients.chapter organized follows.\r\nSection 5.1 describes clinical value patient stratification, briefly reviews previous approaches example disorder, discusses significant challenges phenotyping high-dimensional data.\r\npresent workflow components Section 5.2, namely clustering algorithm, novel visualization types, web application.\r\nSection 5.3 lists features used clustering provides details CHA data subset used workflow evaluation.\r\nSection 5.4, present discovered tinnitus phenotypes describe characteristics.\r\nSection 5.5, discuss findings medical perspective compare related work.\r\nFinally, conclude chapter summary outlook Section 5.6.","code":""},{"path":"phenotypes.html","id":"brief-chapter-summary-2","chapter":"5 Visual Identification of Informative Features","heading":"Brief Chapter Summary","text":"Knowledge different disease phenotypes can help understand () patient subpopulations seek treatment (b) response treatment within subgroup.\r\npresent workflow () determine distinct phenotypes medical conditions high-dimensional data, (ii) visualize phenotypes explore compare essential subpopulation characteristics, (iii) interactively inspect change time interactive web application.\r\nevaluate workflow CHA data identifying four distinct phenotypes tinnitus patients.","code":""},{"path":"phenotypes.html","id":"phenotypes-motivation","chapter":"5 Visual Identification of Informative Features","heading":"5.1 Motivation and Comparison to Related Work","text":"Challenges management treatment tinnitus primarily caused clinical heterogeneity, including individual perception, risk factors, comorbidities, degrees perceived stress, treatment response (cf. Section 2.2.2).\r\nfactors make difficult clinicians () choose treatment effective individual patient (b) design unified treatment strategy patients equally benefit.\r\nawareness existence distinct patient subgroups may stimulate development effective therapy modules [151].\r\nSince clinically relevant subgroups established yet, clustering emerges promising approach identify characteristic tinnitus phenotypes data-driven hypothesis-free way.Previous studies found subgroups tinnitus patients cluster analysis based small number audiometric features [52], combination features extracted self-reports, audiometry, psychoacoustics [53], neuroimaging data \r\nsocio-demographics [150].\r\nAlthough studies provided insights tinnitus subgroup patterns, acceptance among medical scholars may increased presenting clustering results intuitive visualizations show individual subgroup patterns enable visual juxtaposition multiple subgroups high-dimensional data.Addressing requirement, Schlee et al. [152] proposed compact radar graph compare degree health burden individuals subgroups based measurements self-report questionnaires.\r\nvisualization applied disease domain, Schlee et al. demonstrated efficacy showing subgroup differences regarding measurements tinnitus distress associated comorbidities.\r\nHowever, aim visualize clustering results.\r\nInstead, restricted pre-defined cohorts graphically comparing female male patients patients low tinnitus frequency patients high tinnitus frequency.Challenges clustering high-dimensional data.\r\nseveral challenges clustering high-dimensional medical datasets:can determine appropriate number clusters absence ground truth?can represent high-dimensional data compactly also faithfully?can visualize essential characteristics high-dimensional clusters?present related work challenge .Determination appropriate number clusters absence ground truth.\r\nPractical considerations data clustering include set number clusters \\(k\\).\r\nSince ground truth often available, several heuristics automatically determine \\(k\\) proposed.\r\npopular approach -called “elbow” method, involves running clustering algorithm different \\(k\\) values, cf. Section 6.3.1 application elbow method density-based clustering.\r\nnumber clusters plotted cluster compactness.\r\npopular k-means algorithm, cluster compactness quantified total within-sum squares (WSS), sum squared distances observation centroid clusters.\r\nWSS similar goodness fit measures increase monotonically increasing \\(k\\), idea elbow method identify curve’s characteristic “knee point,” first point adding another cluster leads minor improvement compactness.\r\nplot guaranteed exhibit distinctive knee point universal compactness thresholds exist, approach sometimes impracticable.\r\nAnother popular clustering evaluation measure Silhouette coefficient (see Section 4.2.1), favors clusterings assign similar objects cluster dissimilar objects different cluster.\r\nInstead evaluating clustering quality post-hoc, decided leverage algorithm automatically determines suitable number subgroups already clustering.Dimensionality reduction.\r\nDimensionality reduction (DR) techniques often used project original high-dimensional data onto low-dimensional projection allows simple visualization types scatterplots used.\r\nIdeally, DR projection preserves original data’s essential structures, relative pairwise distance, clusters, outliers, correlations.\r\nPrincipal Component Analysis [153] (PCA) seminal DR algorithm generates linear, orthogonal combinations original dimensions.\r\nnew dimension, called principal component, contains loading indicating much variability data covers.\r\nTypically, first two three dimensions carry highest loads selected visualization.\r\nPCA robust outliers capture nonlinear relationships.\r\nMultidimensional scaling [154] (MDS) another early DR technique emphasizes preservation pairwise distances, .e., objects close high-dimensional space also close low-dimensional projected space.\r\ncomplex, arbitrarily shaped structures, pairwise distances may subject curse dimensionality, leading poor results.\r\n\\(t\\)-stochastic neighborhood embedding [155] (\\(t\\)-SNE) Uniform Manifold Approximation Projection [156] (UMAP) nonlinear dimensionality reduction methods represent matrix pairwise similarities.\r\nidea preserve global structures clusters local structures distances neighbors.\r\n\\(t\\)-SNE UMAP can produce superior projections compared traditional linear techniques, provided hyperparameters appropriately tuned.\r\nHowever, shortcoming techniques degree preservation contribution original features measured.\r\nMoreover, projection applied new observations; instead, new projection must recomputed.\r\nstochasticity, different runs hyperparameters may yield different results.\r\nMedical researchers usually interested understanding relationship original variables new dimensions draw actionable conclusions patterns found projected space.\r\nSince semantics original dimensions lost DR, prefer maintain high-dimensional space clustering visualization.Visualization high-dimensional clusters.\r\nVisualizing clusters high-dimensional data challenging.\r\nScatterplot matrices (SPLOMs) can intuitively represent \r\nrelationship pairs features matrix two-dimensional scatterplots [101], [157].\r\nHowever, number features increases, number scatterplots grows quadratically, leading scalability problems overplotting.\r\nSeveral advanced visualization techniques proposed remedy, merely adding transparency colors points sophisticated density contours, hexagon binning, layers aggregated geometric features (minimal spanning trees, alpha shapes, convex hulls), animation, combinations several techniques \r\nsplatterplots [158].\r\nHowever, SPLOMs traditional visualization techniques parallel coordinate graphs [159] still suitable low-dimensional data.\r\ngeneral, focus phenotype visualization represent specifics individual subjects show important general characteristics subpopulation.\r\nTherefore, represent patterns high-dimensional space, create multiple, often cluttered subplots, approaches , represent essential phenotype characteristics single visualization.\r\nprovide web application explore one juxtapose multiple phenotypes interactively.","code":""},{"path":"phenotypes.html","id":"phenotypes-phenotyping","chapter":"5 Visual Identification of Informative Features","heading":"5.2 Discovery and Visualization of Phenotypes","text":"propose workflow determining, visualizing, inspecting essential phenotypes medical condition high-dimensional datasets.\r\nfollowing, present individual steps workflow.\r\nSection 5.2.1 describes clustering algorithm X-means, internally determines appropriate number phenotypes.\r\n, present solution visualizing phenotypes radial bar graphs radar graphs Section 5.2.2.\r\nSection 5.2.3 describes web application combines visualizations interactive elements phenotype inspection comparison.","code":""},{"path":"phenotypes.html","id":"phenotypes-phenotyping-xmeans","chapter":"5 Visual Identification of Informative Features","heading":"5.2.1 X-means Clustering","text":"X-means [160] parameter-free adaption popular k-means algorithm, incorporates Bayesian information criterion [161] (BIC) find good tradeoff low total sum squares small number clusters.Let \\(\\mathcal{D}\\) dataset \\(d\\) dimensions let \\(D\\) subset \\(\\mathcal{D}\\), .e., \\(D\\subseteq \\mathcal{D}\\).\r\nk-means clustering \\(D\\) creates set clusters\r\n\\(\\mathcal{C}=\\left\\{C_1,\\ldots,C_k,\\ldots,C_K\\right\\}\\), \\(c_k\\) centroid cluster \\(k\\), \\(r_k\\) number objects \\(D\\) assigned \\(C_k\\) \\(p\\) number free parameters, .e., \\(p = (d+1) \\cdot K\\).\r\nBIC cluster \\(C_k\\) using Schwarz criterion calculated \\[\\begin{equation}\r\n\\text{BIC}(C_k) = \\hat{l}_k(\\mathcal{D}) - \\frac{p_k}{2} \\cdot \\log |\\mathcal{D}| \r\n\\tag{5.1}\r\n\\end{equation}\\]\\(\\hat{l}_k(\\mathcal{D})\\) log-likelihood \\(\\mathcal{D}\\) according \\(C_k\\).\r\npoint probabilities computed \r\n\\[\\begin{equation}\r\n\\hat{P}(x_i)=\\frac{r_{()}}{|\\mathcal{D}|}\\cdot \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}}\\text{exp}\\left(\\frac{1}{2\\hat{\\sigma}^2}||x_i-c_{()}||\\right)\r\n\\tag{5.2}\r\n\\end{equation}\\]\r\n\r\nmaximum likelihood estimate variance (identical spherical Gaussian assumption) \r\n\\[\\begin{equation}\r\n\\hat{\\sigma}^2=\\frac{1}{|\\mathcal{D}|-K}\\sum_{=1}^{|\\mathcal{D}|}\\left(x_i - \\mu_{()}\\right)^2.\r\n\\tag{5.3}\r\n\\end{equation}\\]\r\nlog-likelihood \\(\\mathcal{D}\\) according \\(\\mathcal{C}\\) \r\n\\[\\begin{equation}\r\nl(\\mathcal{D})=\\log\\prod_{=1}^{|\\mathcal{D}|} P(x_i)=\\sum_{=1}^{|\\mathcal{D}|}\\left(\\log \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}} - \\frac{1}{2 \\sigma^2} ||x_i-c_{()}||^2 + \\log \\frac{r_{()}}{|\\mathcal{D}|}  \\right).\r\n\\tag{5.4}\r\n\\end{equation}\\]main steps X-means algorithm summarized Figure 5.1.\r\nstart, initial partitioning generated ordinary k-means \\(K\\) = \\(K\\)lower, \\(K\\)lower lower bound number clusters.\r\n, cluster bisected; resulting two child centroids placed opposite direction along randomly chosen vector distance proportional cluster radius.\r\npair child clusters, local k-means clustering \\(K=2\\) run.\r\nnew partitioning’s BIC score exceeds BIC score parental one, child centroids kept; otherwise, parent centroid retained.\r\niterative steps repeated cluster whose bisection leads better BIC score number clusters exceeds optional upper bound \\(K\\)upper.\r\nused R implementation Ishioka [162].\r\nSince aim restrict solution space respect number clusters, set \\(K\\)lower 2, .e., lowest possible value, set \\(K\\)upper.\r\n\r\nFigure 5.1: Principal steps X-means (simplified). figure adapted [160].\r\nStability clustering result.\r\nLike predecessor, X-means also non-deterministic algorithm since initial centroids’ positions set randomly, leading different clustering results.\r\nassess stability total cluster number, performed internal validation recorded number clusters generated X-means 500 bootstrap samples.","code":""},{"path":"phenotypes.html","id":"phenotypes-phenotyping-vis","chapter":"5 Visual Identification of Informative Features","heading":"5.2.2 Phenotype Visualization with Radial Bar Graphs and Radar Graphs","text":"Requirements phenotype visualization.\r\nTogether domain experts, transformed visualization challenges (recall Section 5.1) following requirements:represent high-dimensional data dozens features,preserve semantics original features,allow comparison multiple clusters glance, andcontrast cluster characteristics overall patient mean.Following requirements, implemented () radial bar graph visualization single cluster (Figure 5.2) (b) radial line graph visualization comparing multiple clusters (Figure 5.3).\r\nradial bar graph used compare observations assigned single cluster overall population.\r\nmean values features within cluster represented bars arranged radial layout.\r\nbar begins black “zero line,” representing feature means entire population, .e., subjects used clustering.\r\nfeatures standardized (.e., z-scored) clustering, bars inclined outside represent feature averages population average, bars inclined inside represent feature averages population average.\r\nexample, bar whose top positioned -1 characterizes feature average within cluster one standard deviation smaller population average.\r\naddition combination bar height bar direction, within-cluster averages also mapped bar color sequential color gradient dark blue (lower boundary) yellow (population average) light red (upper boundary).\r\nGray error lines top bar represent within-cluster standard error.\r\nallow quick feature localization, features can grouped (expert-defined) categories, displayed inner circle along cluster name number subjects assigned.\r\nFigure 5.2: Radial bar graphs four phenotypes (PT). () PT 1 characterizes subpopulation lowest health burden. (b) PT 2 includes suffering subjects, mean values psychosomatic somatic characteristics exceed population mean 0.5 standard deviations (SD). (c) PT 3 indicates somatic indicator scores population mean. (d) PT 4 indicates subjects elevated distress scores, including subjective stress perceived quality life. Bars arranged circular layout, bar’s height direction representing within-cluster feature average gray line centered top bar illustrating 95% confidence interval. characteristics grouped nine categories defined tinnitus experts. categories shown inside inner circle. See Appendix feature description. figure adapted [163].\r\nradial line graph (Figure 5.3) allows comparison clusters single display.\r\nInstead bars, feature averages represented points.\r\nLine segments connect points cluster feature category.\r\nPoints line segments colored according respective cluster.\r\nFigure 5.3: Radial line graph phenotypes comparison. Points show within-phenotype feature averages. Points depicting features category connected line segments. Points lines colored cluster. See Figure 5.2 description phenotypes Appendix description features. figure adapted [163].\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-phenotyping-app","chapter":"5 Visual Identification of Informative Features","heading":"5.2.3 Interactive Exploration of Phenotypes","text":"provide interactive application phenotype visualizations web application1 (Figure 5.4).\r\nfollowing interactive components added visualizations described Section 5.2.2.\r\nHovering bar, line, feature label opens tooltip additional cluster summaries compact feature descriptions.\r\nClicking bar label triggers additional plot showing original (unscaled) distribution respective feature stratified cluster , selected, treatment.\r\ncontinuous features, semi-transparent boxplots placed top violin plot [164] layers.\r\ncategorical features, points error lines show percentages category 95% confidence interval.\r\nclustering performed static data, added option display also indicators temporal change, helps, example, discover potential differences response treatment phenotypes visually.\r\nend, extended radial bar graph showing cluster averages\r\ntwo time points (T0 T1) adjacent bars.\r\n\r\nline connects ends pair bars arrowhead pointing T0 score T1 score.\r\nconnecting lines feature labels colored according relative value change T0 T1.\r\nGiven user-defined parameter \\(\\Delta\\), .e., minimum relative difference T0 T1 considered change, elements coloredred T1 score greater T0 score least \\(\\Delta\\),green T1 score smaller T0 score least \\(\\Delta\\), andblack otherwise.\r\nFigure 5.4: user interface phenotype exploration application. Interactive components enhance radial bar graphs: hovering bar feature label opens tooltip additional cluster summaries compact feature description. Clicking feature updates right plot showing distribution selected feature stratified cluster, selected, also treatment. Continuous features shown using semi-transparent boxplots placed violin plot [164] layers. contrast, nominal features, category proportions alongside 95% confidence intervals displayed points error lines, respectively.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-features","chapter":"5 Visual Identification of Informative Features","heading":"5.3 Selection of Measurement Instruments","text":"Discussions tinnitus experts selection measurement instruments (hereafter denoted features) clustering CHA data resulted two main requirements:\r\n(1) selected features cover clinical heterogeneity tinnitus high degree.\r\n(2) available, robust compound scores preferred single items questionnaire.routine questionnaire assessment battery (cf. Section 2.2.2), selected total 64 features2 14 questionnaires.\r\ninclude questionnaire total scores, questionnaire subscale scores, items questionnaires neither subscales total scores.\r\nfeatures measure\r\ngeneral tinnitus characteristics,\r\nphysical quality life,\r\npain experiences,\r\nsomatic expressions,\r\naffective symptoms,\r\ntinnitus-related distress,\r\ninternal resources,\r\nperceived stress, \r\nmental quality life.total 4103 patients, data 2875 (70.1%) incomplete therefore excluded.\r\nN = 1228 patients included final sample slightly, yet significantly younger excluded ones (\\(\\mu\\)included = 50.0, \\(\\sigma\\)included = 11.9; \\(\\mu\\)excluded = 51.7, \\(\\sigma\\)excluded = 13.6; \\(t\\)(2630.8) = 4.0, p \\(<\\) 0.01).\r\nAdditionally, 989 included patients (80.5%), post-treatment data also available used explore treatment effect differences clusters visually.\r\nSince features greater scores higher health burden, reversed remaining features greater scores higher quality life.\r\nfeature \\(X\\) reversed \\(X_{reversed} = \\max{(X)} - X\\).\r\nasterisk suffix feature name (e.g., ACSA_qualityoflife*) denotes reversed feature.\r\nDue widely differing value ranges, feature standardized via z-score scaling.\r\nfeature \\(X\\) expected value \\(E(X)=\\mu\\) variance \\(Var(X) = \\sigma^2\\) standardized \\(Z = \\frac{X - \\mu}{\\sigma}\\).\r\n\\(Z\\), holds \\(\\mu\\) = 0 \\(\\sigma^2\\) = 1.","code":""},{"path":"phenotypes.html","id":"phenotypes-results","chapter":"5 Visual Identification of Informative Features","heading":"5.4 Results","text":"According X-means, four clusters (also referred phenotypes hereafter) represent optimal solution CHA data.\r\nresult confirmed bootstrap validation: Figure 5.5 shows four clusters formed frequently (82 times; 16.4%) among 500 runs.\r\nFigure 5.5: Results internal validation. Bars show frequency number clusters generated X-means 500 bootstrap samples. common cluster number 4, occurred 82 samples (16.4%).\r\nPhenotype 1 (PT 1) represents largest subgroup (697 1228 patients; 56.8%), characterized substantially -average symptom expression tinnitus-related general psychosomatic symptom indices, including affective symptoms, perceived stress, tinnitus-related distress, somatic symptoms, well (-average) quality life internal resources (Figure 5.2 ()).\r\ngroup patients potentially help-seeking, presents clinic frequently, wishes participate multimodal treatment, can assumed experience psychological distress strive present unburdened possible.\r\nTherefore, phenotype referred “avoidance group”.\r\nPatients subgroup comparatively high levels education, employment, duration illness psychotherapeutic treatment (Figure 5.6 (b), (e), (), (g)).PT 2 included 173 patients (14.1%) reported highest emotional somatic burden among PTs (Figure 5.2 (b)).\r\nspecifically, PT 2 represents patient subgroup high\r\npsychosomatic comorbidity therefore referred “psychosomatic group”.\r\npatient subgroup shows high tinnitus burden besides clinically relevant impairment affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear consistent somatoform expressions distress, including somatic symptoms.\r\nPatients subgroup report severely reduced quality life coping behaviors, pessimism, less experienced self-efficacy, optimism.\r\nRelative overall population, subgroup higher percentage women, patients live alone, unemployed, overall lower educational status.\r\nPatients cluster also report consulting physicians, taking sick days, using psychotherapy.\r\nPT 2 patients reported tinnitus noise audible throughout head (.e., unilateral) higher percentage groups.PT 3 contains 187 patients (15.2%) characterized -average scores features measuring somatic complaints near-average scores affective symptoms (Figure 5.2 (c)).\r\npain scores SF8_bodilyhealth* SSKAL_painfrequency similar magnitude PT2, patient subgroup referred “somatic group”.\r\nPT 3 includes oldest subgroup, largest percentage female patients largest reported time since tinnitus onset.contrast PT3, PT 4 (n=171; 13.9%) -average values affective scores, quality--life components, perceived stress (Figure 5.2 (d)), e.g., mental component summary score (SF8_mentalcomp*; 0.85) anxious depression score (BSF_anxdepression; 0.79).\r\nTherefore, PT 4 referred “distress group”.\r\nPT 4 represents youngest four subgroups (mean 47.3 years), largest share male patients (Figure 5.6 (c)).\r\nFigure 5.6: Inter-phenotype comparison demographic characteristics. Summaries given mean [95% confidence interval] entire population four phenotype subpopulations. Confidence intervals estimated using nonparametric Basic Bootstrap Sampling [165] 2000 samples . Kruskal-Wallis test used compare differences phenotypes continuous features (age), Pearson’s chi-square test used categorical features (gender). asterisk indicates statistical significance (\\(\\alpha\\) = 0.05). Correction multiple comparisons performed due approach’s exploratory nature. figure adapted [163].\r\nFigure 5.7 depicts top 10 features greatest average change T1 T0 per cluster.\r\nPT 1 PT 3, BSF_elevatedmood* decreased , namely 0.48 \\(\\pm\\) 0.75 0.65 \\(\\pm\\) 0.85 (\\(Z\\) units), respectively.\r\nPT 2 PT 4, top-ranked feature ADSL_depression average difference T1 T0 0.73 \\(\\pm\\) 0.88 0.74 \\(\\pm\\) 0.83, respectively.\r\nSix ten features among top 10 features clusters, including BSF_elevatedmood*, TQ_cogintivedistress, TQ_psychodistress, TQ_emodistress, TQ_distress, BSF_fatigue.\r\nFigure 5.7: Cluster-specific top 10 features highest average treatment effect magnitude. Bars depict intra-cluster average differences measurements T1 T0 based standardized values. Lower values represent better treatment effectiveness. symbols right feature names indicate whether feature among top 10 features cluster position . example, character string ✓ ✗ ✓ ✗ TQ_intrusiveness (ranked 2nd PT 1) means feature among top 10 PT 1 PT 3, PT 2 PT 4.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-discussion","chapter":"5 Visual Identification of Informative Features","heading":"5.5 Discussion of Findings","text":"","code":""},{"path":"phenotypes.html","id":"juxtaposition-of-the-phenotypes","chapter":"5 Visual Identification of Informative Features","heading":"5.5.1 Juxtaposition of the Phenotypes","text":"clusters comparison showed questionnaires characteristics differed considerably patient phenotypes.\r\nparticular, patient subgroups differed substantially coping behaviors, stress, tinnitus burden, perceived pain, discomfort, perception life quality.\r\ncontrast, patients appear differ concerning localization noise.\r\nRegarding separability phenotypes, predominantly high correlations features within category suggest phenotyping also possible fewer questionnaires, especially since questionnaires overlap semantically, e.g., PHQK_depression, ISR_depression, ADSL_depression, among others.","code":""},{"path":"phenotypes.html","id":"interpretation-of-the-phenotypes-from-the-medical-perspective","chapter":"5 Visual Identification of Informative Features","heading":"5.5.2 Interpretation of the Phenotypes from the Medical Perspective","text":"discussed phenotypes’ clinical relevance three five tinnitus experts co-authored original publication [163].PT 1 (avoidant group) represents half patient sample.\r\nBesides actual tinnitus symptom, patients subgroup reported affective psychosomatic symptoms.\r\npatients’ biased presentation (“everything fine tinnitus”), clinicians might easily led believe assessment possible factors contributing individual distress unnecessary.\r\nHowever, clinical experience suggests thorough assessment psychosocial stressors.\r\npsychosocial resourcefulness subgroup enables patients seek help quickly solution-oriented manner.\r\nAdequate tinnitus-specific counseling individualized (online) therapy modules include audiological, psychological, relaxation techniques may represent adequate treatment strategy patient subgroup.PT 2 (psychosomatic group) represents 15% patients high tinnitus distress clinically relevant impairment across affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear interact strongly somatoform expressions distress, including physical complaints somatic symptoms.\r\nPatients subgroup reported greatly reduced quality life coping behaviors, higher pessimism, lower experienced self-efficacy, optimism.\r\nfrequently asked question whether increased tinnitus-related distress contributes increase depression vice versa.\r\ngroup, depressive anxious symptoms may considered crucial underlying factor overall symptom distress, treatment must initially focus improving mood alleviating depression.\r\n, tinnitus-related distress may need viewed broader context medical psychological contributing factors require patient-specific conceptualization.PT 3 (somatic group) represents patient subgroup\r\ncharacterized somatopsychic symptom expression, .e., physical symptoms may reflect stress underlying medical conditions.\r\nmeet patient subgroup’s needs, multimodal interventions may include proportion body-oriented procedures relaxation exercises physiotherapy.\r\nHowever, effects interpreted terms direct indirect psychological effects (e.g., increased well-affection others).Patients PT 4 (distress group) reported -average perceived stress, accompanied physical exhaustion anxious-depressed mood.\r\ngroup includes younger, employed, male patients reported chronic distress may susceptible burnout syndrome subjectively reduced mental performance (“hamster wheel”), describes life situations even absence tinnitus distress.\r\nMultimodal therapy initially focus stress regulation techniques, including relaxation individually tailored behavior modification approaches.\r\nLike PT 2, high psychosomatic burden, patients PT 4 also benefit longer psychotherapeutic multimodal treatment procedures (inpatient rehabilitative).","code":""},{"path":"phenotypes.html","id":"comparison-to-related-work-on-tinnitus-phenotypes","chapter":"5 Visual Identification of Informative Features","heading":"5.5.3 Comparison to Related Work on Tinnitus Phenotypes","text":"Without “ground truth” given different sets available measurements used, difficult compare results similar studies.\r\nadvantage approach inclusion wide range self-report questionnaire assessments.\r\nstudies also used audiometry [52], [53], cardiac imaging data [150].\r\nPT 2 (psychosomatic distress group) seems associated “constant distressing tinnitus” subgroup reported Tyler et al. [53], mean scores tinnitus-related health distress much larger subgroups.\r\nObviously, selecting meaningful set characteristics central effectiveness cluster analysis.closest radial bar graph visualization radar graph proposed Schlee et al. [152], whose solution tends overplot two subsets need displayed simultaneously.\r\nTherefore, fill areas spanned connected points color \r\navoid one polygon completely overlaps another.\r\nFurthermore, inferences radar map [152] depend heavily arrangement features since primary criterion comparison polygons’ shapes.\r\nsolution compute arrangement yields areas achieve maximum mean area difference subgroups minimum area variance within subgroups partially solves problem.\r\nStill, moderate number 20 features can represented.\r\nchose organize 64 features according expert-determined categories, e.g., quality life, making easier find features compare similar features.","code":""},{"path":"phenotypes.html","id":"phenotypes-conclusion","chapter":"5 Visual Identification of Informative Features","heading":"5.6 Conclusion","text":"presented workflow determining, visualizing, inspecting essential phenotypes medical condition high-dimensional datasets using example tinnitus.\r\nAlthough demonstrated workflow’s usefulness specific disorder, can easily adapted medical condition.reduce amount input necessary medical expert, leverage parameter-free clustering algorithm phenotype discovery.\r\nAlthough optimal number clusters four dataset, expect number may different different sample tinnitus patients even clustering algorithm.\r\nFigure 5.5 shows high variance number clusters returned X-means different bootstrap samples.\r\nConsidering slightly lower occurrence 5 6 clusters, clustering result certainly set stone.\r\nInstead using X-means, evaluated k-means different k using cluster quality function.\r\nsuitability heuristic approach investigated Chapter 7.novel visualization types provide medical expert quick overview important characteristics differences subpopulations.\r\nintegrated web application interactive functionalities cluster inspection juxtaposition.\r\nvisualization application tinnitus-specific can used display compact summary condition subset index symptoms.\r\nWhether clinicians adopt visualizations guide appropriate tinnitus management strategies remains tested.\r\npreliminary user study, clinicians suggested graphical summaries possible patient subtypes ease challenge assigning appropriate treatment strategy specific combinations symptom presentations.Chapter 8, revisit CHA data focus supervised subpopulation discovery post-hoc interpretation classification models example tinnitus-related distress depression, respectively.\r\nfuture work, validate phenotypes different cohort tinnitus patients.\r\nFurthermore, explore clustering algorithm families.\r\nexample, self-organizing maps [166] seem suitable candidate, already offer dedicated visualizations [167] box, extension timestamped data [168] may allow us study phenotype changes time.excluding patients complete questionnaires T0, may selection bias.\r\nPossible reasons non-completion include unfamiliarity technical equipment used record item responses, loss motivation due relatively large number questionnaires, collisions baseline studies laboratory.\r\nNevertheless, analysis 15 questionnaires led insights questionnaires’ contributions phenotyping, possibly allowing reduction number questionnaires.\r\nresults reflect snapshot patients’ situation baseline, patient may transition one phenotype another later stages life, depending tinnitus management.\r\nTherefore, next step investigate effects treatment phenotypes detail determine whether patient phenotypes benefit others.","code":""},{"path":"evo.html","id":"evo","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6 Constructing Evolution Features to Capture Change over Time","text":"chapter partly based :Uli Niemann, Tommy Hielscher, Myra Spiliopoulou, Henry Völzke, Jens-Peter Kühn. “Can classify participants longitudinal epidemiological study previous evolution?” : Computer-Based Medical Systems (CBMS). 2015, pp. 121-126. DOI: 10.1109/CBMS.2015.12.Medical studies longitudinal design collect participant data questionnaires, medical examinations, laboratory analyses, imaging repeatedly time [12]–[14].\r\nHidden temporal information made explicit constructing features describe subjects’ change time.\r\nHowever, lack applicable methods timestamped data tiny (< 5) number moments.\r\npresent framework addressing shortcoming demonstrate augmenting feature space -called evolution features increases classification performance yields understandable descriptors change worthy investigation.Section 6.1 describes work related construction temporal representations medical data.\r\nSection 6.2 introduces notation problem formulation.\r\nSection 6.3 presents evolution feature framework, including full workflow encompasses steps extract evolution features, dealing class imbalance, feature selection.\r\nSection 6.4 describe evaluation setup.\r\nreport results Section 6.5 present evolution features found important class separation Section 6.6.\r\nconclude chapter Section 6.7.","code":""},{"path":"evo.html","id":"brief-chapter-summary-3","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"Brief Chapter Summary","text":"propose framework extract “evolution features” timestamped medical data, describe study participants’ change time.\r\nshow exploiting novel features improves classification performance validating workflow SHIP data target variable hepatic steatosis.","code":""},{"path":"evo.html","id":"evo-intro","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.1 Motivation and Comparison to Related Work","text":"Epidemiological studies serve basis identification risk factors associated medical condition [3]–[5].\r\nMachine learning still relatively little used epidemiology, mainly due hypothesis-driven nature research.\r\nHowever, examples machine learning applications identification health failure subtypes [169] discovery factors (including biomarkers) modulate medical outcome [170], [171].\r\nlongitudinal cohort studies, measurements performed multiple study waves; hence researchers obtain access sequences recordings.\r\ncontext machine learning, extracting leveraging inherent temporal information sequences may increase model performance , thus, understanding medical condition interest.technical point view, classification problems timestamped data can different.\r\ncan broadly summarized three categories:instance (patient, study participant) label time point \\(t\\). goal assign instance’s label \\(t\\) + 1. Typically, data stream classification problem [172], [173].instance associated one label time points. Typically, timeseries classification problem [174], [175].labels time point event predicted. problem concerned event prediction [176]–[178].classification problem falls (), use advances stream classification data unlabeled except last time point stream three time points tiny.clinical applications, temporal information often exploited, predominantly analysis patient records.\r\nexample, Pechenizkiy et al. [179] analyzed streams recordings predict rehospitalization heart failure events remote patient management.\r\nSun et al. [180] computed similarity streams patients patient monitoring data.\r\nCombi et al. [181] reported streams life signals, particularly temporal analysis timestamped medical records hospital patients.However, participants epidemiological, population-based study hospital patients – random sample studied population, often skewed class distribution.\r\nlongitudinal study kind, recordings cohort member made moment.\r\nHielscher et al. [182] presented feature engineering approach extract temporal information multiple patient recordings longitudinal epidemiological study.\r\nFirst, assessment, clusters feature-value sequences associated target variable found.\r\nAfterward, original sequence features used conjunction classification.\r\nHielscher et al. [182] showed classification performance increases features temporal information incorporated feature space.\r\nInstead modeling individual change measurement values, approach involves deriving multivariate change descriptors.\r\n\r\n\r\n\r\nPatient evolution clustering studied Siddiqui et al. [183].\r\nproposed method predicts patient evolution timestamped data clustering similarity predicting cluster movement multi-dimensional space.\r\nHowever, patient data considered [183] labeled moment.\r\nworkflow combines labeled unlabeled timestamped data longitudinal study improve classification performance skewed data.\r\ntarget variable, study multi-factorial disorder hepatic steatosis (fatty liver) sample participants longitudinal population-based “Study Health Pomerania” (SHIP) [12], recall Section 2.2.1.\r\nSHIP cohort, assessments (interviews, medical tests, etc.) recorded several moments (SHIP-0, SHIP-1, etc.), ca. 5 years apart.\r\nTemporal information often used analyzing patient data hospital, time granularity different.\r\nexample, intensive care unit, timestamped data collected fast pace, .e., every minute even every second.\r\ncontrast, participants longitudinal epidemiological study monitored months even years.\r\nHence, measurements assessment epidemiological dataset possibly far apart.\r\nlarge period two consecutive recordings complicates applying methods designed data arrive higher frequency.\r\nexample, participant may exhibit alcohol abuse become pregnant, stop smoking start , take antibiotics affect liver, experience lifestyle changes turn medical recordings taken five years ago irrelevant learning participant’s current health state.\r\nAnother patient may lifestyle changes illnesses, past data reflect aging.\r\nchallenge label unavailable several waves.\r\nreliable estimate fat accumulation liver computed magnetic resonance tomography images.\r\nSHIP-0, MRT unavailable.\r\nInstead, liver fat accumulation derived ultrasound – procedure lower clinical accuracy.\r\nSHIP-1, calculation omitted altogether.\r\nConsequently, given SHIP participant, class label available SHIP-2, label SHIP-1, partially reliable indicator SHIP-0.\r\nSince hepatic steatosis reversible disorder, label imputation – using growth model [184] – possible; participant evolution must learned one moment labeled data.address challenges follows.\r\nFirst, group study participants moment similarity, thus building clusters cohort members similar recordings one three moments.\r\n, connect clusters across time, thus capturing cluster’s transition one study wave next.\r\ntransitions reflect evolution subpopulations, individuals.\r\nHence, next single labeled recording per cohort participant, also exploit earlier, unlabeled recordings, description cluster assigned , information clusters evolve.\r\nshow new, augmented dataset, combining labeled unlabeled data individuals subpopulations, improves classification delivers additional insights factors associated hepatic steatosis.","code":""},{"path":"evo.html","id":"evo-problem-formulation","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.2 Problem Formulation","text":"now provide basic symbols essential functions classification problem, summarized Table 6.1.\r\nTable 6.1: Symbols essential functions.\r\nlongitudinal study comprises data participant \\(x\\X\\) every feature \\(f\\F\\) every time point \\(t\\\\{1,\\ldots, T\\}\\) (also referred moment).\r\n(single) value \\(x\\) feature \\(f\\) moment \\(t\\) denoted \\(v(x,f,t)\\);\r\nset values \\(x\\) \\(t\\), .e., \\(\\{v(x,f,t)|\\forall f\\{}F\\}\\), denoted \\(obs(x,F,t)\\).\r\nparticipant, \\(v(\\cdot)\\) NULL, .e., may missing values.\r\ngoal predict class label \\(x\\) moment \\(T\\), .e., \\(label(x,T)\\) using data observations previous moments \\(t\\\\{1\\ldots T-1\\}\\).\r\nclass label participant unavailable except \\(t=T\\).\r\nmodel prediction task conventional classification problem.\r\naim improve classification performance expanding feature space change descriptors, -called evolution features, presented next.","code":""},{"path":"evo.html","id":"evo-concept","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.3 Evolution Features","text":"leverage latent temporal information longitudinal cohort study dataset extracting informative features based individual change participants transition respective clusters time.\r\npurpose, exploit similarity among participants moment surrogate labels available first two moments, assuming similar participants evolve similarly.\r\n\r\ncall new features “evolution features”.\r\napproach illustrated Figure 6.1 ().\r\nmonitor individual change participants across study waves, trace clusters’ change separately, extract new features (labeled unlabeled data) augment original data space new descriptors change.\r\ncomplete classification workflow depicted Figure 6.1 (b).\r\nFigure 6.1: Concept evolution feature extraction classification performance improvement. () Clustering longitudinal cohort data subsequent generation evolution features change individuals (red) whole clusters (green). (b) Overview classification workflow.\r\nfollowing, describe clustering study participants (Section 6.3.1), generation evolution features (Section 6.3.2), feature selection strategy extract subset informative features input classification (Section 6.3.3), dealing class imbalance undersampling majority class.","code":""},{"path":"evo.html","id":"evo-concept-clustering","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.3.1 Clustering","text":"clustering, prefer density-based clustering partitional algorithms (like k-means) data contain extreme cases, clusters may arbitrarily shaped different sizes, determine number advance.\r\nmoment \\(t\\), run DBSCAN [185] algorithm cluster set \\(Z(t)\\) recordings cohort members observed \\(t\\).\r\nparticipant \\(x\\), \\(v(x,f,t)\\) denotes value \\(x\\) feature \\(f\\\\) feature-set \\(F\\) \\(t\\), \\(obs(x,F,t)\\) set feature recordings \\(x\\) \\(t\\) (cf. notation Table 6.1).Distance function.\r\ndistance participants \\(x,z\\) \\(t\\), use adjusted heterogeneous Euclidean overlap metric [182], [186], weights difference two values \\(x,z\\) feature \\(f\\) feature’s information gain \\(G(f)\\), scaled largest observed value \\(G^*\\), defined :\r\n\\[\\begin{equation}\r\nd(x,z,t)=\\sqrt{\\sum_{f \\F} \\left(\\frac{G(f)}{G^*}\\cdot \\delta\\left(v(x,f,t),v(z,f,t)\\right)\\right)^2}.\r\n\\tag{6.1}\r\n\\end{equation}\\]\r\ncontinuous features, \\(\\delta(,b)\\) min-max-scaled difference values \\(,b\\), .e., \\((-b)/(\\max(f)-\\min(f))\\).\r\nnominal features, \\(\\delta(,b)\\) 0 \\(=b\\), 1 otherwise.DBSCAN Parameter setting.\r\nDBSCAN relies two parameters: radius \\(eps\\) neighborhood around data point, minimum number \\(minPts\\) neighbors point core point.\r\nuse “elbow” heuristic Ester et al. [185], determines suitable \\(eps\\) value given \\(minPts\\) value, illustrated Figure 6.2.\r\nspecifically, define parameter \\(k\\) compute \\(x\\{}Z(t)\\) distance k-dist\\((x,k)\\) kth nearest neighbor.\r\nsort distances, draw k-dist\\((x,k)\\) graph g, span line l connecting smallest k-dist() value largest one.\r\n, set \\(eps\\) k-dist value maximum distance g l.\r\nFigure 6.2: Setting \\(eps\\) based k-dist graph given \\(minPts\\). k-dist graph g depicts sorted distances points’ kth next neighbors. suitable \\(eps_{opt}\\) can identified maximum distance k-dist line l connects first last point g. DBSCAN clustering \\(minPts\\) = k, \\(eps_{opt}\\), points k-dist \\(\\leq\\) \\(eps_{opt}\\) become core points, else border noise points.\r\n","code":""},{"path":"evo.html","id":"evo-concept-evo-features","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.3.2 Constructing Evolution Features","text":"extract information participants’ evolution, distinguishing among evolve smoothly switch among clusters.\r\ntransfer information evolution features, thus enriching feature space information unlabeled moments.\r\nTable 6.2 describes evolution features.\r\ndivided four categories: () features participant moment, (B) evolution features describing aspect change study waves individual participant, (C) evolution features measuring changes study waves concerning feature values participant, (D) evolution features linked whole cluster.cohort member \\(x\\) moment \\(t\\), record cluster containing \\(x\\) (feature 1 Table 6.2),\r\n(2) distance \\(x\\) cluster’s centroid,\r\n(3) fraction positively labeled participants among k nearest neighbors \\(x\\),\r\n(4) (graph-based) cohesion [187] (5) Silhouette coefficient [187] \\(x\\), (6) (graph-based) separation [187] \\(x\\) cohort participants outside cluster.\r\ncompute difference cohesion, Silhouette, separation values \\(t\\) later moments \\(\\{t' \\T|t'>t\\}\\) (7-9), also check much values metrics change \\(x\\) moves \\(c(x,t)\\) \\(c(x,t')\\) (10-12).\r\nrecord whether \\(x\\) outlier, .e., DBSCAN noise point moment (13).\r\n\\(t\\) \\(\\{t' \\T|t'>t\\}\\), compute fraction cohort members cluster \\(x\\) \\(t\\) \\(t'\\) (14), fraction common k nearest neighbors (15), change distance \\(x\\) centroid \\(t\\), \\(t\\) \\(t'\\) (16).\r\nrecord changes sequence values feature, including real (17), absolute (18), relative (19) differences values two moments.\r\nmeasure cluster shrinks/grows \\(t\\) \\(t'\\) (20), much members move (average) closer far apart previous positions (21-23).\r\nTable 6.2: Overview extracted features. first group features () comprises cluster membership aggregated distance information participant moment; feature group (B) changes participant’s position (hyperspace) relative cluster closest neighbors; feature group (C) captures changes values participant’s recordings; feature group (D) refers changes clusters.\r\n","code":""},{"path":"evo.html","id":"evo-concept-feature-selection","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.3.3 Extending Correlation-Based Feature Selection to Include Evolution Features","text":"imbalanced data, feature selection classification often biased favor majority class [188].\r\nHence, feature selection, undersample majority class generate balanced data set select features informative respect classes.\r\nuse feature selection method Hielscher et al. [189] follows:\r\ninvoke correlation-based feature selection [190] (CFS), builds feature set iteratively inserting feature adds “merit” .\r\nmerit \\(M_F\\) feature set \\(F\\) computed calculating information gain pair features \\(F\\) (lower gain corresponds low correlation thus preferred) feature \\(F\\) towards target variable (higher gain better).\r\nContinuous features first discretized entropy-based method Fayyad Irani [136].\r\ndiscretize feature selection; clustering classification, use original values.shown Figure 6.1, perform feature selection twice.\r\nfirst time, consider features recorded moments, essential evolution tracing: can compute distances objects clusters located topological space.\r\ngenerating evolution features, build complete set features, also considering recorded moment.\r\nset, perform feature selection discard unpredictive (original evolution) features.\r\nfinal feature set used classification.","code":""},{"path":"evo.html","id":"evo-evaluation","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.4 Evaluation Setup","text":"evaluate workflow 10-fold cross-validation four --shelf classification algorithms: random forest [191] (RF), C4.5 decision tree [192], Naïve Bayes (NB), k-nearest neighbor (kNN).\r\nFirst, compare algorithm’s generalization performance used alone (baseline variant) vs. incorporated workflow (workflow-enhanced variant).\r\n, study impact different combinations three workflow components undersampling (U), feature selection (F), incorporation generated evolution features (G).\r\nSensitivity (true positive rate), specificity (true negative rate), F-measure (harmonic mean precision recall) serve evaluation measures.\r\nshown Table 6.3, Baseline invokes classification algorithm; use classification algorithm achieves highest F-measure scores.\r\nvariant U-G performs undersampling uses generated evolution features classification.\r\nSince undersample feature selection build classification models original dataset, U-G identical --G U-- identical Baseline variant, omit list U-G U-- explicitly.main parameter \\(k\\) number neighbors data point: set \\(minPts\\) = \\(k\\) use \\(k\\) derive values DBSCAN parameter \\(eps\\) (cf. Section 6.3.1) parameters features same_kNN_\\(k\\)_t_1_t_2 fraction_Of_POS_kNN_\\(k\\)_t (cf. Table 6.2).\r\n, number nearest neighbors k-NN classification algorithm also set \\(k\\).\r\nvary \\(k\\) measure impact classification performance.Following findings Chapters 3  4 differences female male participants respect outcome, run experiments whole dataset (PartitionAll) partitions female (PartitionF) male (PartitionM) participants.\r\nFinally, list important features found PartitionAll two subsets.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 6.3: Workflow variants. UFG complete workflow.\r\n","code":""},{"path":"evo.html","id":"evo-results","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.5 Results","text":"Figure 6.3 shows sensitivity (left), specificity (center), F-measure (right) simple classifiers (gray curves) workflow-enhanced counterparts (line style, colored) different k.\r\nFigure 6.3: Comparison classification performance workflow baseline. Sensitivity (left), specificity (center) F-measure scores (right) different classifiers varying number k neighbors cohort member impacts clustering result. classifier, two performance curves shown: colored one workflow-enhanced version gray one baseline counterpart. Higher values better measures. figure adapted [193].\r\nOverall, workflow-enhanced variant outperforms simple counterpart respect sensitivity F-measure, outperforms performs slightly worse specificity.\r\nworkflow-enhanced Naive Bayes performs best concerning sensitivity k best k = 31.\r\nDecision trees exhibit highest F-measure, improvements sensitivity F-measure compared simple variant, albeit specificity slightly worse; improvements less large k.\r\nRandom Forests benefit workflow, absolute improvement F-measure 30% (green vs. gray “+” curves right part Figure 6.3).\r\nOne explanation relatively low sensitivity simple RF variant large number trees (100) learned data samples containing positive examples: RF trapped many majority class examples.\r\nresult consistent specificity curve (almost straight line around 95%) simple RF, F-measure slightly 40%.\r\nworkflow improves RF sensitivity (63%) F-measure (65%), specificity remains high (90%).\r\nOverall, impact k three measures limited algorithms except workflow-enhanced baseline k-NN, naturally affected stronger value k algorithm.\r\nTherefore, workflow-enhanced variants outperform simple counterparts terms sensitivity F-measure. algorithms, workflow prevents overfitting negative class.results workflow component-specific results Figure 6.4 show complete workflow UFG variants UF- --G outperform variants sensitivity F-measure.\r\nvariants -F- -FG perform well regarding specificity, suggesting feature selection may beneficial without undersampling datasets class imbalance.\r\nFigure 6.4: Comparison classification performance workflow components. Sensitivity (left), specificity (center) F-measure scores (right) workflow variant baseline using decision tree learning. figure adapted [193].\r\n","code":""},{"path":"evo.html","id":"evo-important-features","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.6 Identification of Important Evolution Features","text":"performance workflow variants, include feature selection, indicates small number features sufficient class separation.\r\nHereafter, report evolution features selected classification appearing among top 15 features according information gain partition; term features “important.”Figure 6.5 shows PartitionAll 3 15 features generated evolution features.\r\nboxplots () (c) Figure 6.5 refer differences values recorded two moments.\r\nfeature separationDelta_g_1_2 measures difference cluster separation participant based cluster assignment moment 1 2, corresponds entry #9 Table 6.2.\r\nParticipants belonging positive class exhibit higher median separationDelta_g_1_2 participants without disorder, indicating clusters harboring mostly positive participants cover larger, sparse areas.\r\nfeature relative_Difference_som_huef_g_0_1 (#19) quantifies difference participant’s hip circumference SHIP-0 SHIP-1, relative value SHIP-0.\r\naverage, study participants classes lose weight grow older. Negative participants reduce weight positive participants (cf. Figure 6.5 (c)), generally reflects differences lifestyles.\r\nmosaic chart Figure 6.5 (b) feature fraction_of_Positives_kNN_1_g_2 (#3) indicates “nearest neighbor” fatty liver participant also likely exhibit disorder participant without fatty liver.\r\nFigure 6.5: Selected evolution features highest contribution class separation PartitionAll. () Difference cluster separation SHIP-1 SHIP-2. (b) Whether (=1) (=0) nearest neighbor positive class SHIP-2. (c) Relative difference hip circumference SHIP-0 SHIP-1.\r\nPartitionF, 5 top 15 features evolution features, cf. Figure 6.6.\r\nCompared female participants without disorder, female subjects fatty liver exhibit larger distance centroid cluster SHIP-1 (#2), lower silhouette coefficient SHIP-1 (#5), higher difference waist circumference SHIP-0 SHIP-2 (#19), lower relative difference serum triglycerides concentration SHIP-0 SHIP-1 (#19).\r\nFigure 6.6: Selected evolution features highest contribution class separation PartitionF. () Distance cluster centroid SHIP-1. (b) Silhouette coefficient SHIP-1. (c) Relative difference waist circumference SHIP-0 SHIP-2. (d) Absolute value relative difference serum triglycerides SHIP-0 SHIP-1.\r\nPartitionM, 2 top 15 features evolution features (Figure 6.7), including relative difference waist circumference SHIP-1 SHIP-2 (#19) difference cluster separation SHIP-0 SHIP-1 (#6).\r\nfeatures, participants exhibiting disorder greater values.\r\nFigure 6.7: Selected evolution features highest contribution class separation PartitionM. () Relative difference waist circumference SHIP-1 SHIP-2. (b) Difference cluster separation SHIP-0 SHIP-1.\r\n","code":""},{"path":"evo.html","id":"evo-conclusion","chapter":"6 Constructing Evolution Features to Capture Change over Time","heading":"6.7 Conclusion","text":"extended set methods static medical data presented Part proposing workflow classification longitudinal cohort study data exploits inherent temporal information clustering cohort participants moment, linking clusters, tracing participant evolution study’s moments.\r\nextract evolution features clusters transitions, added feature space subsequently used classification.\r\nworkflow improves generalization performance respect sensitivity F-measure scores.\r\ngenerated evolution features contribute improvement, even used alone without undersampling skewed data.\r\nshown change somatographic variables’ values cluster quality indices time predictive.limitation concerns assessment feature’s importance using information gain.\r\nadditional merit feature towards model predictions assessed decoupled actual model.\r\nFurthermore, models may incorporate complex feature interactions captured subpopulation-specific differences feature importance.\r\nPart III addresses post-hoc model interpretation, includes sophisticated approaches measuring feature’s attribution model.","code":""},{"path":"diabfoot.html","id":"diabfoot","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7 Feature Extraction from Short Temporal Sequences for Clustering","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Fred Samland, Thorsten Szczepanski, Jens Grützner, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. “Learning Pressure Patterns Patients Diabetic Foot Syndrome.” : Computer-Based Medical Systems (CBMS). 2016, pp. 54-59. DOI: 10.1109/CBMS.2016.31.Uli Niemann, Myra Spiliopoulou, Thorsten Szczepanski, Fred Samland, Jens Grützner, Dominik Senk, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. “Comparative Clustering Plantar Pressure Distributions Diabetics Polyneuropathy May Applied Reveal Inappropriate Biomechanical Stress.” : PLOS ONE 11.8 (2016), pp. 1-12. DOI: 10.1371/journal.pone.0161326.Uli Niemann, Myra Spiliopoulou, Jan Malanowski, Juliane Kellersmann, Thorsten Szczepanski, Silke Klose, Eirini Dedonaki, Isabell Walter, Antao Ming, Peter R. Mertens. “Plantar temperatures stance position: comparative study healthy volunteers diabetes patients diagnosed sensoric neuropathy.” : EBioMedicine 54.102712 (2020), pp. 1-11. DOI: 10.1016/j.ebiom.2020.102712.Nowadays, mHealth devices, smart wearables, ubiquitous everyday life [194].\r\nembedded systems continuously record process various vital functions send feedback user, example, via popup notifications dashboard reports.\r\nThus, designed help live healthier contributing effective exercise better diet.\r\nmHealth solutions also increasingly used clinical settings.\r\nPattern recognition timestamped mHealth data used timely detect adverse health events.example, diabetes patients sensory neuropathy, repeated excessive pressure loads can aggravate plantar tissue destruction ultimately lead foot ulcers, worst case, even amputation [195].\r\nSince frequent clinical examinations costly bothersome patient [196], sensor-equipped systems patient’s shoe insoles developed.\r\nsystems measure foot pressures temperatures continuously transmit recordings wirelessly smart device, processes data send offloading instructions potentially harmful pressures detected [197] temperature differences left right foot exceed pre-specified thresholds [196].\r\nHowever, universal pressure threshold high sensitivity specificity respect ulcer development yet determined [198], [199].\r\nBesides, significant difference temperature left right foot noted, often already late warn patient, ulceration already begun.\r\nEvidence suggests subpopulations distinct plantar pressure patterns requiring different preventive measures, example, different peak pressure thresholds [200]–[203].previous chapters, presented methods analyzing static data (Chapters 3–5) timestamped data (Chapter 6) small number time points.\r\nRaw mHealth data comprises hundreds time points, thus requiring extraction meaningful features actual data analysis.\r\nchapter proposes similarity measures short temporal sequences create representations distinct subpopulations mHealth data.\r\nDIAB data described Section 2.2.3 serve proof--concept validation.chapter organized follows.\r\nSection 7.1 describes medical background diabetic foot syndrome reviews previous subtyping approaches detect plantar pressure patterns.\r\nSection 7.2 presents approaches modeling regional plantar pressure similarity identifying distinct foot profiles clustering.\r\nreport results Section 7.3 compare similar studies Section 7.4.\r\nconclude chapter Section 7.5.","code":""},{"path":"diabfoot.html","id":"brief-chapter-summary-4","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"Brief Chapter Summary","text":"Embedded mHealth devices continuously record vital functions.\r\ntemporal sequences arrive mostly raw sensor measurements, requiring extraction meaningful features.\r\npropose similarity measures short temporal sequences build representations distinct subpopulations.\r\nvalidate approach identifying characteristic plantar pressure profiles recordings controlled experiment diabetic foot syndrome patients non-diabetic volunteers.","code":""},{"path":"diabfoot.html","id":"diabfoot-intro","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.1 Medical Background","text":"Diabetic foot syndrome (DFS) substantial negative impact life quality [69].\r\nAffected patients higher mortality [69], [204] higher risk foot ulcerations [74].\r\n85% foot amputations relate foot ulcers [71], [72].Peripheral sensory neuropathy predisposing foot problems diabetes patients [195].\r\nsensory neuropathy, damaging events injuries go unnoticed, continued insults can exacerbate tissue destruction.\r\nHowever, understanding underlying pathomechanisms tissue destruction absence trauma limited.Lower extremity (micro)blood flow diabetes patients compared healthy controls establish causal relationship decreased blood flow occurrence diabetic foot syndrome [73], [205]–[209].\r\nPersistent elevated plantar pressure major risk factor ulceration diabetes patients [210]–[212].\r\nCustom-made footwear orthopedic shoes used ulcer prevention individual therapy [213]–[215].\r\nHowever, pressure threshold high sensitivity specificity respect ulcer development yet determined [198], [199].\r\nResearchers also investigate pressure applied foot region.\r\nBennetts et al. [202] suggested differences foot types biomechanics result differences pressure distribution regions foot.\r\nDeschamps et al. [203] proposed stratification patients based plantar pressure pattern homogeneity. approach may avoid pitfall smoothing away variations within pathophysiological subgroup.absence ground truth, clustering often used derive subgroups patients share similarities pressure distribution, usually focusing peak plantar pressure, .e., maximum observed pressure recorded single measurement, example, maximum pressure sensor step [200]–[203].\r\nGiacomozzi Martelli [200] examined plantar pressure curves DFS patients control subjects.\r\nperformed clustering based curves’ similarity shape amplitude.\r\njuxtaposed clusters predefined subgroups: example, subjects group increased peak pressure muscle weakness limited joint mobility assigned one cluster [200].\r\nDe Cock et al. [201] examined peak pressure measured different foot regions jogging identified four pressure patterns different pressure centers.\r\nexample, “M2 pattern” cluster maximum mean total regional impulse located second metatarsal bone.\r\nBennetts et al. [202] considered seven plantar regions formed subgroups patients similar peak pressure distributions regions.\r\nDeschamps et al. [203] focused plantar forefoot peak pressure distribution studied patients controls.\r\nformed clusters patients similar forefoot pressure distributions identified cluster consisting diabetes patients.Similar [201], [202], focus patient stratification clustering.\r\n[200], [202], consider foot regions, just forefoot.\r\ncontrast aforementioned methods determined similarity single way used k-means clustering, present alternative ways modeling similarity terms plantar pressure contrast clusters formed different clustering algorithms terms quality.\r\nBesides, provide visualization aids inspecting representative instance (patient foot) cluster.Initially, study experiment performed exclusively patients.\r\nVolunteer data became available gradually.\r\nHence, chapter divided two parts:\r\npart , present alternative distance measures regional plantar pressure load evaluate clustering experiment data diabetes patients.\r\npart B, use best distance measure part cluster subgroups diabetes patients, healthy volunteers, combined group separately.","code":""},{"path":"diabfoot.html","id":"diabfoot-approach","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.2 Modeling Similarity of Regional Plantar Pressure","text":"approach includes feature extraction (Section 7.2.1),\r\nplantar pressure distribution similarity modeling (Section 7.2.2),\r\nclustering study participant feet (Section 7.2.3), clustering evaluation (Section 7.2.4), visualization.\r\npresent steps .","code":""},{"path":"diabfoot.html","id":"diabfoot-approach-feature-extraction","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.2.1 Feature Extraction from Short Time Series","text":"pressure measurements one foot example participant shown Figure 7.1.\r\nexperiment, foot, eight sensors, pressure recordings result time series.\r\nWithin time series, stance episodes, .e., phases patient standing, indicated dashed boxes.\r\nepisodes, example patient applied pressure central sensors MTB-3 Calcaneus Digitus-1 Lateral, suggesting balanced pressure pattern.\r\nFigure 7.1: Insole sensor locations pressure time curve examples. Sensor locations insole (center) four pressure time curves representative foot regions example patient. Dotted boxes highlight time intervals patient asked stand apply pressure. lasted 5, 10, 20 minutes, respectively. D1: Digitus-1; MTB: metatarsal bone. figure adapted [216].\r\nsession \\(\\), .e., experiment, participant foot separately, minimum maximum observed pressure values sensors identified, \\(r_{}^{\\text{min}}\\) \\(r_{}^{\\text{max}}\\), respectively.\r\n, within session \\(\\) sensor \\(s\\) observed pressure value \\(r_{,s}\\) normalized relative plantar pressure (RPP) value\\[\\begin{equation}\r\nr^*_{,s}=(r_{,s}-r_{}^{\\text{min}})/(r_{}^{\\text{max}}-r_{}^{\\text{min}}).\r\n\\tag{7.1}\r\n\\end{equation}\\]","code":""},{"path":"diabfoot.html","id":"diabfoot-approach-similarity-measures","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.2.2 Distance Measures","text":"consider three alternative ways modeling plantar pressure similarity:similarity based relative plantar pressure,similarity based pressure distribution pairs sensors, andsimilarity based centers pressure.Similarity based relative plantar pressure.\r\ndefine distance \\(d_{\\text{RPP}}\\) sessions \\(,j\\) Euclidean distance mean RPP eight plantar regions:\\[\\begin{equation}\r\nd_{\\text{RPP}}(,j) = \\sqrt{\\sum_{s} \\left( \\mu(r^*_{,s}) - \\mu(r^*_{j,s}) \\right)^2},\r\n\\tag{7.2}\r\n\\end{equation}\\]\\(\\mu(r^*_{,s})\\) average RPP session \\(\\) region \\(s\\).Similarity based pressure distribution pairs sensors.\r\npair regions \\(s,t\\) session \\(\\), simple linear regression model \\(X_t = \\beta_0+\\beta_1X_s\\) derived, capturing linear relationship plantar pressure recordings \\(s\\) \\(t\\).\r\nFigure 7.2 shows example MTB-3 MTB-4 sensors three feet.\r\ngenerally applied less pressure b, linear relationship recordings MTB-3 MTB-4 similar, indicated nearly parallel regression fit.\r\nCompared b, regression fit c smaller slope, indicating increase pressure MTB-3 associated smaller increase pressure MTB-4.\r\nregression lines also differ goodness fit.\r\nexample, residual values , average, smaller b c.\r\nFigure 7.2: Example three RPP distributions. three example feet , b, c, RPP values MTB-3 (x-axis) MTB-4 (y-axis) shown. Regression lines fitted set points. angular distance values \\(\\Theta(\\cdot)\\) small pairs regression lines (,b), \\(\\Theta(,c)\\) \\(\\Theta(b,c)\\) large, quantifying slope fit c different fit b, respectively. MTB: metatarsal bone.\r\nTwo feet similar slopes \\(\\binom{8}{2}\\)\r\nregression lines similar, taking account goodness fit line.\r\nLet \\(\\) session, let \\(s,t\\) two plantar regions.\r\nregression line’s slope \\(s\\) \\(t\\) session \\(\\) denoted \\(m(,s,t)\\) Pearson correlation coefficient \\(cor(,s,t)\\), quantifying goodness fit regression lines.\r\ndissimilarity two sessions \\(,j\\) two regions defined difference slopes regression lines \\(m(,s,t)\\) \\(m(j,s,t)\\), given \\[\\begin{equation}\r\n\\Theta(,j,s,t) = \\tan^{-1}\\left(\\frac{m(,s,t)-m(j,s,t)}{1+m(,s,t)\\cdot m(j,s,t)}\\right).\r\n\\tag{7.3}\r\n\\end{equation}\\]sum angular distances pairs plantar regions obtain value expressing distance two feet \\(d_{\\text{pairs}}(,j)\\), defined \\[\\begin{equation}\r\nd_{\\text{pairs}}=1-\\sum_{r}\\sum_{s} \\Theta(,j,s,t)\\cdot \\frac{|cor(,s,t)| + |cor(j,s,t)|}{2}\r\n\\tag{7.4}\r\n\\end{equation}\\]\r\n(\\(\\neq j\\); otherwise 0), \\(cor(,s,t)\\) Pearson correlation coefficient plantar pressure records \\(r\\) \\(s\\) foot \\(\\).\r\nangular distance depends goodness fit regression, angular distances reliable regression lines assigned higher weight distance calculation.Similarity based centers pressure.\r\n\r\nthird variant similarity, cluster average RPPs region.\r\nTwo feet similar k centroids similar.\r\nregion, call k-means multiple times different k select run optimal Silhouette coefficient.\r\nHowever, due possibly different number clusters different regions, consider distance two centroids may smaller clustering many clusters clustering clusters. Therefore, add weighting factor \\(d_{\\text{centers}}(,j)\\) defined :\\[\\begin{equation}\r\nd_{\\text{centers}}(,j) = \\sum_{s}{}\\left|ctr(,s) - ctr(j,s)\\right| \\cdot \\log(|C(s)|)/\\sum_{t}\\log(|C(t)|)\r\n\\tag{7.5}\r\n\\end{equation}\\]\r\n\\(ctr(,r)\\) centroid cluster \\(\\) assigned respect clustering sensor \\(s\\) \\(|C(s)|\\) number clusters clustering \\(s\\).","code":""},{"path":"diabfoot.html","id":"diabfoot-approach-clustering","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.2.3 Identifying Foot Profiles by Clustering","text":"identify distinct pressure pattern subgroups, carry k-medoids [217], DBSCAN [185], agglomerative hierarchical clustering.k-medoids.\r\nLike k-means X-means (recall Section 5.2.1), k-medoids cluster represented instance sum distances observations cluster minimal.\r\ndifference k-means centroid k-medoids medoid latter must actual observation.\r\ncontrast, centroid artificial observation derived mean values.\r\nCompared k-means, k-medoids can robust outliers [218] , least application, arguably intuitive cluster representation: \r\nmedoid can interpreted representative foot cluster, whereas centroid can different pressure distribution patient within cluster.\r\nThus, notation allows us use clustering examine temporal patterns plantar pressure visualizing cluster medoids’ RPP time curve.DBSCAN.\r\nDBSCAN [185] partitions instances clusters based estimated density distribution builds clusters arbitrary shape size. Pressure distributions feet differ considerably cluster declared outliers, may abnormal pressure patterns patients potentially requiring medical supervision.\r\nDBSCAN two parameters: radius \\(eps\\) defining “neighborhood” instance minimum number \\(minPts\\) neighbors instance core point (recall Section 6.3.1).Hierarchical clustering.\r\nagglomerative hierarchical clustering, similar instances iteratively merged clusters bottom-fashion.\r\norder two clusters merged depends linkage strategy: complete linkage, distance two clusters defined maximum distance pair instances (one cluster).\r\ntwo clusters minimize maximum distance selected merging.\r\nUsing dendrogram, possible break “tree” clusters understand progressive merging process.\r\nOptionally, parameter \\(k\\) can specified obtain specific partitioning \\(k\\) clusters.","code":""},{"path":"diabfoot.html","id":"diabfoot-approach-evaluation","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.2.4 Evaluation Setup","text":"Preprocessing.\r\nsensor recordings identified noisy erroneous.\r\nreduce impact extreme recordings, values replaced median recordings sensor session.\r\nFollowing inner fence outlier definition boxplots [219], value \\(x\\) flagged extreme fell outside range \\([Q_1 - 1. 5 \\cdot (Q_3 - Q_1), Q_3 + 1.5 \\cdot (Q_3 - Q_1)]\\), \\(Q_i\\) \\(\\)th quartile \\((Q_3 - Q_1)\\) interquartile range sensors \\(\\).\r\ntime curves smoothed using locally weighted scatterplot smoothing (LOWESS) [137] smoother span 5%.\r\nRecordings stance episodes used distance calculation clustering.Experimental setup.\r\npart , use three distance measures Section 7.2.2 cluster participant feet, using algorithms k-medoids, DBSCAN, hierarchical agglomerative clustering (Section 7.2.3).\r\nDBSCAN, estimate appropriate value \\(eps\\) using elbow method presented Section 6.3.1, \\(k\\) set \\(minPts\\).\r\nhierarchical clustering, cut dendrogram obtain \\(k\\) clusters.\r\nCluster quality measured Silhouette coefficient (recall Section 4.2.1).\r\nk-medoids, vary k 2 10.\r\nDBSCAN, set \\(minPts\\) 2 10 automatically determine \\(eps\\).\r\nhierarchical clustering, set number clusters 2 10 use single linkage, complete linkage, average linkage, respectively.\r\npart B, restrict best performing combination distance measure clustering algorithm part .Included datasets.\r\npart , foot insole sensor recordings 20 patients (5 female, 15 male, 66.2 ± 8.4 years) type 1 type 2 diabetes sensomotoric peripheral polyneuropathy available.\r\n19 participants performed experiment twice, remaining participant .\r\n34 39 experiments, recordings feet available, reaching total 73 experiment datasets.part B, data 253 diabetes patients (6 females, 19 males, age 64.8 ± 9.8 years) 18 non-diabetic healthy volunteers (10 females, 8 males, age 62.9 ± 7.6 years) available.\r\n43 sessions, feet’ sensor recordings available, used independently, reaching total 86 session datasets.","code":""},{"path":"diabfoot.html","id":"diabfoot-results","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.3 Validation on DIAB","text":"","code":""},{"path":"diabfoot.html","id":"results-on-part-a","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.3.1 Results on Part A","text":"Table 7.1 shows clustering results.\r\nmaximum Silhouette score achieved k-medoids \\(d_{\\text{RPP}}\\) (k = 4; Silh = 0.78).\r\nnumber clusters found 3 clustering algorithms differs: k-medoids finds 4 clusters distance measure, DBSCAN returns one large cluster outliers (.e., noise points); hierarchical clustering, number clusters varies , 2 10.\r\nk-medoids outperforms DBSCAN agglomerative hierarchical clustering three distance measures.\r\nBesides, \\(d_{\\text{RPP}}\\) outperforms distance measures respect three algorithms.\r\nTable 7.1: Clustering results (part ). Optimal number clusters \\(k_{opt}\\) Silhouette coefficient \\(Silh_{opt}\\) distance measure algorithm.\r\nbest clustering (\\(d_{\\text{RPP}}\\), k-medoids), boxplot-based visualization RPP distributions medoids sensor shown Figure 7.3.\r\nRPP distribution medoid 1 balanced, medoid 2 pressure focus centrally located MTB-3; medoid 3 overall higher pressure load, especially Digitus-1, MTB-2, MTB-3, MTB-5, Lateral, medoid 4 represents “skewed” pattern increasing pressure medial lateral forefoot (measured MTB-5).\r\nsubstantial differences RPP distribution medoids suggest high variability DFS patients apply pressure feet.\r\nDetermining patient subpopulations serves basis reducing risk DFS-related foot complications supporting early detection, treatment, prevention [203], [212], [214], [220]–[222], example, personalized footwear tailored patient needs [202].\r\nFigure 7.3: Cluster medoids best clustering. Visualization RPP distribution medoids best clustering (\\(d_{\\text{RPP}}\\); k = 4). boxplot panel background color represents median RPP, light gray (low median RPP) dark blue (high median RPP). D1: Digitus-1; MTB: metatarsal bone; C: Calcaneus.\r\noptimal clustering resulted 4 subgroups.\r\nnumber consistent results Deschamps et al. [203] (diabetes group) De Cock et al. [201].\r\nFurthermore, findings similar study Bennetts et al. [202].\r\nRPP distribution second cluster’s medoid corresponds “central pattern” De Cock et al. [201] cluster 2 Deschamps et al. [203].\r\nmedoid largest four clusters (n = 25 73 feet (ca. 34%)), characterized low medium RPP focal point pressure central forefoot regions MTB-3.\r\nfourth medoid similar cluster 4 Deschamps et al. [203].\r\nHowever, cluster 4 second largest study (n = 22 feet (ca. 30%)), relative proportion Deschamps et al. [203] much smaller (n = 30 194 feet (ca. 15%)).\r\nRPP pressure rather low medial regions, median RPP gradually increases toward lateral forefoot regions.\r\nbalanced, moderate pressure regions first medoid (n = 12 (ca. 16%)) represents well-distributed pressure loading pattern.\r\nsimilar largest cluster (cluster 4) Bennets et al. [202].\r\noverall high pressure loading third medoid may indicator adverse posture.\r\nPatients cluster 3 (n = 14 (ca. 19%)) may overloading feet therefore warned urgently subgroups.","code":""},{"path":"diabfoot.html","id":"results-on-part-b","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.3.2 Results on Part B","text":"Figure 7.4 shows Silhouette coefficient optimal number clusters subgroup.\r\nPatients, controls, combination groups can best described four plantar pressure profiles.\r\nclustering optimum k group, summary relative plantar pressure distribution session datasets cluster provided Figure 7.5.\r\nFigure 7.4: Silhouette coefficient different numbers clusters. Silhouette coefficient group k-medoids clustering using distribution eight plantar pressure regions number clusters k set 2 10. group, best clustering obtained k = 4 clusters.\r\n\r\nFigure 7.5: Summary visualization intra-cluster RPP distributions. Relative plantar pressure distribution cluster sensor whole session (including pauses). Panel background boxplots depicts median relative plantar pressure linear color gradient, light gray (low relative plantar pressure) violet (high relative plantar pressure). Pie charts show percentages healthy volunteers diabetes patients cluster combined group. D1: Digitus-1; MTB: metatarsal bone; C: Calcaneus. figure adapted [216].\r\nHealthy volunteers.\r\n\r\ncluster 1, representing 50% volunteer participants, median RPP high regions.\r\nClusters 2 3 show variability pressure forefoot regions.\r\nCluster 4, characterized low median RPP MTB-1 MTB-5, describes 2 36 feet volunteer group.Diabetes patients.\r\n\r\nCluster 1 largest subgroup characterized high plantar pressure (median RPP 80%).\r\nCluster 2 represents evenly balanced median RPP profile.\r\n, median relative plantar pressures Digitus-1, MTB-1, MTB-5 range 30% 50% maximum, median relative plantar pressures central forefoot (MTB-2, MTB-3, MTB-4), Lateral, Calcaneus recorded 50% 75%.\r\ncluster 3, median RPP 80% regions’ maximum values except medial regions Digitus-1 MTB-1.\r\nlatter cluster highest variance four clusters, high difference first third quartiles several regions.\r\ncluster 4, high median relative plantar pressures perceived MTB sensors (RPP > 75%) almost pressure Lateral (median RPP < 2%).perform clustering combination groups, clusters diabetes patients merge ones controls.\r\noptimal number clusters describe pressure distribution best remains 4.\r\nClusters 1-3 contain patients controls, cluster 4 summarizes pressure distribution patterns found diabetes patients severe polyneuropathy.Table 7.2 summarizes data diabetes patients healthy volunteers within combined group’s four clusters.\r\nclusters 1-3, mean values weight, height, BMI patients higher volunteers.\r\nSince cluster 4 highest values height, weight, BMI, can assumed anthropometric characteristics also influence measured pressure values class separation explained exclusively pathology.\r\nTable 7.2: Baseline characteristics. Characteristics diabetes patients (P) healthy volunteers (V) respect clustering combined group.\r\n","code":""},{"path":"diabfoot.html","id":"diabfoot-discussion","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.4 Discussion of the Findings from the Medical Perspective","text":"Four pressure clusters identified healthy volunteers diabetes patients.\r\nclusters reflect way groups distribute pressure foot regions vulnerable ulceration.\r\nultimate goal profiles reduce ulcer risk: clinical examination remains essential, pressure distribution analysis, proposed , can serve basis preventive strategies [203], [212], [214], [220]–[223], including manufacturing footwear tailored patient needs [202].\r\ninteresting finding unsupervised procedure alone, .e., without using target variable (presence diabetes), generated cluster consisting exclusively diabetes patients, see Figure 7.5 (c).result remarkable study protocol, strict sequence standing sitting episodes, specifies exactly long pressure loads applied.\r\nFurthermore, experiment, participants instructed adhere protocol, .e., apply pressure continuously standing episodes.\r\npossible diabetes patients adhered strictly study protocol due neuropathy.\r\ncan also assumed pain-related fatigue effect occurs frequently second loading phase 20 min (stance episode 6) compared first (stance episode 3), indicated larger percentage recordings pressure release.\r\nquantify changes, calculated percentage recordings participants released pressure stance episodes 3 6 (Figure 7.6).\r\nsignificant intra-group differences stance episodes 3 6 found.\r\nHowever, mean percentage pressure release recordings significantly different (p < 0.01) healthy volunteers diabetes patients sensoric polyneuropathy.\r\nFigure 7.6: Average percentage recordings without pressure application 20-minute standing episodes. Intermittent pressure release diabetes patients neuropathy occurs less frequently healthy controls. underlying dataset previous publication [31] 114 propensity-score matched observations (57 volunteers, 57 diabetes patients) analyzed. intra- inter-group statistical comparison, two-sided Student’s t-test used. figure adapted [31].\r\nComparing clusters healthy subjects diabetes patients revealed high overlap plantar pressure distribution.\r\nexample, cluster 1 cluster 3 healthy subjects diabetes patients characterized almost identical pressure loading patterns.\r\ninvestigate whether group-specific pressure distribution identified, cluster analysis performed combination groups.\r\nrandom sample diabetes patients selected ensure balanced distribution two groups.\r\nobserved pressure patterns Figure 7.5 (c) support assumption common pressure patterns: clusters 1, 2, 3 comprise pressure distributions groups.\r\nHowever, cluster 4 unique diabetes patients.\r\npattern characterized lowest median relative plantar pressure Lateral Calcaneus.Also, unlike studies, avoided k-means sensitive outliers cluster centroids composite objects, true feet; rather, provide cluster’s medoid, representative foot cluster, cluster centroid (k-means) derived vector averages may different patient’s pressure distribution inside cluster.observed differences compared related studies differences study participants, population sizes, experimental protocols, measurement devices [201]–[203].\r\nexample, neither medial M1 pattern M2 pattern [201], focuses either MTB-1 MTB-2, replicated.\r\nBesides, results observed previous studies, pattern disparity lateral regions cluster 4 (diabetes patients).main finding group patients (cluster 4) applies plantar pressure way found healthy volunteers.\r\ndraw conclusions clusters, necessary consider larger sample healthy individuals, allow reducing idiosyncrasies possibly found form homogeneous clusters healthy individuals.study protocol’s simplicity makes easily reproducible, protocol capture complexity foot movement everyday life.\r\nThus, limitation study detect differences pressure distribution can detected changes posture.\r\npatients apply pressure uncontrolled environment, variance pressure profiles inevitably increase.\r\nTherefore, study considers effortful activities patients (running, cycling, climbing stairs, etc.) needed complement results.Another limitation results small sample size, prevented examining differences female male participants gender samples small generalization.Finally, long-term follow-allow us determine patients eventually develop ulceration.\r\nprovide critical endpoint evaluate whether certain clusters associated higher risk developing diabetic foot syndrome.","code":""},{"path":"diabfoot.html","id":"diabfoot-conclusion","chapter":"7 Feature Extraction from Short Temporal Sequences for Clustering","heading":"7.5 Conclusion","text":"proposed three similarity measures short temporal sequences extracted raw mHealth data create representations distinct subpopulations via clustering.\r\nDIAB data served proof--concept validation.\r\nsimilarity measures based () relative plantar pressure, (ii) pressure distribution pairs sensors, (iii) centers pressure.\r\nfound four distinct subpopulations plantar pressure patterns.approach needs validated larger cohorts.\r\nmodeling similarity can transferred pressure temperature recordings plantar regions, possibly closing gap early detection tissue damage ulcer formation.\r\nformation diabetes patient cluster emphasizes pathological differences, .e., impaired sensation microcirculatory defects leading tissue damage.\r\nresults may lay groundwork approaches identify pressure (temperature) patterns predictive emerging foot problems diabetes future.","code":""},{"path":"iml.html","id":"iml","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8 Post-Hoc Interpretation of Classification Models","text":"chapter partly based :Uli Niemann, Philipp Berg, Annika Niemann, Oliver Beuing, Bernhard Preim, Myra Spiliopoulou, Sylvia Saalfeld. “Rupture Status Classification Intracranial Aneurysms Using Morphological Parameters.” : Computer-Based Medical Systems (CBMS). 2018, pp. 48-53. DOI: 10.1109/CBMS.2018.00016.Uli Niemann, Benjamin Boecking, Petra Brueggemann, Wilhelm Mebus, Birgit Mazurek, Myra Spiliopoulou. “Tinnitus-related distress multimodal treatment can characterized using key subset baseline variables.” : PLOS ONE 15.1 (2020), pp. 1-18. DOI: 10.1371/journal.pone.0228037.Uli Niemann, Petra Brueggemann, Benjamin Boecking, Birgit Mazurek, Myra Spiliopoulou. “Development internal validation depression severity prediction model tinnitus patients based questionnaire responses socio-demographics.” : Scientific Reports 10 (2020), pp. 1-9. DOI: 10.1038/s41598-020-61593-z.medical applications, understanding clearly communicating results machine learning model critical deriving actionable knowledge can ultimately used improve disease prevention, diagnosis, treatment [224].\r\n\r\n\r\nObtaining results data scientists medical experts easily understand helps formulate new hypotheses regarding relationship potential risk protective factors target; significance relationships can tested follow-studies.\r\nCurrent state---art machine learning algorithms produce models superior performance compared simpler interpretable models, decision trees, rule lists, linear regression fits.\r\nHowever, opaque black boxes involve many complex feature interactions decisions, non-linear, often difficult explain understandably.\r\nArising need provide understandable insights otherwise opaque models, interpretability community machine learning gained traction, intending resolve dilemma choosing moderately accurate interpretable models highly accurate opaque black-box models [25], [26], [225].\r\nchapter describes end--end data analysis workflow high-dimensional medical data includes steps data augmentation, modeling, interleaving model training feature elimination, post-hoc analysis trained models.\r\n\r\nvalidate approach three datasets.chapter organized follows.\r\nSection 8.1 describes reasons using interpretable machine learning methods provides methodological underpinnings selected pioneering methods.\r\nSubsequently, present components mining workflow Section 8.2.\r\n\r\nSection 8.3, validate workflow three datasets.\r\nSection 8.4 discusses findings medical perspective.\r\nSection 8.5 concludes chapter.","code":""},{"path":"iml.html","id":"brief-chapter-summary-5","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"Brief Chapter Summary","text":"Complex black-box models can high predictive performance difficult interpret.\r\npropose end--end data analysis workflow high-dimensional medical data includes steps data augmentation, modeling, interleaving model training feature elimination, post-hoc analysis trained models.\r\napproach delivers statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance.\r\nvalidate workflow three modeling tasks: () tinnitus-related distress treatment tinnitus patients, (ii) depression baseline tinnitus patients, (iii) rupture risk intracranial aneurysms.","code":""},{"path":"iml.html","id":"iml-motivation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.1 Motivation and Methodological Underpinnings","text":"Current state---art machine learning algorithms, gradient boosting [24] tabular data deep learning [23] unstructured data (images, videos, audio recordings), designed support medical decision-making.\r\nmethods produce models typically achieve better predictive performance simpler models decision trees, rule lists, linear regression fits.\r\nHowever, also complex, making difficult understand prediction made.\r\nThus, medical experts may face dilemma choosing either opaque black-box model high predictive power simple, less accurate model can explained domain expert.\r\nEspecially high-risk domains healthcare, misconceptions can serious consequences, ability explain model’s reasoning highly desirable (sometimes mandatory) property decision-support system [225], [226].result, methods explain predictions complex machine learning models attracted increasing attention recent years [25], [26].\r\nExisting methods classified according different criteria [225].\r\nexample, distinction made intrinsically interpretable models post-hoc explanations.\r\nformer often entails limiting model complexity choosing algorithms produce transparent models, decision trees linear regression models.\r\nDecision trees, example, can intuitively visualized node-link diagrams.\r\nFeatures split conditions near tree root generally higher impact predictions features occurring lower tree levels within leaf nodes.\r\nQuantitative measures calculate overall importance feature decrease impurity variance nodes feature occurs compared parent nodes [227].\r\nFurthermore, data partitions created decision tree can described understandable conditions “body mass index > 30,” decision paths root leaf nodes provide insights feature interactions.\r\nMoreover, can used contrasting predictions individual instances, e.g., considering alternative feature values effects model prediction (“patient body mass index 25 instead 30, difference terms prediction .”).\r\nDisadvantages decision trees capture linear, non-axis-parallel relationships predictors response, can unstable small value changes training data [228].\r\nTherefore, may unsuitable complex learning tasks.sophisticated model trained, post-hoc methods can applied examine model training.\r\nmethods’ outputs can feature summary statistics, model internals, individual observations, feature summary visualizations [225].\r\ngeneral, feature summary statistics individual scores express overall importance feature model prediction strength feature’s interaction features.\r\nExamples model internals coefficients linear model weight vectors neural network.\r\nIndividual observations can describe representatives (prototypes) observation subgroups model provides consistent predictions subgroup members.\r\nIndividual observations can also provide counterfactual explanations, e.g., determine minimum change cause model predict different class particular observation interest.","code":""},{"path":"iml.html","id":"selected-model-interpretation-methods","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.1.1 Selected Model Interpretation Methods","text":"discuss partial dependence plots [24] (PDP), Local Interpretable Model-Agnostic Explanations [229] (LIME), Shapley Additive Explanations [230] (SHAP), emphasis interpretation outputs.Partial dependence plots. Visualizations feature summaries typically depict trends relationship subset features predicted response, often form curves surface plots.\r\npartial dependence plot [24] (PDP) widely used tool visually depicting marginal effect one predictors predicted response model.\r\nexample, PDP Figure 8.1 () shows roughly S-shaped relationship predictor response estimated model.\r\n\r\nFigure 8.1: Illustrations partial dependence plot (PDP) individual conditional expectation (ICE) plot artificial data. () PDP predictor artificial dataset. Points represent sample predictor distribution. (b) PDP augmented ICE curves. two distinct subsets observations PD different upper half predictor distribution.\r\nLet \\(\\zeta\\) classification model (general function returns single real value) let \\(F=Q\\cup R\\) total set features, \\(Q\\) chosen subset features \\(R\\) complement subset.\r\nAccording Friedman [24], partial dependence \\(PD\\) model \\(\\zeta\\) \\(Q\\) can represented \\[\\begin{equation}\r\nPD(Q)=\\mathbb{E}_{R}\\left[\\zeta(X)\\right]=\\int\\zeta(Q,R)p_R(R)\\,dR.\r\n\\tag{8.1}\r\n\\end{equation}\\], \\(p_R(R)\\) marginal probability density \\(R\\), .e., \\(p_R(R)=\\int p(X)\\,dQ\\), \\(p(X)\\) joint density dataset \\(X\\).\r\ncomplement marginal density \\(p_R(R)\\) estimated training data, \\(PD\\) can approximated \\[\\begin{equation}\r\nPD(Q)=\\frac{1}{N}\\sum_{=1}^N \\zeta(Q,R_{})\r\n\\tag{8.2}\r\n\\end{equation}\\]\\(R_i\\) actual values complementary features observation \\(\\), \\(N\\) total number observations training data.\r\ncardinality \\(Q\\) usually chosen either equal 1 2.\r\nresults visualized line chart (\\(|Q|=1\\)) contour chart (\\(|Q|=2\\)).\r\npractice, random sample often drawn dataset reduce computation time.averaging across observations removes information variability, PD curves can obscure potentially distinct observation subgroups substantially different effects predictors model output.\r\nremedy, Goldstein et al. [95] proposed individual conditional expectation (ICE) plots show curve observation.\r\nFigure 8.1 (b) illustrates example small number observations (black curves) differ rest PD constant second half predictor distribution.\r\nLIME. Another criterion distinguishing model interpretation methods whether explanations global local, .e., whether explanations apply observations, one small number selected observations.\r\nLocal Interpretable Model-Agnostic Explanations [229] (LIME) popular local post-hoc interpretation method.\r\nLIME’s central assumption complex model linear local scale [229].\r\nThus, explain predictions black-box model particular observation interest \\(\\), LIME generates surrogate model intrinsically interpretable whose predictions similar predictions black-box model “proximity” \\(\\).\r\nmain ideas LIME shown Figure 8.2, Figure 8.2 () shows decision boundary black-box model.\r\nSince non-linear decision boundary quite complex, model predictions explained simple terms.\r\nLIME attempts approximate black-box model’s behavior creating linear surrogate model performs particularly well vicinity user-selected instance interest.\r\nend, perturbed training set created repeatedly randomly changing instance interest values.\r\nFigure 8.2 (b) shows instance interest perturbed instances, glyph size represents proximity instance interest.linear surrogate model trained dataset, observation weights proportional distance instance interest.\r\nFigure 8.2 (b), decision boundary surrogate model shown dashed line.\r\nFinally, model internals displayed user explanation, coefficients logistic regression model.\r\nFigure 8.2 (c) shows feature importance ranking, horizontal bar length represents model coefficient feature.\r\nLIME provides intuitive interpretations applies tabular non-tabular data, several design decisions make hyperparameters tune, including neighborhood kernel width, surrogate model family, feature selection method number features considered surrogate model.\r\nstability results LIME critically discussed [231], [232].\r\nFigure 8.2: Illustration LIME’s main ideas. () data set two-class problem represented two-dimensional scatterplot simplicity. non-linear decision boundary black-box model easily explained. (b) LIME aims approximate black-box model’s predictions vicinity instance interest intrinsically interpretable model, logistic regression model. dashed line shows linear decision boundary surrogate model. (c) feature importance ranking can derived model coefficients.\r\nModel-specific interpretation methods limited specific model families, model-agnostic interpretation methods can applied model type.\r\nModel-specific methods based model internals widely used neural networks [233], e.g., layered relevance propagation [234], explicitly uses neural network’s layered structure infer explanations.\r\ncontrast, model-agnostic methods decoupled actual learning process access algorithmic internals.\r\nSince consider model’s output, .e., predictions, model-agnostic methods also post-hoc.\r\nexample, LIME representative model-agnostic, post-hoc interpretability method.SHAP. Closely related LIME Shapley Additive Explanations [230] (SHAP) framework, derives additive feature attributions model’s predictions.\r\nSHAP based Shapley values [235]–[237], originally developed game theory.\r\nterm “additive” denotes given observation, model output equal sum attributions features.\r\nspecifically, observation \\(x\\), model output \\(\\zeta(x)\\) \r\n\\[\\begin{equation}\r\n\\zeta(x)=\\phi(\\zeta,x)_0 + \\sum_{j=1}^M \\phi(\\zeta,x)_j\r\n\\tag{8.3}\r\n\\end{equation}\\]\\(\\phi(\\zeta,x)_0=E(\\zeta(x))\\) expected value model training data, \\(\\phi(\\zeta,x)_j\\) attribution feature \\(j\\) \\(x\\), \\(M\\) total number features.\r\n, combination feature \\(j\\) observation \\(x\\), Shapely value \\(\\phi\\) represents impact predictor added, aggregated weighted average possible feature subsets \\(S\\subseteq S_{}\\):\\[\\begin{equation}\r\n\\phi_{j}(x)=\\sum_{S\\subseteq S_{}\\setminus\\left\\{j\\right\\}}\\frac{|S|!(M-|S|-1)!}{M!}\\left(\\zeta_{S\\cup{j}}(x)-\\zeta_S(x)\\right).\r\n\\tag{8.4}\r\n\\end{equation}\\]SHAP feature importance estimates offer several practical properties:sum feature attributions observation equal difference model’s average prediction actual prediction observation (local accuracy).feature important one model another, regardless features also present, importance attributed feature also higher (symmetry/monotonicity).feature value missing, associated feature importance 0 (missingness).Several approaches proposed reduce complexity Shapley value estimation exponential polynomial time, including KernelSHAP [230], works model type, TreeShap [238] tree-based models.Feature Selection.\r\nSection 5.1, discuss dimensionality reduction techniques conclude methods can used limited extent since original dimensions’ semantics lost projection.\r\ninterpretation transformed data space application context thus hardly possible.\r\nFeature selection (FS) can help limitation.\r\ncontext predictive modeling, FS methods reduce number predictors either () maximize model performance (b) affect model performance little possible.\r\nmodeling families sensitive predictors irrelevant target feature, support vector machines [239] neural networks [23], [240].\r\nOthers, linear logistic regression models, susceptible correlated predictors.\r\nOften domain experts require intrinsically interpretable models [224], requires eliminating predictors contribute substantially model performance.Traditionally, FS methods broadly classified three categories: embedded, filter, wrapper [241].\r\nEmbedded FS refers internal mechanisms modeling algorithms evaluate usefulness features.\r\nExamples algorithms include tree- rule-based models [192], [242], regularization methods least absolute shrinkage selection operator [243] (LASSO), Ridge [244] regression.\r\nFiltering methods rank predictors based measure importance, e.g., correlation target feature.\r\n\r\nPopular examples include correlation-based feature selection [190] (cf. application Section 6.3.3) Relief [245].\r\nWrapper methods rank refine candidate feature subsets iterative search driven model performance.\r\nExamples include sequential forward search, recursive backward elimination, genetic search [246].","code":""},{"path":"iml.html","id":"iml-workflow","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2 Overview of the Mining Workflow","text":"section, describe four components mining workflow, :Data augmentation, including construction new features correlation analysis features (Section 8.2.1),Modeling learning task (Section 8.2.2),Interleaving model training feature elimination (Section 8.2.3), andPost-hoc analysis learned models (Section 8.2.4).","code":""},{"path":"iml.html","id":"iml-workflow-data-augmentation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.1 Data Augmentation","text":"increase model performance, manual derivation predictive features (feature engineering) required many applications (recall evolution features presented Chapter 6).\r\nexample, image data, general approach first derive descriptive features transform data tabular format use --shelve classifiers.\r\nTherefore, derive predictive features image data first step workflow.\r\nalso explore correlations (derived) predictors step exploratory data analysis.","code":""},{"path":"iml.html","id":"iml-workflow-modeling","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.2 Modeling of the Learning Task","text":"order limited particular classification algorithm create model highest possible predictive power, examine total eleven classifiers:Least absolute shrinkage selection operator [243] (LASSO) Ridge [244] extensions ordinary least squares (OLS) regression perform feature selection regularization improve predictive performance interpretability.\r\ndataset \\(n\\) observations, \\(p\\) predictor features target \\(y\\), objective LASSO Ridge solve\r\n\\[\\begin{equation}\r\n\\underset{\\beta}{\\text{argmin}} \\underbrace{\\sum_{=1}^n \\left( y_i - \\left( \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j \\right) \\right)^2}_{\\text{Residual Sum Squares}} + \\alpha \\lambda \\underbrace{\\sum_{j=1}^p |\\beta_j|}_{\\text{L1 Penalty}} + (1-\\alpha) \\lambda \\underbrace{\\sum_{j=1}^p \\beta_j^2}_{\\text{L2 Penalty}}\r\n\\tag{8.5}\r\n\\end{equation}\\]\r\n\\(\\beta\\) determined model coefficients, \\(\\lambda\\) tuning hyperparameter controls amount regularization.\r\nLASSO uses L1 norm penalty term, .e., \\(\\alpha\\) = 1, “shrinks” coefficients’ absolute values, often forcing exactly equal 0.\r\nRidge uses L2 norm penalty term, .e., \\(\\alpha = 0\\), shrinks coefficient magnitudes.\r\ngeneral, LASSO performs better Ridge relatively small number predictors substantial coefficients remaining predictors coefficients close equal zero.\r\nRidge performs better settings response depends many predictors, approximately equal importance.\r\nperspective interpretability, LASSO advantage producing sparser models reducing values predictors’ coefficients exactly zero.Partial least squares another derivative OLS regression, first performs projection extract latent variables capture much variability among predictors possible modeling response well.\r\nlinear regression fit preferably small number latent features projection.\r\nuse generalized partial least squares (GPLS) implementation Ding Gentleman [247].\r\nsupport vector machine (SVM) [239] learns linear non-linear decision boundaries feature space separate classes.\r\ndecision boundary represented training observations difficult classify, .e., support vectors.\r\ngoal find maximum margin hyperplane, .e., separating hyperplane maximum margin support vectors.\r\ncase linear decision boundary exist, non-linear SVM approaches can used, apply -called kernel trick transform original feature space new, higher-dimensional space linear hyperplane can found separate classes.artificial neural network (NNET) consists structure nodes connected directed edges.\r\nnode performs basic unit computation.\r\nNodes supplied data values passed via incoming edges nodes.\r\nedge holds weight controls impact node forwards values.\r\nmain goal NNET adjust weights edges relationship predictors response underlying data represented.\r\nNeural networks extract new useful features original predictors relevant classification.\r\ncombining interconnected nodes complex predictive features, NNETs can extract classification-relevant feature sets compared expert-driven feature engineering dimension reduction techniques.\r\nNNETs undergone widespread adoption last decade led various success stories computer vision natural language processing [23]. used feed-forward NNET one intermediary layer (hidden unit) [240].Weighted k-nearest neighbor [248] (WKNN) variant KNN classification.\r\nclassify observation unknown response value, k nearest training observations identified, modus response values used prediction.\r\nproximity observations quantified distance measure Euclidean distance.\r\nWhereas ordinary KNN, neighbors equal influence prediction, weighted KNN considers actual distance magnitudes.\r\nresult, WKNN assigns weights training observations inversely proportional distance observation classified.Naïve Bayes classifier (NB) uses Bayes’ theorem calculate class membership probabilities.\r\n\r\nnaive property refers assumption class-conditional independence among predictors, employed reduce computational complexity obtain reliable class-conditional probability estimates.Classification regression trees [249] (CART),\r\nC5.0 [192],\r\nrandom forests [191] (RF) \r\ngradient boosted trees (GBT) [24] tree-based models.\r\nAlgorithms model family partition predictor space set non-overlapping hyperrectangles based combinations predictor-value conditions, “age > 52 & body-mass index < 25.”\r\nnew observation classified based majority class training data associated hyperrectangle belongs.\r\nRandom forests gradient boosted trees ensembles different decision trees, tree casting vote final prediction.\r\nrandom forest, base trees created independently.\r\ngradient boosted model, base trees constructed added composite model new tree reduces error current set trees.Classifier evaluation hyperparameter tuning.\r\nuse 10-fold stratified cross-validation (CV) classifier evaluation.\r\nk-fold CV, observations split k disjunct partitions.\r\npartition serves test set model trained remainder partitions.\r\nk performance estimates aggregated obtain overall performance score.\r\nperformed grid search hyperparameter selection (cf. Table 8.1).\r\nthree applications dichotomous responses different skew, accuracy might inappropriate estimate generalization performance.\r\nInstead, used area receiver operating characteristic curve (AUC) performance measure.\r\nreceiver operating characteristic curve (ROC) shows relationship sensitivity (true positive rate (TPR)) false positive rate (FPR) binary classifier.\r\narea ROC curve (AUC) takes values 0 (0% TPR, 100% FPR) 1 (100% TPR, 0 %FPR).\r\nhigher AUC suggests classifier better separating classes.Table 8.1: Overview hyperparameter tuning grid. classifiers implemented statistical programming language R [250] using package mlr [251], provides uniform interface listed machine learning algorithms R packages. grid search used tune hyperparameters using area ROC curve (AUC) evaluation measure. table provides overview classifier, including R package used, tuned hyperparameters, value ranges. hyperparameters set default values. * = {linear, polynomial, radial, sigmoid}","code":""},{"path":"iml.html","id":"iml-workflow-fe","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.3 Iterative Feature Elimination","text":"employ feature selection wrapper successively eliminates subset predictors positively contribute model’s performance.\r\npredictor’s contribution computed using model reliance [258], generalization random forest permutation feature importance [191].\r\nModel reliance estimates merit predictor \\(f\\) toward model \\(\\zeta\\) comparing classification error \\(\\zeta\\) original training set \\(\\mathbf{X}_{orig}\\) classification error \\(\\zeta\\) modified version training set \\(\\mathbf{X}_{perm}\\) values \\(f\\) randomly permuted.\r\nShuffling predictor’s values removes relationship predictor target variable.\r\nHence, assumed permuting important predictor leads higher decrease accuracy opposed predictor lower model contribution.model reliance \\(MR\\) model \\(\\zeta\\) predictor \\(f\\F\\) calculated \\[\\begin{equation}\r\nMR(f,\\zeta) = \\frac{CE(y,\\zeta(\\mathbf{X}_{perm}))}{CE(y,\\zeta(\\mathbf{X}_{orig}))}\r\n\\tag{8.6}\r\n\\end{equation}\\]\\(CE\\) classification error function takes true class labels \\(y\\) vector predicted class labels returns fraction incorrectly classified observations.\r\nhigh MR score represents high dependence model \\(f\\), since shuffling values \\(f\\) increases classification error.\r\nConversely, \\(MR\\) score smaller 1 suggests \\(f\\) potentially adversarial model performance, removal increase model performance. Thus, feature elimination wrapper starts training model full set predictors, followed iterative step subset adversarial predictors according model reliance removed, new model remaining predictors trained.\r\nfirst iteration \\(\\) = 1, initial model \\(\\zeta_1\\) calculated full set predictors \\(F_1 = F\\).\r\npredictor \\(f \\F_i\\), model reliance \\(MR(f,\\zeta_i)\\) calculated.\r\nPredictors \\(f\\F_i:MR(f,\\zeta_i)>1\\) kept iteration \\(\\) + 1 remaining predictors removed.\r\nprocedure repeated \\(MR\\) smaller equal 1, .e., \\(\\forall f \\F_i: MR(f,\\zeta_i) \\leq 1\\), \\(F_{+1} = F_i\\).\r\nrandom feature permutation introduces statistical variability, compute mean \\(MR\\) 10 runs obtain stable estimate.","code":""},{"path":"iml.html","id":"iml-workflow-iml","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.4 Post-Hoc Interpretation","text":"SHAP. facilitate model interpretation, use model-agnostic post-hoc framework SHAP [230], [238] assess feature importance CHA data.\r\nBriefly, SHAP value \\(\\phi_f(\\zeta,x)\\) expresses estimated importance feature \\(f\\) prediction model \\(\\zeta\\) instance \\(x\\) change expected value prediction \\(f\\) feature vector \\(x\\) observed instead random.\r\nSHAP framework composes model prediction sum SHAP values feature, .e., \\(\\zeta(x)=\\phi_0(\\zeta,x)+\\sum_{=1}^M \\phi_i(\\zeta,x)\\), \\(\\phi_0(\\zeta,x)\\) expected value model (bias), \\(M\\) number features.SHAP values calculated best model \\(\\zeta_{opt}\\) according AUC.\r\nranking feature’s attribution towards \\(\\zeta_{opt}\\) determined calculating average SHAP value magnitude instances, .e., \\((j)=\\sum_{=1}^N |\\phi_j(\\zeta_{opt},x)|\\),\r\n\\((j)\\) attribution \\(j\\)-th feature.\r\n\\(N\\times M\\) SHAP matrix clustered agglomerative hierarchical clustering identify subgroups patients similar SHAP values.PDP feature importance. derive global feature importance measure PD predictor.\r\nassume predictors high PD variability important.\r\nConsider two PD curves Figure 8.3: predicted response changes considerably different predictor values blue PD curve, whereas green PD curve flat line.\r\nTherefore, predictor blue PD curve higher importance score predictor green PD curve.\r\ndefine partial dependence importance \\(\\) predictor \\(f\\) average magnitude differences consecutive values along distribution \\(f\\), .e.,\\[\\begin{equation}\r\nI_f = \\frac{1}{k-1}\\sum_{}^{k-1} |PD(Q=s_i) - PD(Q=s_{+1})|\r\n\\tag{8.7}\r\n\\end{equation}\\]\\(k\\) number (sampled) values distribution \\(f\\).\r\n\r\nFigure 8.3: Illustration partial dependence importance. Partial dependence importance \\(I_f\\) two exemplary PD curves.\r\n","code":""},{"path":"iml.html","id":"iml-validation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3 Validation on Three Datasets","text":"validate workflow three datasets:CHA-Tinnitus: CHA data (recall Section 2.2.2) tinnitus-related distress baseline (T0) target variable,CHA-Depression: CHA data depression treatment (T1) target variable, andANEUR: ANEUR data (Section 2.2.4) rupture status target variable.Figure 8.4 illustrates workflow adapted datasets.\r\nFigure 8.4: (dataset-specific) mining workflow. CHA dataset, select patients complete data two classification tasks. ANEUR, segment aneurysms raw image data, perform automated centerline neck curve extraction, generate morphological features. perform correlation analysis identify relevant correlations predictors, correlations predictors response, significant differences correlation predictors response T0 T1. embed model training iterative feature elimination wrapper retains predictors identified important model. select best overall model based AUC used post-hoc interpretation methods identify predictors highest attribution model prediction global, subpopulation observation level.\r\nHereafter, report results three learning tasks, .e., regarding CHA-Tinnitus (Section 8.3.1),\r\nCHA-Depression (Section 8.3.2) \r\nANEUR (Section 8.3.3).","code":""},{"path":"iml.html","id":"iml-results-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.1 Results for CHA-Tinnitus","text":"learning problem.\r\nuse baseline (T0) data predict tinnitus patients’ distress end therapy (T1).\r\nderive target variable TQ_distress total score “Tinnitus Questionnaire” [68].\r\nmean tinnitus-related distress score decreased T0 T1 38.3 ± 17.1 31.7 ± 17.2, indicating positive effect multimodal treatment.\r\napply cutoff 46 suggested Goebel Hiller [68] distinguish patients compensated decompensated tinnitus.Correlational analysis. calculate Spearman correlation coefficient pair predictors.\r\nUsing agglomerative hierarchical clustering complete linkage, arrange predictors correlation heat map visually identify predictor subgroups similar intra-group inter-group correlations.\r\ncalculate median correlation response predictors questionnaire T0 T1 obtain potential candidate predictors important later modeling step.\r\nAlso, identify predictors highest absolute correlation response T0 T1, respectively.\r\nFinally, examine predictors whose correlation values TQ distress score differ T0 T1.\r\nFigure 8.5 () shows pairwise correlations among predictors T0.\r\nidentified two major subgroups moderate high intra-group correlations low negative inter-group correlations.\r\nlarger group (cf. upper black square Figure 8.5 ()) comprises 114 predictors (ca. 55.6%) representing negatively worded items scores higher values represent higher disease burden, e.g., ADSL_depression BI_overallcomplaints.\r\nConsequently, smaller group (cf. lower black square Figure 8.5 ()) contains 47 predictors (ca. 22.9%) positive wording, e.g., SF8 mental health score (SF8_mentalhealth) BSF elevated mood score (BSF_elevatedmood).\r\nPredictors one two subgroups exhibit moderate high negative correlation subgroup predictors.\r\n\r\n\r\nFigure 8.5 (b) compares correlation predictors TQ_distress (x-axis) treatment (y-axis).\r\nOverall, low moderate bivariate correlations observed, values -0.6 +0.6.\r\naverage absolute correlation change T0 T1 0.031.\r\nchange absolute correlation smaller 0.067 ca. 95% predictors (compare distance points diagonal line Figure 8.5 (b)).\r\n137 205 predictors (66.8%), absolute correlation decreased T0 T1.\r\nMedian target-correlations questionnaires ADSL, BSF, BI (SF8) greater (smaller) +0.3 (-0.3) moments, respectively, thus greater remaining questionnaires.\r\nFigures 8.5 (c) (d) reveal predictors ADSL, BSF, BI, SF8, TINSKAL, PSQ among top 20 predictors ranked absolute correlation TQ_distress T0 T1.\r\ngeneral depression score ADSL_depression shows largest correlation magnitude (\\(\\rho\\) = 0.630) treatment (\\(\\rho\\) = 0.564).\r\nFigure 8.5 (e) shows ten predictors largest differences correlation magnitudes T0 T1.\r\nCorrelation treatment larger predictors.\r\nFigure 8.5: Spearman correlation among predictors correlation predictors TQ_distress T0 T1. () heatmap depicts correlation coefficients pairs predictors T0. Predictors arranged result agglomerative hierarchical clustering complete linkage. two black squares depict two major subgroups correlated predictors. (b) relationship predictor TQ_distress T0 (x-axis) T1 (y-axis). diamond symbol represents median correlation predictors questionnaire. (c) Top 20 predictors exhibit highest absolute correlation TQ_distress T0. (d) Top 20 predictors exhibit highest absolute correlation TQ_distress T1. (e) Top 10 predictors highest change absolute correlation TQ_distress T0 T1.\r\nPredictive performance classification models. performances 11 classifiers across feature elimination iterations shown Figure 8.6.\r\ngradient boosted trees model (GBT) yields highest AUC (iteration = 7, AUC = 0.890 ± 0.04; mean ± SD), using 26 predictors (ca. 13%).\r\n\r\nRIDGE classifier achieves second-best performance (= 2, AUC: 0.876 ± 0.05), relying 127 features, followed random forest model (= 3, AUC: 0.872 ± 0.05) using 77 features.\r\nClassification using best model (GBT, = 7) based probability threshold 0.5 result accuracy 0.86, true positive rate (sensitivity) 0.72, true negative rate (specificity) 0.88, precision 0.48, negative predictive value 0.95.\r\nFigure 8.6: Classification results CHA-Tinnitus. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier’s run maximum AUC. Classifiers ordered maximum AUC left right.\r\ntrained using smaller feature space, classifier generates least one model similar even improved performance compared respective model learned whole set predictors.\r\nfact, except WKNN, classification methods benefit feature elimination produce best model predictor subset (cf. Figure 8.6).\r\nGBT, gain AUC 185 features 26 features (= 11) 0.01.\r\nmodel achieves maximum AUC good tradeoff high predictive performance low model complexity, therefore, decide investigate model .Feature importance. best model, attributions 26 selected features shown Figure 8.7 ().\r\nAmong 26 features 6 compound scores, 12 single items, 4 demographic features (number visited doctors, university-level education, lower secondary education, tinnitus duration), 4 features measuring average time spent completing item.\r\nTINSKAL tinnitus impairment score (TINSKAL_impairment) represents predictor highest model attribution exhibits highest average absolute SHAP value (change log odds) 0.448.\r\nADSL depression score (ADSL_depression) single question ADSL (ADSL_adsl11: “past week sleep restless.”) ranked second third important, respectively.\r\nRemarkably, 9 questionnaires, least 1 feature selected.\r\nFigure 8.7 (b) shows patient-individual SHAP values predictor, point color depicts predictor value magnitude.\r\nhigh attribution TINSKAL_impairment highlighted high range SHAP value distribution.\r\npredictor, high values generally correspond increased predicted probability tinnitus decompensation.\r\nHowever, trend non-linear since small values (light green yellow) associated SHAP value just slightly smaller equal 0.\r\nMoreover, large spread SHAP value ca. 0.7 1.2 patients high TINSKAL_impairment values, unlike somewhat dense bulk points representing patients SHAP values ca. -0.7 -0.4.\r\nindicate patients report high tinnitus impairment challenging classify.\r\n, may suggest visual analog scales robust enough quantify tinnitus-related distress.\r\ninference supported SHAP feature dependence plot 8.8 (1), juxtaposes predictor’s actual values corresponding SHAP values patients reveals J-shaped relationship .\r\nspecifically, predicted tinnitus-related distress decreases 0 2.5, remains plateau 2.5 4 increases 4 maximum value 10.\r\nBesides TINSKAL_impairment, features ADSL_depression, TINSKAL_loudness, BI_overallcomplaints, BSF_timestamp, SWOP_pessimism also show non-linear relationship respect SHAP values.\r\nFigure 8.7: SHAP analysis results best model (GBT, feature elimination iteration = 7). () Global feature importance based mean absolute SHAP magnitude observations. Values depict absolute change log odds higher values indicate higher feature attribution towards model. (b) Patient-individual SHAP values. point represents SHAP value predictor (y-axis) individual patient. afar point vertical 0-baseline, larger attribution corresponding predictor value model prediction. Vertically offset points depict high-density regions (similar violin plot), .e., patients similar SHAP values. Actual predictor values mapped point color. (c) Stacked patient-individual SHAP values six predictors highest mean absolute SHAP values. Patients ordered according hierarchical clustering Ward linkage. Black horizontal lines depict average sum SHAP values cluster members k = 5 clusters. inset plot shows Bayesian information criterion (BIC) minimal number clusters.\r\n\r\nFigure 8.8: SHAP feature dependence. relationship actual values predictor (x-axis) corresponding SHAP values (y-axis) shown points representing patient locally weighted scatterplot smoothing (LOWESS) [137] curves indicating overall trend. Predictors ordered mean absolute SHAP value (see Figure 8.7 ()). Gray histograms bar charts depict distributions predictors.\r\nEven though several predictors exhibit low moderate global importance, high attribution towards model prediction specific subgroups.\r\nexample, considering SOZK_lowersec, patients lower secondary education average SHAP value +0.5, whereas patients different education levels average SHAP value -0.1 hence closer population average (cf. Figure 8.7 (b), Figure 8.8 (13)).\r\nfeatures show monotonic relationship actual values SHAP values.\r\nexample, increasing values SF8 physical component score (SF8_physicalcomp) exhibit decreasing likelihood predicted decompensated tinnitus increasing physical health (cf. Figure 8.7 (b), Figure 8.8 (14)).investigate whether subgroups patients similar model explanations, cluster patients based SHAP values.\r\nFigure 8.7 (c) shows stacked patient-individual SHAP values six predictors highest average absolute SHAP values remaining predictors combined.\r\nAccording Bayesian information criterion (cf. inset plot Figure 8.7 (c)), optimal number patients clusters similar SHAP value patterns 5.\r\nClusters 1 5 comprise subgroups sum SHAP values predictors positive; see horizontal lines Figure 8.7 (c).\r\nHence, patients likely predicted decompensated tinnitus.comparison subgroups, patients clusters 1 5 reported higher degrees tinnitus impairment, depression, anxiety, tinnitus loudness, sleeplessness, pessimism, psychosomatic complaints, perceived levels stress social isolation.\r\ngeneral, patients cluster 1 slightly higher values across predictors patients cluster 2.\r\nAlso, cluster 1 contains higher fraction patients lower secondary education (“Hauptschule”), report frequently occurring headaches, higher levels fears future, longer tinnitus duration.\r\nCluster 3 largest subgroup comprising 39.6% patients.\r\nTogether cluster 2, subgroups lowest predicted probability tinnitus decomposition.\r\nPatients cluster 2 3 report highest physical health levels determination.\r\nCluster 4 somewhat close prediction average, means positive negative SHAP values nearly even .\r\nConcerning average patient-sum SHAP values, cluster 3 lies cluster 2 cluster 4.\r\n","code":""},{"path":"iml.html","id":"iml-results-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.2 Results for CHA-Depression","text":"learning problem.\r\nuse baseline (T0) data classify tinnitus patients’ depression severity.\r\nderive target variable total score General Depression Scale [56], [57] (ADSL).\r\nmean depression score T0 18.2 ± 11.7.\r\ntarget variable “depression status” dichotomized ADSL score.\r\nFollowing recommendation Hautzinger Bailer [57], distinguish patients subclinical (0-15) clinical (16-60) depression.Predictive performance classification models.\r\nFigure 8.9 depicts performance classification methods across iterations.\r\nLASSO classifier achieves maximum AUC classification algorithms (iteration = 1, AUC = 0.867 ± 0.037; mean ± SD), followed Ridge (= 1, AUC = 0.864 ± 0.040) GBT (= 1, AUC = 0.862 ± 0.038).\r\nconsidering best model per classifier, models similar performance, ranging AUC 0.809 (C5.0) 0.867 (LASSO).\r\nFigure 8.9: Classification results CHA-Depression. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier’s run maximum AUC. Classifiers ordered maximum AUC left right.\r\nbest model (LASSO, = 1) achieves accuracy 79%, true positive rate (sensitivity) 61%, true negative rate (specificity) 88%, precision 72%, negative predictive value 82% based probability threshold 0.5.\r\nfinal model includes 40 predictors non-zero coefficients.\r\nFigure 8.10 shows median model coefficient features across ten cross-validation folds.\r\nADSL questionnaire alone, 16 single items included final model, including indicators depression (ADSL_adsl09, ADSL_adsl18, ADSL_adsl12) perceived antipathy received people (ADSL_adsl19), sleeplessness (ADSL_adsl11), dejectedness (ADSL_adsl03), lack appetite (ADSL_adsl02), confusion (ADSL_adsl05), anxiety (ADSL_adsl10, ADSL_adsl08), absence self-respect (ADSL_adsl04, ADSL_adsl09), lack vitality (ADSL_adsl09, ADSL_adsl09), taciturnity (ADSL_adsl13) irritability (ADSL_adsl01).\r\nThus, questionnaire contributes highest number predictors model. tinnitus-distress-oriented TQ, five predictors selected.\r\n, model uses another five predictors socio-demographics questionnaire (SOZK), including German nationality (SOZK_nationality), highest absolute model coefficient, university-level graduation (SOZK_graduate), tinnitus duration (SOZK_tinnitusdur), employment status (SOZK_job), marital status (SOZK_unmarried) partnership status (SOZK_partnership).\r\nFigure 8.10: Coefficients LASSO model. Cross-validation (CV) median ± median absolute deviation (line ranges) coefficients best LASSO model (= 1). frequency non-zero coefficients 10-fold CV given parentheses right predictor name. 185 features total, 40 features exhibit non-zero model coefficient least one CV fold.\r\nTable 8.2 provides description predictor Figure 8.10.Effect feature elimination classification performance. classifiers SVM show high stability performance smaller feature subsets.\r\nFigure 8.9, see LASSO difference AUC trained 185 features (= 1) vs. trained 6 features (= 7) -0.017.\r\nSeveral classifiers benefit feature selection terms predictive performance.\r\nGPLS, NNET, CART, C5.0 RF, max. AUC achieved feature subset.\r\ndecision tree variants CART C5.0 gain performance feature removal since respective maximum AUC obtained smallest predictor subset, cardinality 22 10, respectively.\r\nTable 8.2: important features LASSO model. Predictors highest absolute coefficient final LASSO model (iteration = 1). 185 predictors total, 40 predictors exhibit non-zero model coefficient least one ten cross-validation folds.\r\n","code":""},{"path":"iml.html","id":"iml-results-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.3 Results for ANEUR","text":"learning problem.\r\npredict rupture status total 100 intracranial aneurysms morphological parameters.\r\nlearn different models subset sidewall aneurysms (SW; 9 24 ruptured), subset bifurcation aneurysms (BF; 29 62 ruptured), combined group (43 100 ruptured) 14 additional samples determined either SW BF.\r\nANEUR, necessary extract morphological features raw image data first.Segmentation neck curve extraction.\r\nAneurysms vessels segmented using threshold-based approach [259] digital subtraction data reconstructed 3D rotational angiography images.\r\nSubsequently, centerline vessel extracted using Vascular Modeling Toolkit (VMTK, vmtk.org) [260].\r\nSubsequently, plane separating aneurysm parent vessel determined using automatic ostium detection Saalfeld et al. [261].Morphological feature extraction.\r\n3D surface mesh, obtain neck curve, dome point \\(D\\), two base points \\(B_1\\) \\(B_2\\).\r\ndescribed [261], \\(B_1\\) \\(B_2\\) approximated points centerline largest distance rays \\(B_1\\) \\(B_2\\) \\(D\\) intersect surface mesh.\r\nFigure 8.11 illustrates extracted parameters, \\(H_{max}\\), \\(W_{max}\\), \\(H_{ortho}\\), \\(W_{ortho}\\), \\(D_{max}\\) (Figure 8.11 ()) describe aneurysm shape [77], [262].\r\nangle parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) (Figure 8.11 (b)) extracted based \\(B_1\\), \\(B_2\\), \\(D\\), respectively.\r\nabsolute difference \\(\\alpha\\) \\(\\beta\\) denoted \\(\\Delta_{\\alpha\\beta}\\).\r\nseparating aneurysm parent vessel neck curve, derive estimate surface area \\(A_A\\) volume \\(V_A\\) aneurysm (Figure 8.11 (c)).\r\nprovide two variants surface area ostium, \\(A_{O1}\\), \\(A_{O2}\\) (Figure 8.11 (d)).\r\n\\(A_{O1}\\) area ostium, .e., area triangulated ostium surface resulting connection neck curve points centroid \\(C_{NC}\\), \\(A_{O2}\\) denotes area neck curve projected plane [261].\r\nTherefore, \\(A_{O2}\\) extracted parameter comparable studies often use cutting plane determine ostium.\r\nmethod achieves local optimum highly lobulated aneurysms considers one many dome points.\r\nAlthough estimated positions \\(B_1\\) \\(B_2\\) may vary slightly, neck curve detection still performed, morphological parameters calculated. Table 8.3 provides overview extracted morphological features.\r\nFigure 8.11: Illustration extracted morphological features. () Features describe aneurysm width, height, diameter. (b) angles \\(\\alpha\\), \\(\\beta\\) \\(\\gamma\\) extracted base points \\(B_1\\), \\(B_2\\) dome point \\(D\\). (c) separating aneurysm parent vessel via neck curve, area \\(A_A\\) volume \\(V_A\\) computed. (d) area ostium \\(A_{O1}\\) area projected ostium \\(A_{O2}\\) extracted estimating center neck curve \\(C_{NC}\\).\r\n\r\nTable 8.3: Overview morphological features extracted ANEUR.\r\nFigure 8.12 shows classification results data subset.\r\nGBT achieves maximum AUC (cross-validation average 67.2% ± 1.8% standard deviation), followed C5.0 (AUC 64.6% ± 1.9%) GPLS (AUC 63.3% ± 1.2%).\r\nsubset SW, SVM comes best 75.2% ± 5.7% AUC, slightly superior GPLS (AUC 73.6% ± 4.4%) NNET (AUC 71.6% ± 5.5%).\r\nBF, WKNN yields best (AUC 64.0% ± 1.1%) model, GPLS (AUC 62.9% ± 2.6%) RF (AUC 62.7% ± 2.3%) similar yet slightly inferior generalization performances.\r\nresults indicate classifiers yield better performance subset sidewall aneurysms.\r\nOverall, none classification algorithms outperforms others across three subsets.\r\nFigure 8.12: Classification results ANEUR. combination data subset classification algorithm, performance run preprocessing transformation achieves highest AUC shown. SW = sidewall; BF = bifurcation.\r\nConcerning PD importance, Figure 8.13 illustrates high attribution angle parameter \\(\\gamma\\) towards rupture status classification, feature ranked first third best models BF.\r\nSVM model trained SW subset, ellipticity index (EI) important.\r\nFigure 8.13: Relative PD importance (ANEUR). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\nFigure 8.14 shows PDP ICE curves important predictors according \\(I_f\\) best models data subset.\r\nICE curves GBT model SVM model (Figure 8.14 ()  (b)) exhibit nearly identical trends different intercepts.\r\ncontrast, ICE curves WKNN (Figure 8.14 (c)) appear jittery.\r\nplots summarize idiosyncrasies different model families.\r\nGBT model (Figure 8.14 ()) produces jagged curves distinct vertical cuts, representing splits base decision trees tree ensemble.\r\nexample, plot \\(\\gamma\\) shows 4 splits {16.54, 49.26, 54.42, 64.35}.\r\nSVM classifier (Figure 8.14 (b)) linear model; hence ICE PD curves lines fixed slope.\r\nmarginal model posterior aneurysm rupture increases higher values ellipticity index, max. width aneurysm body, aspect ratio \\(H_{ortho}/N_{avg}\\) max. aneurysm diameter, whereas area ostium (variant 2), lower values indicative high rupture likelihood.\r\nWKNN, PDP better able clearly show marginal posteriors individual predictors ICE curves.\r\ndue property WKNN “lazy” learner, produce actual model makes predictions based observation-individual similarity.\r\nFigure 8.14: Relative PD importance (ANEUR). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\n","code":""},{"path":"iml.html","id":"iml-discussion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4 Interpretation of the Findings from the Medical Perspective","text":"section, discuss findings respect three classification tasks, .e., regarding\r\nCHA-Depression (Section 8.4.1), CHA-Tinnitus (Section 8.4.2) \r\nANEUR (Section 8.4.3).","code":""},{"path":"iml.html","id":"iml-discussion-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.1 CHA-Depression","text":"Machine learning used build predictive models depression severity based structured patient interviews [263], [264].\r\nrefrain quantitative comparison studies due differences population characteristics measurements.\r\ngood best models actually?\r\nreasonable baseline classifier carries depression status T0 prediction T1; model yields 79% accuracy.\r\nmodels outperform baseline, although likely provide good fit sample, patient subgroups centers yet studied.\r\nHowever, models promising first step supporting timely prediction depression severity selecting appropriate treatment questionnaire items.Consistent previous studies [265], found strong association tinnitus distress depression severity.\r\nFurthermore, predictors measuring perceived stress demands significant contributors depression tinnitus patients [266].\r\nfact predictors selected different questionnaires confirms multifactoriality depression, whose assessment requires inclusion different measurements.\r\nTherefore, concomitant emotional symptoms comorbidities must taken account meet patient-specific needs.\r\nprevious study [267], high sensitivity detecting depression achieved using two-item questionnaire.\r\nOne two items “past month, often bothered feeling , depressed, hopeless.” [267], similar ADSL_adsl06 (“past week, felt depressed.”), second-largest absolute coefficient best LASSO model.Generally, care must taken interpreting model coefficients:\r\nexample, identified strong relationship non-German citizenship depression severity (cf. Figure 8.10 Table 8.2).\r\nAlthough studies reported ethnic differences depression [268], [269], item’s occurrence tends suggest higher perceived social stress patients predominantly Turkish origin due higher unemployment rates, larger families, lower housing conditions demographic group.\r\n5.0% cohort population non-German citizens, results also result overfitting.\r\nSince associated predictor first iteration feature elimination model reliance score less 1.0, omitted sparser models.Regarding stability models smaller feature sets, results show simpler models slightly inferior predictive model.\r\nspecifically, classification methods show improvement AUC number predictors decreases.\r\nfact, 5 11 classifiers improve feature selection, .e., AUC second later iteration superior AUC first iteration (205 predictors used).\r\nexample, two decision tree variants achieve highest performance smallest feature subset case.\r\nRegarding LASSO classifier, performs best, encouraging 6 predictors 4 questionnaires show similar performance (AUC = 0.850) compared best overall model (AUC = 0.867).\r\nnoteworthy neither predictors tinnitus localization quality socio-demographic predictors included model.\r\nfinding used reduce number questions entire questionnaires patients must answer treatment.\r\nCosts can measured financial expense examination, also psychological physical burden subject undergoing examination (e.g., painful biopsy vs. blood test).\r\nexample, Yu et al. [270] performed feature selection budget, cost feature acquisition derived medical experts’ suggestions based total financial burden, patient privacy, patient inconvenience.\r\nKachuee et al. [271] derived feature costs based convenience answering questions, performing medical exams, blood urine tests.terms clinical relevance, results first step guide clinicians making treatment decisions regarding clinical depression patients chronic tinnitus.\r\nmodels used design appropriate treatment pathway.\r\nHowever, using models practice, one must aware trained cross-sectional data, .e., models separate subclinical clinical depression based questionnaire responses socio-demographic data treatment.\r\nAlso, one must keep mind treatment 7-day treatment, response depression status treatment.also limitations approach.\r\nFirst, models might subject selection bias patients complete seven questionnaires admission treatment excluded analyses.\r\nHowever, consider data “missing values” lead problematic suggestion using imputation methods.\r\nuse imputation () proportion patients complete entire questionnaire (rather individual items) (ii) know whether data missing random.\r\nHowever, number patients large, believe results sufficiently robust.\r\nfuture work, investigate possible systematic differences included excluded patients.\r\nexclusion patients dropped completing questionnaires prematurely, partly gradual loss motivation, technical unfamiliarity computer, possible interruptions staff complete baseline assessments, lead selection bias.\r\npatient population one hospital, future work involves external validation models data different populations hospitals.\r\ncross-sectional data limits interpretation prediction depression severity beyond end therapy, future work need validate models longitudinal data.Another potential limitation greedy process iterative feature selection wrapper, can miss global optima result.\r\niteration, predictors prevent model correctly classifying removed feature set.\r\npredictor eliminated, included subsequent iteration.\r\nHowever, possible including predictor removed early iteration lead better model later iteration.\r\npossible solution backtrack revisit earlier iterations turns removed predictors actually contributed positively model performance.\r\nAlternatively, \\(MR\\) cutoff value discarding features (set 1 experiments) subjected hyperparameter tuning.\r\nTherefore, future work includes comparison feature selection algorithms.","code":""},{"path":"iml.html","id":"iml-discussion-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.2 CHA-Tinnitus","text":"trained classification models predict tinnitus-related distress multimodal treatment (T1) patients chronic tinnitus based self-report questionnaires data acquired treatment (T0).\r\ngradient boosted trees model, uses 26 (12.7%) total 205 predictors, separates patients “compensated” vs. “decompensated” tinnitus best AUC.\r\n\r\nAmong features measurements describe variety psychological psychosomatic patient characteristics socio-demographics, therefore confirming multi-factorial nature tinnitus-related distress.\r\ncharacteristics used phenotyping Chapter 5.\r\nAdditionally, predictors can investigated follow-study characteristics influence treatment success.\r\n\r\nexpected, predictors directly linked tinnitus quality show high model attribution, degree perceived tinnitus impairment loudness.\r\ntime, depression, attitudinal factors (self-efficacy, pessimism, complaint tendency), sleep problems, educational level, tinnitus location, duration also emerged highly important model prediction.Quantitative predictors, tinnitus impairment loudness, show non-monotonic relationships respect predicted outcome.\r\nNotably, low self-reported impairment loudness measured visual analog scales generally indicate low tinnitus-related distress measured TQ.\r\nOne explanation simple measurements like TINSKAL_impairment TINSKAL_loudness less robust show higher variability compound scale combines multiple single questionnaire items.\r\nfindings investigated , e.g., whether relationship towards subgroup fatigued patients fill questionnaires less thoroughly.results confirm intricate interplay depression tinnitus-related distress, elucidated numerous previous studies [265], [272]–[275].\r\nbest model, ADSL score 20 associated increased predicted risk tinnitus decompensation (cf. Figure 8.8 (2)), close cutoff clinical relevance depression [57].context parsimonious learning, general strategy determine set predictors small possible inclusion predictor yield considerable performance improvement.\r\nmany predictors really necessary accurate tinnitus distress prediction?\r\nFigure 8.15 () illustrates change performance GBT classifier predictors iteratively added feature space order SHAP values respect best model.\r\nmodel uses TINSKAL_impairment achieves AUC = 0.79 ± 0.06.\r\nAdding ADSL_depression leads improvement AUC 0.06.\r\nHowever, none remaining 24 predictors results improvement 0.01, respectively.\r\nMoreover, 3 predictors necessary model AUC = 0.85, 8 predictors model AUC = 0.87 15 predictors model AUC = 0.89 (cf. Figure 8.15 ()).\r\nFigure 8.15: Cumulative feature contribution correlation network. () Cross-validation AUC (average ± standard deviation) GBT model trained feature subset comprising predictors denoted y-axis iteration. ordering features according mean absolute SHAP value (cf. Figure 8.7 ()). (b) Network illustrating 3 groups features among 26 selected predictors best model high intra-group correlation (\\(|\\rho| \\geq\\) 0.5). Eight predictors (predominantly SOZK) without moderate high pairwise correlation shown.\r\nOne potential explanation multicollinearity among groups predictors.\r\nFigure 8.15 (b) shows network 3 predictor groups among 26 features best model.\r\nexample, features TINSKAL_impairment TINSKAL_loudness moderately correlated (Spearman correlation \\(\\rho\\) = 0.69), raises question whether one two predictors removed without considerable loss AUC.\r\nlargest subgroup spanning 14 features involves descriptors depression, perceived stress, reported physical health.\r\nfuture work, investigation possible interaction effects among moderately strongly correlated features investigated understand better selected determine whether removed achieve better tradeoff model accuracy complexity.workflow leverages potential machine learning identifying key predictors variety features collected treatment post-treatment tinnitus compensation ensuring every potential predictor included analysis internal validation classification models using cross-validation hyperparameter tuning.\r\nFurthermore, selecting various classification algorithm families, linear non-linear relationships feature outcome identified.\r\nlimitation hypothesis-free approach learned models contain features quantify similar patient characteristics.\r\nexample, best model study included two highly correlated features ADSL_depression BSF_anxdepression (anxious depressiveness score).\r\ninclusion features contributed model performance, medical perspective, predictive model certain features might beneficial.\r\nPreselecting features avoid multicollinearity direction future work.Finally, exclusion 2701 4117 patients (65.6%) complete 10 questionnaires resulted selection bias.\r\nMany patients spent one hour completing questionnaire dedicated minicomputer , therefore, likely drop completion process.\r\n\r\nCompleters slightly younger non-completers (mean age 49.8 ± 12.2 vs. 51.7 ± 13.8), likely highest German school degree “Abitur” (48.2% vs. 42.0%) suffering tinnitus longer (> 5 years: 33.3% vs. 25.1%).\r\n\r\n\r\n\r\nfuture work, intend investigate extent insights completers can used subsamples non-completers.\r\nTherefore, can use DIVA framework Hielscher et al. [104].\r\nHowever, psychological treatment approaches likely benefit report psychological problems tinnitus perception associated tinnitus perception.","code":""},{"path":"iml.html","id":"iml-discussion-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.3 ANEUR","text":"classification results promising, morphological parameters alone can provide models moderate power.\r\nprevious studies found hemodynamic parameters also predictive [276], [277], future work includes exploring potential combining morphologic hemodynamic features classification rupture status.\r\nfocus, now, quantifying merit morphologic parameters, ignored demographic characteristics, age sex, also correlate strongly aneurysm rupture [278].\r\nexpect adding patient characteristics improve classification performance.\r\nPDP analysis shows best model (gradient boosted trees), parameters angle dome point \\(\\gamma\\), ellipticity index \\(EI\\), maximum aneurysm width \\(W_{max}\\), nonsphericity index \\(NSI\\), aneurysm area \\(A_{O2}\\) highest attribution (see Figure 8.13 ()).\r\ndiffer found two subsets sidewall bifurcation aneurysms, respectively.\r\nFigure 8.14 shows none features appear among top 5 predictors sidewall aneurysms, bifurcation aneurysms, overall data set.\r\nalso partly family best model different subsets.\r\nConsequently, Figure 8.16 shows PDP curves top 5 features differ substantially.\r\nTherefore, argue PDPs appropriate intra-model comparisons feature attributions.\r\nFigure 8.16: PDP curves top 5 predictors - GBT data subset’s best model.\r\nobserve classification performance consistently higher subset sidewall aneurysms vs. bifurcation aneurysms different parameters found high model attribution.\r\npartially due relatively small sample size already mentioned differences model families.\r\nHowever, Baharoglu et al. [79] identified significant differences sidewall bifurcation aneurysms morphological parameters can predict rupture status.findings also suggest form higher-level interactions groups features.\r\nexample, ellipticity index (\\(EI\\)) found second important - GBT important SW - SVM, although differences \\(EI\\) unruptured ruptured aneurysms significant (p = 0.323, Wilcoxon rank-sum test, \\(\\alpha\\) = 0.01).limitations analysis.\r\nsmall sample size, especially subset sidewall aneurysms (N=24), lead overfitting.\r\nfuture work, like retrain models larger number datasets incorporate wider variety predictors, hemodynamic demographic features, mentioned .\r\nlimitation concerns validity class label.\r\nSamples labeled unruptured ruptured later moment.\r\n, like investigate samples high classification error detail.\r\n, goal derive descriptions aneurysms subgroups hard classify understand reasons misclassification better signalize medical expert manual diagnosis necessary.","code":""},{"path":"iml.html","id":"iml-conclusion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.5 Conclusion","text":"previous chapters present methods subpopulation discovery yield interpretable results.\r\nlast years, accurate black-box models gained traction.\r\nHowever, additional post-learning steps required interpret behavior extract actionable insights due opacity.chapter, complemented previous chapters’ approaches proposing end--end data analysis workflow high-dimensional medical data includes steps data augmentation, modeling, interleaving model training feature elimination, post-hoc analysis trained models.\r\npost-mining step workflow removes limitation medical researchers limited intrinsically interpretable model families determining key variables corresponding value ranges model-, subpopulation-, observation-level model training.\r\nFuture work includes investigating robustness uncertainty interpretability methods bring, described “application-grounded” evaluation Doshi-Velez Kim [279].applications, already predefined subpopulations, e.g., female male patients.\r\ninteresting study differences regarding relationship features target variable black-box model.\r\ntackle issue Chapter 9.","code":""},{"path":"gender.html","id":"gender","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","text":"chapter partly based :Uli Niemann, Benjamin Boecking, Petra Brueggemann, Birgit Mazurek, Myra Spiliopoulou. “Gender-Specific Differences Patients Chronic Tinnitus – Baseline Characteristics Treatment Effects.” : Frontiers Neuroscience 14 (2020), pp. 1-11. DOI: 10.3389/fnins.2020.00487.previous chapter describes workflow post-hoc analysis models built entire population.\r\nseveral medical applications, either already established subpopulations need investigation, unclear whether indeed differences predefined subgroups interest.chapter presents workflow explore two disjoint subpopulations differ concerning predictive features.\r\nFirst, build machine learning models separate two subpopulations identify informative features associated either subpopulation.\r\n, subpopulation separately, train models predict value target variable.\r\nuse quantitative qualitative mechanism compare differences feature importance subpopulations.\r\nvalidate workflow CHA SHIP data samples.\r\nCHA, investigate questionnaire items scores predictive tinnitus-related distress depression.\r\nSHIP, identify variables baseline examinations potentially long-term determinants fatty liver measured second follow-ten years later.chapter organized follows.\r\nSection 9.1, motivate subpopulation-specific model interpretation discussing gender differences tinnitus.\r\nSection 9.2 presents workflow.\r\nSection 9.3 describes measure quantify visualize subpopulation-specific model differences.\r\nSection 9.4 explains validation setup, including learning tasks, selected learning algorithms, dataset-specific preprocessing steps.\r\nSection 9.5, report results main findings.\r\nchapter closes summary brief discussion Section 9.6.","code":""},{"path":"gender.html","id":"brief-chapter-summary-6","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"Brief Chapter Summary","text":"present workflow examine differences two priori defined subpopulations temporal data.\r\npurpose, derive post-hoc interpretation measure visually assess difference features’ relationship predicted target variable two subpopulations.\r\nvalidate approach two data samples target variables tinnitus distress (CHA), depression (CHA), liver fat concentration (SHIP).\r\ndetermine gender-specific differences, .e., separate discriminating features () , (ii) one two, (iii) neither subpopulations.","code":""},{"path":"gender.html","id":"gender-intro","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.1 Motivation and Comparison to Related Work","text":"One example relationship subpopulation membership target variable well understood yet gender differences tinnitus patients.\r\nFemale gender identified important risk factor psychological comorbidities many studies: women show higher prevalence rates regarding depression, anxiety, psychosomatic diseases [280]–[286].\r\nHowever, previous studies presented conflicting results relationship gender tinnitus severity distress.\r\nstudies found gender differences respect tinnitus severity [287], annoyance [288], similar tinnitus-related scores [289].\r\nOthers found higher tinnitus distress women [290], higher loudness annoyance men [291], high association severe tinnitus suicide attempts women [292], high correlation tinnitus severity life quality, depression, stress men [293].\r\nOverall, consensus gender-specific determinants prevalence rates accompanying symptoms chronic tinnitus depression anxiety.\r\nHowever, gender differences psychological response profiles coping strategies substantially influence tinnitus chronification treatment success rates [294].\r\nUnderstanding gender differences may therefore facilitate detailed identification symptom profiles, increase treatment response rates, help provide access vulnerable populations may less visible clinical setting.","code":""},{"path":"gender.html","id":"gender-workflow","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.2 Workflow","text":"present workflow consisting modeling component post-modeling component, extending approach presented Chapter 8.\r\nspecifically, () build machine learning models capable predicting values target variable performing hyperparameter optimization cross-validation scheme, (ii) use post-hoc interpretation mechanisms identify variables contributed prediction best model.\r\ndeviate workflow presented earlier interested comparing priori defined disjoint subpopulations instead determining .proceed follows.\r\nFirst, train model population subpopulation membership target variable.\r\n, build models subpopulation separately compare model reliance values\r\n\r\nbest performing models.\r\nspecifically, compare variables appear important () subpopulations, (ii) one two subpopulations, (iii) neither subpopulation.\r\nexample, split dataset two non-overlapping based variable defining subpopulations validate workflow examining factors discriminate subpopulations factors predictive response variable either two subpopulations.","code":""},{"path":"gender.html","id":"gender-measure","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3 Comparing Differences in Feature Importance between Two Subpopulations","text":"measure feature-individual attributions model’s predictions compare two subpopulations, use model reliance (MR; [258]), described context iterative feature elimination Section 8.2.3.\r\nRecall \\(MR(f,\\zeta)\\) permutation-based variable importance measure calculates increase error model \\(\\zeta\\) values variable interest \\(f\\) randomly shuffled within training set.\r\n\\(f\\) important prediction \\(\\zeta\\), \\(MR(f,\\zeta)\\) > 1.\r\nrandom permutation feature values leads higher performance model, feature’s attribute model quality low, whereupon \\(MR\\) < 1.models predict subpopulation membership, rank features MR value report important features.\r\nsubpopulation-specific models, use scatterplots (see Figure 9.1) depicting feature’s contribution (\\(MR\\) value) model Model_1 (x-axis), trained one subpopulation, model Model_2 (y-axis), trained subpopulation.\r\nFeatures contribute equally models diagonal line, higher \\(MR\\) values one model diagonal.\r\nvariables highest average \\(MR\\) score highest difference magnitude subpopulation-specific \\(MR\\) scores colored labeled.\r\nFigure 9.1: Subpopulation-specific variable importance. position point represents model reliance score variable best model trained respective subpopulation, denoted Model_1 (x-axis) Model_2 (y-axis). Higher values represent higher attribution variable relative model prediction. four characteristic areas: () important variables similar attributions Model_1 Model_2; (ii) important variables higher attribution one subpopulation-specific models; (iii) variables important either Model_1 Model_2 adversarial model; (iv) variables adversarial models.\r\n","code":""},{"path":"gender.html","id":"gender-learning-tasks","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.4 Learning Tasks and Evaluation Setup","text":"validate workflow Charité tinnitus patient dataset (CHA, Section 2.2.2) SHIP dataset (Section 2.2.1).\r\ndefine five learning tasks (LT) following response variables subpopulations:LT 1: gender (CHA)LT 2: tinnitus-related distress (CHA)\r\nLT 2a: female subpopulation\r\nLT 2b: male subpopulation\r\nLT 2a: female subpopulationLT 2b: male subpopulationLT 3: depression (CHA)\r\nLT 3a: female subpopulation\r\nLT 3b: male subpopulation\r\nLT 3a: female subpopulationLT 3b: male subpopulationLT 4: gender (SHIP)LT 5: liver fat concentration second follow-(SHIP)\r\nLT 5a: female subpopulation\r\nLT 5b: male subpopulation\r\nLT 5a: female subpopulationLT 5b: male subpopulationLT 1 LT 4 gender target variable, characteristics identified predictive one genders.\r\nlearning task, use variables first study predictors.\r\nlearning tasks LT 2, LT 3, LT 5, build separate models two gender subpopulations.\r\nrefer models “F_model” “M_model,” respectively; learning task explicitly stated inferred context.Selection algorithms evaluation.\r\nBased encouraging performances relative classifiers Chapter 8, use following five algorithms:\r\nleast absolute shrinkage selection operator (LASSO [243]),\r\nRIDGE [244], support vector machine (SVM [239]), random forest (RF [191]) gradient boosted trees (GBT [24]), see Section 8.2.2 description.\r\nuse 10-fold cross-validation evaluate model generalization performance perform grid search hyperparameter selection (cf. listing parameter candidates Table 8.1).\r\nchoose evaluation measures based type target variable.\r\nLT 1, employ accuracy sensitivity gender.\r\nLT 2, use root mean squared error (RMSE) coefficient determination R2, defined \\(R^2=1-\\frac{\\sum_{=1}^N (\\hat{y}_i-y_i)^2}{\\sum_{=1}^N(\\bar{y}-y_i)^2}\\), \\(\\bar{y}\\) average response value.\r\nHigher values better measures except RMSE.learning task, define baseline performance.\r\nclassification problem LT 1 LT 4, baseline equal model always predicts majority class training observations.\r\nSimilarly, regression problems LT 2, 3, 5, average value target variable training observations used predict every test instance’s response value.Data preparation CHA.\r\nCHA, use baseline data collected start therapy.\r\nmake learning tasks nontrivial, remove variables questionnaire response variable task.\r\nexample, LT 2, exclude variables TQ questionnaire target variable calculated .\r\ninclude 1628 patients (828 female, 800 male) complete data ADSL, PSQ, SF8, SOZK, TINSKAL, TQ, TLQ questionnaires (cf. Table 2.1).\r\nselection questionnaires motivated obtain comprehensive assessment tinnitus, comorbid conditions (e.g., depression), general quality life, socio-demographic data.\r\nCHA SHIP, multinomial variables, .e., variables take three symbolic values, reported gender, marital status, education level, encoded dummy variables.\r\nexample, smoking status smoking_s0 can one following values: 0 = never smoker, 1 = ex-smoker, 2 = current smoker.\r\ndummy variable smoking_s0_1 indicates whether study participant ex-smoker.\r\navoid multicollinearity, remove first dummy variable, leaving n-1 dummies.\r\nsmoking status example, keep smoking_s0_1 smoking_s0_2, remove smoking_s0_0.\r\nFinally, total 181 variables baseline measurements used predictors, including responses individual questionnaire items, subscale scores, total scores, , questionnaire, average time taken complete item.\r\nTinnitus-related distress measured TQ total score (TQ_distress).\r\nseverity depression measured ADSL total score (ADSL_depression).Data preparation SHIP.\r\nSHIP data, consider variables recorded SHIP-0 use liver fat concentration (liverfat_s2) measured via MRT SHIP-2.\r\nuse subset 886 labeled participants Chapter 4, 460 female 426 male.\r\nremove variables related ultrasound diagnosis hepatic steatosis, stea_s0 stea_alt75_s0, already identified high correlation target Chapters 3  4.\r\nFurthermore, remove “near-zero” variance variables frequent value occurs least 19 times frequently second frequent value.\r\nTypical examples include ATC_* medication variables participants report taking .\r\nVariables variance near zero can lead resampling problems resamples may constant values variable.\r\nBesides, difficult infer significant correlations , unclear whether measured effects can generalized overall population whether just artifact small thus unrepresentative sample.\r\n\r\n\r\n\r\n\r\nFinally, remove highly correlated features using algorithm Kuhn Johnson [242].\r\nfirst compute Pearson correlation coefficient pair features.\r\nfeature pairs absolute correlation value \\(r \\geq\\) 0.9, keep feature lower average correlation features.\r\nrepeat process none correlation values exceed specified threshold.\r\noriginal 350 variables, 118 remain used modeling.\r\nSince determining appropriate type missingness variable beyond workflow’s scope, assume missingness occurs completely random (MCAR).\r\nMissing values thus imputed random sampling replacement.","code":""},{"path":"gender.html","id":"gender-results","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.5 Validation on Two Datasets","text":"","code":""},{"path":"gender.html","id":"results-for-cha","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.5.1 Results for CHA","text":"Distribution target variables.\r\nFigure 9.4 shows target variables’ distributions CHA learning tasks (LTs) 1-3.\r\nslightly female male patients.\r\ngeneral, female patients report higher levels tinnitus-related distress (median ± median absolute deviation (MAD) 39.0 ± 17.8 vs. 35.5 ± 20.0) depression (18.0 ± 11.9 vs. 14.00 ± 11.9).\r\nTQ_distress ADSL_depression right-skewed subpopulation.\r\nUsing TQ cutoff 46 tinnitus distress [68], 34.1% females 30.8% males show decompensated tinnitus.\r\nUsing ADSL cutoff 15 depression severity [57], 57.4% female 45.0% male subjects exhibit clinical depression.\r\nFigure 9.2: Distribution target variables (LT 1-3). numerical targets, median, median absolute deviation (MAD), non-parametric 95% confidence interval (CI) using bootstrap sampling [295] 2000 samples presented.\r\nTables 9.1-9.5 provide overview generalization performances method learning task.\r\nTable 9.1: Classifier performance (LT 1). Performance values cross-validation mean ± standard deviation. best performance measure highlighted bold. Sens. = Sensitivity.\r\nLearning task 1 (CHA, gender classification).\r\nRidge achieves best cross-validation accuracy (mean: 72.2% ± standard deviation: 2.9%) sensitivity 71.4% ± 5.5% female patients 73.0 ± 4.3% male patients.\r\nFigure 9.3 illustrates item response frequencies variables among top 5% respect model reliance (MR), .e., 8 variables highest attribution model prediction.\r\nvariable, horizontal legend shows corresponding text questionnaire item.\r\nvertical axis shows responses item, horizontal axis depicts relative frequency gender.\r\nfrequencies shown bars, red-violet female patients blue male patients.\r\ndifference length two bars answer means percentage giving response different gender; thus, variable contributing class separation.item ADSL_adsl17 (Figure 9.3 ()) discriminating variable model (MR = 1.167): 16% female patients report crying spells either “mostly” “occasionally” past week, 4% male patients ; predominantly give answer “rarely” (86.2%).\r\nFemale patients tend express higher levels worry (see Figures 9.3 (b) 9.3 (f)) subjective stress (see Figure 9.3 (h)).\r\nBesides, gender-differences tinnitus quality: half (52.4%) male patients report tinnitus sound (MR = 1.056) “whistling,” substantially frequent female patients (35.6%), describe tinnitus “rustling” noise often (33.3%) male patients (22.9%).\r\nFigure 9.3: Top 8 variables gender (LT 1). Gender-specific item response frequencies top 5% variables highest attribution towards model prediction according model reliance (MR).\r\nLearning task 2 (CHA, tinnitus distress prediction).\r\nLT 2, ran five algorithms female patients (LT 2a) male patients (LT 2b).\r\nTable 9.2 shows RMSE R2 algorithm.\r\nLT 2a LT 2b, GBT exhibits best performance terms RMSE (LT 2a: 10.92 ± 0.68, LT 2b: 10.11 ± 1.12) R2 (LT 2a: 0.55 ± 0.04, LT 2b: 0.68 ± 0.06).\r\nnoticeable GBT models slightly accurate male patients females.\r\nhighest MR feature attribution achieved TINSKAL_impairment, .e., TINSKAL visual analog scale tinnitus impairment.\r\nFigure 9.4 () illustrates MR scores higher male patients (MR = 1.42 vs. 1.24).\r\nFurthermore, variables ADSL_depression (depression), ADSL_adsl11 (sleep problems), TINSKAL_loudness (tinnitus loudness) appear important models.\r\nnoteworthy MR scores 120 variables close 1, visualized clump points Figure 9.4 ().\r\nfact, 8 variables exhibit substantial attribution MR > 1.05 either gender-specific models.\r\nTable 9.2: Regression model performance (LT 2a, LT 2b). Performance values cross-validation mean ± standard deviation. best performance measure highlighted bold.\r\n\r\nFigure 9.4: Juxtaposition variable importance (LT 2 LT 3). scatterplot shows model reliance (MR) score predictor best model subpopulations female (x-axis) male (y-axis) patients learning task; see Figure 9.1. Variables among top 10 highest-ranking variables MR F_model (red-violet), M_model (blue), models (yellow) highlighted. () LT 2 (CHA, response: tinnitus-related distress); (b) LT 3 (CHA, response: depression severity).\r\nLearning task 3 (CHA, depression prediction).\r\ndepression severity, LASSO provides best model female patients (RMSE = 5.80 ± 0.73; R2 = 0.74 ± 0.06) male patients (RMSE = 5.10 ± 0.38; R2 = 0.81 ± 0.03), depicted Table 9.3.\r\nSimilar LT2, RMSE R2 estimates models consistently better subgroup male patients.\r\nFigure 9.4 (b) shows mental health indicator SF8_mentalhealth important predictor F_model M_model.\r\nFurthermore, features measuring subjective stress (PSQ_stress05: “feel lonely isolated.”), worry (PSQ_worries score), vitality (SF8_sf05: “much energy last 4 weeks?”) contribute substantially predictions genders’ models.\r\nTable 9.3: Regression model performance (LT 3a, LT 3b). Performance values cross-validation mean ± standard deviation. best performance measure highlighted bold.\r\n","code":""},{"path":"gender.html","id":"results-for-ship","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.5.2 Results for SHIP","text":"Learning task 4 (SHIP, gender classification).\r\nTable 9.4 shows LASSO performs best terms accuracy (99.4% ± 0.8%) sensitivity male class (99.5% ± 1.0%), GBT achieves highest sensitivity female class (99.6% ± 0.9%).\r\nregression models outperform baseline, constantly predicts female majority class (51.9% ± 4.8%).\r\nFigure 9.5 depicts distributions 4 features MR score least 1.05 LASSO model.\r\ntwo anthropometric measures, waist circumference (som_tail_s0) hip circumference (som_huef_s0), appear predictive gender study participant.\r\nFigure 9.5 (), can inferred , general, men higher waist circumference women (median: 92.2 cm vs. 77.5 cm).\r\nmedian hip circumferences similar (f: 99.8 cm, m: 100.0 cm), can seen Figure 9.5 (d) distribution women wider spread values beyond distribution tails men, .e., values 86 cm 123 cm.\r\nsecond important feature, gfr_mdrd_s0, glomerular filtration rate, measure overall renal function describes flow rate filtered fluid kidney.\r\nFigure 9.5 (b) shows gfr_mdrd_s0 generally higher men, consistent literature [296].\r\nthird important feature, crea_s_s0 (Figure 9.5 (c)), measures serum creatinine concentration, another indicator renal function, higher men [297].\r\nTable 9.4: Classifier performance (LT 4). Performance values cross-validation mean ± standard deviation. best performance measure highlighted bold. Sens. = Sensitivity.\r\n\r\nFigure 9.5: Top 4 variables gender (LT 4). Gender-specific item response frequencies 4 variables highest attribution towards model prediction according model reliance (MR). crea_s_s0, outlier (creas_s_s0 = 281) removed preserve plot readability. GFR = glomerular filtration rate.\r\nLearning task 5 (SHIP, prediction liver fat concentration).\r\ntarget variable liver fat_s2 highly right-skewed gender subpopulation (Figure 9.6 ()).\r\nexample, whereas lower half females’ distribution lies 1.18% 3.29%, upper half much wider spread, ranging 3.29% 41.8%.\r\nMales higher median liver fat concentration (5.57%) females (3.29%).\r\nfemale subpopulation (LT 5a), LASSO performs best terms RMSE (5.76 ± 1.05) R2 (0.23 ± 0.12), whereas GBT achieves minimum RMSE (6.02 ± 1.11) maximum R2 (0.20 ± 0.16) male subpopulation (LT 5b), see Table 9.5.\r\nregression models LT 5a outperform baseline predicting subpopulation mean liver fat concentration.\r\nSVM performs worse RMSE baseline LT 5b.Figure 9.6 (b) visualizes LASSO models’ MR scores female male subpopulations.\r\nWaist circumference (som_tail_s0) serum uric acid concentration (hrs_s_s0) predictors appear among top 10 features subpopulations.\r\nnote, several features considerably responsible model predictions one subpopulation , illustrated points near horizontal vertical dashed lines.\r\nRecall lines indicate MR = 1, expresses feature neither contributes hinders model performance.Waist circumference (som_tail_s0) serum uric acid concentration (hrs_s_s0) predictors appear among top 10 features subpopulations.\r\nfeature metabolic syndrome (metsyn_s0_1) appears predictive female subpopulation.\r\nIndeed, Pearson point biserial correlation metsyn_s0_1 liverfat_s2 \\(r\\) = 0.44 female subpopulation, considerably larger male subpopulation, \\(r\\) = 0.22.\r\nSimilarly, feature alat_s_s0, measures alanine aminotransferase (ALAT) concentration, informative liver fat concentration male subpopulation (\\(r\\) = 0.38 men vs \\(r\\) = 0.16 women).\r\nFurthermore, important characteristics women increased waist circumference categorization (waiidf_s0_1; waist circumference \\(\\geq\\) 80 cm; cutoff 94 cm men), serum triglyceride concentration (tq_s_s0), heart rate (heartr_s0), sleep problems (sleepp_s0_1), waist--hip ratio (whratc_s0_1), antihypertensive medication (antihyp_s0_1), ferritin concentration (ferri_s0).\r\nmen, features highest model attribution include restless legs syndrome (rlegs_s0_1), serum creatinine concentration (crea_s0_s0), body mass index (som_bmi_s0), diastolic blood pressure (diabp_s0), smoking status (smoking_s0_1), genetic marker rs11597086 (gx_rs11597086_2) associated ALAT concentration [116], “SF-12 Physical Mental Health Summary Scale” score (mcs_sf12_s0; [298]).\r\nfemales (males), 10 (16) 116 features, holds MR \\(\\geq\\) 1. numbers identical number features nonzero coefficient respective LASSO models.\r\nFigure 9.6: Distribution target variable variable importance (LT 5). () Distribution liverfat_s2 subset female male SHIP participants. (b) scatterplot shows model reliance (MR) score predictor best model subpopulations female (x-axis) male (y-axis) patients learning task; see Figure 9.1. Variables among top 10 highest-ranked variables MR F_model (red-violet), M_model (blue), models (yellow) highlighted. alat_s_s0: alanine aminotransferase (ALAT) concentration; antihyp_s0_1: antihypertensive medication; crea_s_s0: serum creatinine concentration; diabp_s0: diastolic blood pressure; ferri_s0: ferritin concentration; gx_rs11597086_2: genetic marker associated ALAT concentration [116]; heartr_s0: heart rate; hrs_s_s0: serum uric acid concentration; mcs_sf12_s0: SF-12 Physical Mental Health Summary Scale [298]; metsyn_s0_1: metabolic syndrome; rlegs_s0_1: restless legs syndrome; sleepp_s0_1: sleep problems; smoking_s0_1: ex-smoker; som_bmi_s0: body mass index; som_tail_s0: waist circumference; tg_s_s0: serum triglycerides concentration; waiidf_s0_1: increased waist circumference; whratc_s0_1: waist hip ratio.\r\n\r\nTable 9.5: Regression model performance (LT 5a, LT 5b). Performance values cross-validation mean ± standard deviation. best performance measure highlighted bold.\r\n","code":""},{"path":"gender.html","id":"gender-conclusion","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.6 Conclusion","text":"presented workflow juxtapose important predictors two priori defined subpopulations based black-box models.\r\nadapted model reliance estimate feature’s attribution regarding model investigate subpopulation-specific differences respect.\r\nCompared workflow Chapter 8, goal find parsimonious model; hence perform feature selection.goals related causal inference, aims measure true, unconfounded effect variables.\r\nCurrent efforts include building methods detect causal relationships observational data [299], [300], opposed randomized clinical trial (RCT) clinical research.\r\nRCTs considered gold standard inferring causal effects treatment [36].\r\nRCT, individual patients patient population randomly assigned one two subgroups: treatment subgroup control subgroup.\r\nformer receives treatment.\r\nRandomization subgroup membership serves minimize effects potential confounders selection bias.\r\ntwo subgroups similar possible intervention possible calculate average treatment effect quantify true causal efficacy treatment [301].\r\nTranslated application, appropriate causality question : effect gender tinnitus severity depression?\r\nHowever, goal different: wanted exploratively examine similarities differences subpopulations regarding factors predictive response interest.\r\nThus, interested relationship gender response, also differences female male subpopulations predictability variables response.\r\ngeneration new hypotheses main goal mind, focus providing exploratory methods comparing differences predefined subpopulations.","code":""},{"path":"conclusion.html","id":"conclusion","chapter":"10 Summary and Future Work","heading":"10 Summary and Future Work","text":"Data-driven machine learning solutions can complement traditionally hypothesis-driven workflows medical research discovering characteristic subpopulations patients study participants.\r\nKnowledge characteristic subpopulations can starting point investigation, e.g., identifying (long-term) risk factors, determining differences treatment response, building robust statistical models explain cause-effect relationships medical condition interest.thesis proposed solutions assist expert-driven subpopulation discovery high-dimensional timestamped medical data.\r\nSection 10.1 reviews whether addressed core research question three challenges presented Chapter 1.\r\nFinally, discuss directions future work  Section 10.2.","code":""},{"path":"conclusion.html","id":"conclusion-summary","chapter":"10 Summary and Future Work","heading":"10.1 Summary","text":"solutions subpopulation discovery Part tackled challenge:GOAL1: Comprehensibility distinctiveness subpopulationsWe dealt challenge proposing three workflows presented Chapters 3, 4,  5.Chapter 3 presented workflow interactive application subpopulation discovery cohort data.\r\nleveraging classification rules, workflow enables building self-explaining concise descriptions subpopulations distributions regarding target variable.\r\napplication Interactive Medical Miner allows interactive expert-driven subpopulation discovery features drill-derived models explore subpopulations worthy investigation.\r\nproof concept, validated workflow SHIP dataset, focusing hepatic steatosis (“fatty liver”) target variable.\r\nconfirmed variables value ranges shown associated target variable, including obesity, age, sex, high serum concentrations liver enzyme Gamma-glutamyltransferase (GGT), genetic markers.Chapter 4 extended workflow tackling problem redundancy large rule sets, introducing algorithm extracts small number representative classification rules.\r\n-called proxy rules minimize instance overlap across rule groups, thus covering different subpopulations.\r\ndemonstrated algorithm delivers distinct rules compared baselines SHIP data samples hepatic steatosis goiter target variables, respectively.\r\nBesides variables found Chapter 3, conditions proxy rules involve hypertension low physical health hepatic steatosis, high intima-media thickness angiotensin II receptor blocker intake goiter.solutions Chapters 3  4 assume availability target variable, many medical applications unknown costly obtain.\r\nHence, Chapter 5 proposed workflow unsupervised subpopulation discovery, visualization, interactive exploration absence ground truth.\r\nworkflow () exploits clustering algorithm automatically determines appropriate number clusters, (ii) provides visualization techniques show essential characteristics high-dimensional clusters compactly, (iii) serves medical experts web application investigate juxtapose found subpopulations , including treatment effect indicators.\r\nshowcased workflow’s efficacy identifying juxtaposing four distinct tinnitus patient phenotypes CHA dataset.aforementioned solutions designed primarily static data, Part II thesis addressed exploitation temporal information medical data, translated challenge:GOAL2: Exploitation timeWe presented solutions challenge Chapters 6  7.Chapter 6 proposed framework extract evolution features timestamped medical data, .e., tiny streams five time points months years apart (often case population studies).\r\nchange descriptors quantify study participants’ change time.\r\nshown augmenting original feature space evolution features improves classification performance.\r\nUsing SHIP data sample,\r\nshown somatic changes changes cluster quality indices time associated hepatic steatosis.Chapter 7 focused data many recordings short time period proposes clustering approach build representations new similarity measures applicable raw multivariate (sensor) timeseries.\r\nvalidated approach identifying plantar pressure patterns patients diabetic foot syndrome healthy volunteers.Part III presented solutions models interpreted intrinsically.\r\nmedical decisions serious consequences, accurate models increasingly preferred research suggest higher degree confidence output.\r\nHowever, post-modeling methods needed translate findings black-box models expert-understandable form, leading challenge:GOAL3: Post-hoc interpretation complex black-box modelsChapter 8 proposed end--end data analysis workflow steps data augmentation, modeling, interleaving model training feature elimination, post-hoc analysis trained models.\r\nworkflow yields (type model) statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance.\r\nvalidated workflow three modeling tasks: () tinnitus-related distress treatment tinnitus patients, (ii) depression baseline tinnitus patients, (iii) rupture risk intracranial aneurysms.Finally, Chapter 9 presented solution examine pre-defined subpopulations differ concerning predictive characteristics.\r\nderived post-hoc interpretation measure assess differences predictors’ associations two subpopulations.\r\nvalidated solution finding gender sex differences (subpopulations female male patients) target variables tinnitus-related distress, depression, liver fat concentration.","code":""},{"path":"conclusion.html","id":"conclusion-future-work","chapter":"10 Summary and Future Work","heading":"10.2 Future Work","text":"Causal subpopulation discovery.\r\nmachine learning solutions can initiate generation new hypotheses,\r\nlimited discovering mere associations variables, cause-effect relationships.\r\nexample, shown depression strongly correlates distress tinnitus patients (recall Chapters 5  8).\r\ndirection relationship inferred – tinnitus distress cause depression vice versa?\r\ncan correlation instead explained confounder, .e., “lurking” third variable, low physical health socioeconomic status?\r\nPearl Mackenzie [299] give example Simpson’s paradox, phenomenon traditional correlation analysis even contradicts true cause-effect relationships.\r\nconsidering pairwise relationships, data suggested people exercise increased cholesterol levels.\r\nHowever, relationship masked age.\r\nOlder participants exhibited higher activity levels study population.\r\nage-stratified analysis, became evident exercise leads decrease cholesterol levels.causal inference, relationships variables modeled directed acyclic graph (DAG) nodes represent variables edges direction causality [299], [301].\r\nSchölkopf [300] described potential links machine learning causal inference.\r\nYu et al. [302] reviewed causal feature selection methods, assume optimal feature subset target variable equal Markov boundary.\r\nMarkov boundary comprises nodes outgoing (parents) incoming (children) edges target parents child nodes (spouses) DAG (Figure 10.1).\r\nassumption conditioning target variable’s Markov boundary, variables DAG become independent target [302].\r\nHence, possible direction future work investigate potential combining methods causal inference, example, leveraging causal feature selection predictive modeling.\r\n, interested determining whether features Markov boundary sufficient predict response causal approaches compare non-causal counterparts regarding classification performance.\r\nFigure 10.1: Illustrative example directed acyclic graph Markov boundary target X. labeled variables P-W constitute Markov boundary X, consisting parents, children additional parents children (“spouses”).\r\nDomain adaption.\r\ntime , findings datasets investigated thesis replicated datasets.\r\nexample, expect models evolution feature framework (Chapter 6) built SHIP perform poorly applied data samples population studies, MONICA [13], KORA [14], Rotterdam Study [15].\r\nDomain adaption deals transferring knowledge source target domain.\r\nLemberger Panico [303] summarized four major challenges domain adaption:Prior shift refers difference source () target (B) domain target variable’s distribution. example, situation, liver fat concentration (average) higher B. due differences sampling, e.g., , higher percentage subjects pre-existing conditions drawn.Covariate shift refers difference predictors’ distributions B, relationships predictors target variable assumed . example, subjects older subjects B, time, high body mass index predictive domains.Concept shift refers difference relationship predictors target variable. example, , smoking observed subjects increased liver fat concentration, smokers B generally exhibit lower liver fat values non-smokers.Subspace mapping refers situation feature sets B different. example, hepatic steatosis determined ultrasound MRT B. Besides, different questionnaires used B assess life quality, neither questionnaires used B vice versa.Future efforts include leveraging methods domain adaption, example, transfer tinnitus phenotypes Chapter 5 detected CHA patient data tinnitus centers different baseline characteristics.Parsimonious cost-aware learning. methods disregard fact features always available free.\r\nFeature acquisition medical research associated certain costs, divided financial expense health burden patient undergoing potentially painful invasive examination [271].\r\n\r\nexample, biopsy usually higher diagnostic value simple blood test also expensive physically mentally stressful patient.\r\nYu et al. [270] perform feature selection budget balance model performance feature acquisition cost.\r\nConsequently, models favor cheaper features like demographics questionnaire responses costly ones requiring expensive strenuous physical examinations laboratory tests.\r\nRecall Chapters 3  4, somatic ultrasound variables appeared classification rules describing subpopulations increased liver fat concentration.\r\nmethods distinguish variables simple body measurements (e.g., obtain participant’s waist circumference body weight) elaborate imaging procedures.\r\nFuture research includes implementing mechanisms enable cost-aware learning.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"appx-pheno.html","id":"appx-pheno","chapter":"A Overview of Variables Selected for Phenotyping","heading":"A Overview of Variables Selected for Phenotyping","text":"64 variables selected phenotyping Chapter 5, phenotype-stratified distributions brief description shown .\r\nHalf-violin/half-boxplot geometries represent numerical variables;\r\nbar graphs represent categorical variables.\r\norder variables analogous clockwise arrangement Figures 5.2  5.3.\r\nVariables asterisk end name reversed ensure higher scores consistently represent higher health burden across variables.prefix variable’s name denotes respective questionnaire:\r\nACSA: Anamnestic Comparative Self-Assessment [55];\r\nADSL: General Depression Scale [56], [57];\r\nBI: Berlin Complaint Inventory [58];\r\nBSF: Berlin Mood Questionnaire [59];\r\nISR: ICD-10 Symptom Rating [60];\r\nPHQK: (Short-form) Patient Health Questionnaire [61];\r\nPSQ: Perceived Stress Questionnaire [62];\r\nSES: Pain Perception Scale [63];\r\nSF8: Short Form 8 Health Survey [64];\r\nSOZK: socio-demographics questionnaire [65];\r\nSWOP: Self-Efficacy- Optimism-Pessimism Scale questionnaire [66];\r\nTINSKAL: Visual analog scales;\r\nTLQ: Tinnitus Localization Quality questionnaire [67];\r\nTQ: Tinnitus Questionnaire (German version) [68].","code":""}]

[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"preview current status Uli‚Äôs thesis.Use navigation bar top download complete manuscript PDF Word document.","code":""},{"path":"index.html","id":"outline-list-of-chapters","chapter":"Preface","heading":"Outline / List of Chapters","text":"1 üü¢ Introduction\r\n2 üîµ Medical BackgroundPART SUBPOPULATION DISCOVERY HIGH-DIMENSIONAL DATA3 üîµ Interactive Discovery Inspection Subpopulations\r\n4 üü¢ Identification Distinct Subpopulations\r\n5 üü¢ Visual Identification Informative FeaturesPART II EXPLOITING DYNAMICS6 üîµ Extraction Evolution Features Cohort Data\r\n7 üî¥ Extraction Features Short Temporal SequencesPART III POST-MINING INTERPRETATION8 üü¢ Post-hoc Interpretation Classification Models\r\n9 üî¥ Subpopulation-specific Learning Post-hoc Model InterpretationPART IV SUMMARY10 üî¥ Conclusion Future WorkLegend:\r\nüèÅüèÅ = submission ready\r\nüèÅ = feedback reviewers incorporated\r\nüü¢ = draft ready\r\nüîµ = maturing\r\nüî¥ = unwritten","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"motivation-and-objectives","chapter":"1 Introduction","heading":"1.1 Motivation and Objectives","text":"Common objectives data analysis medical research include () identifying long-term determinants protective factors medical condition interest, (ii) discovering subpopulations increased outcome prevalence, (iii) generating robust statistical models can explain relationships one independent variables target variable.\r\nexample, epidemiologists attempt discover associations multiple risk factors outcome cohort studies collecting data include extensive information participants obtained questionnaires, medical examinations, laboratory analyses, imaging.\r\nOften data collected repeatedly time, example longitudinal studies.\r\nmeans latent often overlooked temporal information data, investigation can potentially lead new insights.find associations variables, medical researchers usually first carefully derive hypotheses clinical practice, experimental studies, extensive literature reviews test formally statistical significance.\r\nHowever, ever-increasing volume heterogeneity medical data, traditional hypothesis-driven workflows becoming increasingly impractical, reason important inherent associations variables may go undetected.\r\nMachine learning can improve medical research discovering understandable descriptions patient study participant subpopulations similar outcome, thus can used derive new hypotheses.proliferation medical machine learning applications triggered several reasons, desire make automated use plethora information collected study subjects, sometimes just based ubiquity deep learning success stories media.\r\nHowever, ease creating complex data-driven models guarantee insights can effortlessly derived.\r\nstate---art machine learning algorithms deep neural networks gradient boosting machines generate -called black-box models multiple layers complexity involve many multivariate, nonlinear interactions variables difficult represent intuitively.\r\ncritical application expert, practitioner scientist working clinical epidemiological setting, equipped tools understand, explore, visualize models can drill specific individual patterns gain actionable insights ultimately contribute prevention, diagnosis, treatment clinical practice.\r\nmedical data come wide variety sources key characteristics collected datasets vary, requiring adaptation methods specifics application scenario.goal work develop methods serve intelligent assistance medical researchers analysis high-dimensional, temporal medical data.\r\nHence, core research question thesis : derive accurate yet understandable patterns subpopulation discovery high-dimensional temporal medical data?\r\n, , generation machine learning models, several challenges must overcome order domain expert able derive actionable knowledge.\r\nchallenges can translated following four requirements.\r\n(1) Comprehensibility patterns: extracted models, including clusters, rules, patterns, must made understandable; preferably, model generation process also comprehensible.\r\n(2) Exploitation time: latent temporal information must exploited, satisfying requirement 1.\r\n\r\n(3) Minimization redundancy: redundancy must minimized, satisfying requirement (1).\r\nPatterns may overlap terms topics cover.\r\nleads redundancy patterns, negative impact perceived quality model.\r\ntask extract, process display relevant (temporal) patterns expert-driven model exploration.","code":""},{"path":"intro.html","id":"structure-and-contributions-of-this-thesis","chapter":"1 Introduction","heading":"1.2 Structure and Contributions of This Thesis","text":"thesis presents solutions support medical researchers data-driven analysis high-dimensional, temporal medical data.\r\nDesign decisions developments partly inspired suggestions respective domain experts cooperation partners, including specialist internal medicine, epidemiologist statistical expertise, diabetes expert three tinnitus specialists.\r\nthesis organized three parts ten chapters tackling aforementioned research question challenges.\r\nPART covers methods discovering subpopulations high-dimensional data.\r\nPART II focuses specifically temporal aspects medical datasets provides approaches extract informative representations latent temporal data.\r\nPART III addresses post-hoc analysis machine learning models includes solutions derive model-, observation-, subpopulation-level insights otherwise ‚Äúopaque‚Äù black boxes.Chapter¬†2 (Medical Background) presents relevant medical background information, brief comparison medical study types overview case studies solutions tailored .Chapter¬†3 (Interactive Discovery Inspection Subpopulations) presents workflow TODOChapter¬†4 (Identifying Distinct Subpopulations) examines redundancy large rule sets describing subpopulations. present workflow extracts smaller number representative rules. rules selected avoid instance overlap much possible, thus covering different concepts data space. evaluate workflow two samples longitudinal cohort study two target variables, respectively.Chapter¬†5 (Visual Identification Informative Features) describes approach identify distinct tinnitus phenotypes parameter-free clustering, presents novel visualizations juxtapose phenotypes high-dimensional feature space explore phenotype-specific characteristics.Chapter¬†6 (Constructing Evolution Features Capture Study Participant Change Time) present framework cohort analysis longitudinal cohort studies constructs ‚Äúevolution features‚Äù latent temporal information describing change cohort participants time. show exploiting novel features improves generalization performance classification models report results longitudinal cohort study.Chapter¬†7 (Feature Extraction Short Temporal Sequences Clustering) present approach build representations short temporal sequences via clustering example pressure- posture-dependent plantar temperature pressure patients diabetic foot syndrome.Chapter¬†8 (Post-Hoc Interpretation Classification Models) focuses making already learned, complex classification models understandable domain experts. provide workflow combines classification high-dimensional medical data model explanation using post-hoc interpretation methods.\r\nend, use Shapely value explanations (SHAP), LASSO coefficients, partial dependency graphs.\r\napproach delivers statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance, help illuminate complex black-box machine learning models.\r\nreport results three applications: () tinnitus-related distress tinnitus patients, (ii) depressivity tinnitus patients, (iii) rupture risk intracranial aneurysms.Chapter¬†9 (Subpopulation-Specific Learning Post-Hoc Model Interpretation) describes approach examines subpopulations differ respect predictive characteristics temporal data. , derive post-hoc interpretation measure assess difference association predictors two subpopulations. report results CHA gender differences (subpopulations female male patients) two outcomes tinnitus-related distress depression, effect treatment outcomes.Chapter¬†10 (Conclusion Future Work) concludes thesis giving summary contributions detailed perspectives presented work.","code":""},{"path":"background.html","id":"background","chapter":"2 Medical Background & Datasets","heading":"2 Medical Background & Datasets","text":"OutlineTypes medical studiesShort introduction medical applications","code":""},{"path":"background.html","id":"fundamental-terms-and-study-types","chapter":"2 Medical Background & Datasets","heading":"2.1 Fundamental terms and study types","text":"Typical goals medical research include identifying long-term determinants protective factors outcome interest, discovering sub-populations increased disease prevalence, studying intervention effects generating statistical (causal) models explain cause-effect relationships.Traditional data analysis pipelines usually structured follows:medical scholar formulates hypothesis based observations clinical practice current research. Examples: risk factor like alcohol abuse affects prevalence certain outcome? effect novel therapy patients depressive symptoms?investigates hypothesis, small set relevant variables chosen, might controlled confounders.\r\nAdequate data collected.\r\nVariable selection may incorporate controlling confounders.strength associations selected variables outcome assessed regression models statistical methods.Based results, inferential statistics computed conclusions drawn may foster implementation novel preventive measures application suited treatment forms high-risk patients.Primary medical research can classified basic, clinical epidemiological studies.","code":""},{"path":"background.html","id":"basic-research","chapter":"2 Medical Background & Datasets","heading":"2.2 Basic research","text":"Basic medical research (experimental research) aims improve understanding cellular, molecular physiological mechanisms human health diseases conducting cellular molecular investigations, animal experiments, studies drug material properties strictly controlled laboratory environments [1].\r\ninvestigate (causal) effects one variables interest outcome, variables usually kept constant variables interest varied.\r\ncarefully standardized experimental conditions basic medical studies ensure high internal validity, conditions often easily transferred clinical practice without loss generalizability results.!!! Basic research also includes development improvement analytical (e.g., analytical determination enzymes, markers, genes) imaging measurement procedures (e.g., computer tomography, magnetic resonance tomography) well gene sequencing (e.g., relationship eye color specific gene sequence), development biometric procedures statistical test procedures, modeling statistical evaluation strategies.brief overview important medical study types given (see also [1], [2].","code":""},{"path":"background.html","id":"clinical-studies","chapter":"2 Medical Background & Datasets","heading":"2.3 Clinical studies","text":"Clinical studies generally classified interventional (also: experimental) studies non-interventional (also: observational) studies.general goal interventional study compare different treatments within patient population whose members differ little possible, except treatment branch.\r\ncommon example pharmaceutical study aims validate efficacy harmlessness drug investigating establishing main adverse effects, resorption, metabolization excretion drug.\r\nSelection bias can avoided appropriate measures, particular randomly assigning patients groups.treatment can drug, surgical procedure therapeutic use medical product (e.g.¬†stent), also physiotherapy, acupuncture, psychosocial intervention, rehabilitation, training diet.randomized controlled trial (RCT) considered study design ‚Äúgold-standard‚Äù minimizes selection bias . randomly allocating patients treatment control group, b.ensuring equal distribution known unknown influencing variables (confounders), risk factors, comorbidities genetic variabilities.\r\nThus, RCTs suitable obtaining unambiguous answer clear question proving causality.contrast, non-interventional clinical studies patient-related observational studies patients receive individually defined therapy (patients receive exact therapy).\r\nData analysis often takes place retrospectively.\r\nObservational studies often used generate hypotheses.\r\n- example: study investigates regular use drugs therapies\r\ntreatment, including diagnosis monitoring, follow pre-defined trial protocol, medical practice","code":""},{"path":"background.html","id":"prospective","chapter":"2 Medical Background & Datasets","heading":"2.3.0.1 prospective","text":"Prospective studies can recognized chronological sequence hypothesis generation data collection.\r\nFirst, hypotheses tested determined, example respect new treatment procedure.\r\n, data collected specifically hypothesis testing.\r\nformulating testable hypotheses first, can ensured research questions can actually answered measured data.\r\n","code":""},{"path":"background.html","id":"retrospective","chapter":"2 Medical Background & Datasets","heading":"2.3.0.2 retrospective","text":"data collection already taken place start studyexamples:digitalized medical databases","code":""},{"path":"background.html","id":"epidemiological-studies","chapter":"2 Medical Background & Datasets","heading":"2.4 Epidemiological studies","text":"TODO: Siehe BuchEpidemiological studies usually interested distribution temporal change frequency diseases causes general population sub-populations.\r\nAnalogous clinical studies, epidemiology also distinguishes experimental observational studies. ???cross-sectional study (prevalence study) conducted whereas longitudinal study, study carried several points time results individual waves investigation compared.\r\nLongitudinal studies categorized trend panel design.\r\ntrend study, wave can involve different participant sample, .e., individual participant followed time.\r\ncontrast, panel study investigates population multiple points time allows also measure intra-individual temporal changes.","code":""},{"path":"background.html","id":"controlled-vs.-uncontrolled","chapter":"2 Medical Background & Datasets","heading":"2.4.0.1 controlled vs.¬†uncontrolled","text":"epidemiological vs clinical vs experimental studywhat cohortsamples (clinical samples, medical research samples) -> validity?","code":""},{"path":"background.html","id":"challenges","chapter":"2 Medical Background & Datasets","heading":"2.5 Challenges","text":"heterogeneity (numeric, categorical)pre-processing raw image data (non-tabular data)different requirements different studies","code":""},{"path":"background.html","id":"medical-applications","chapter":"2 Medical Background & Datasets","heading":"2.6 Medical Applications","text":"","code":""},{"path":"background.html","id":"ship","chapter":"2 Medical Background & Datasets","heading":"2.6.1 The Study of Health in Pomerania (SHIP)","text":"reunification Germany, found life expectancy considerably lower east west [3].\r\nFurthermore, regional differences within former East Germany, lowest life expectancy northeast [3], [4].\r\ninvestigate causal relationship high mortality northeastern German population risk factors, research center community medicine established Study Health Pomerania (SHIP) [5], longitudinal epidemiological study two independent cohorts northeast Germany.\r\nSHIP attempts describe wide spectrum health-related conditions rather focusing specific target disease [5].\r\nparticular, major study goals comprise investigations regarding prevalence common diseases risk factors, correlation interaction risk factors diseases, progression subclinical manifest diseases, identification subgroups elevated health risk, prediction incidental diseases, well utilization costs medical devices. TODO: shortenCohort inclusion criteria age 20 79 years, main residency study region German nationality.\r\nParticipants SHIP underwent extensive, recurring (ca. every 5-6 years) examination program encompasses personal interviews, body measurements, exercise electrocardiogram, laboratory analysis, ultrasound examinations full body magnetic resonance tomography (MRT).\r\nFigure @ref:(fig:ship-sankey-plot) illustrates participant response age distribution show-ups across study waves.\r\nBaseline examinations first cohort performed 1997 2001 (SHIP-0, N=4308).\r\nFollowup examinations done 2002-2006 (SHIP-1, n=3300), 2008-2012 (SHIP-2, n=2333), 2014-2016 (SHIP-3, n=1718) since 2019 (SHIP-4).\r\nBaseline information second, independent cohort (SHIP-Trend-0, N=4420) collected 2008 2012 followup conducted 2016 2019.\r\nMajor strengths SHIP high level quality assurance, standardized examination protocols, high cohort representativeness.\r\nFigure 2.1: Participation response age distribution show-ups across study waves SHIP. Average population age shown black vertical line histograms.\r\nexamination program changed across waves.\r\nexample, magnetic resonance imaging (MRI) conducted since SHIP-2; liver ultrasound carried SHIP-0 SHIP-2, SHIP-1; dermatological examinations conducted SHIP-1 SHIP-2, SHIP-0.analyses, focus disorder hepatic steatosis, also known fatty liver, condition characterized high accumulation fat liver present ca. 30% adults [5], [6].\r\nRisk factors include alcohol abuse, obesity, metabolic syndrome, diabetes, hyperlipidemia [7].\r\nliver biopsy considered diagnosis gold standard [7], associated moderate risk patient.\r\nNon-invasive diagnosis forms include MRT, CT ultrasound.\r\nSince hepatic steatosis mostly asymptomatic, often remains undiscovered may develop serious disease, steatohepatitis, cirrhosis, liver cell carcinoma liver failure.","code":""},{"path":"background.html","id":"the-diabetic-foot-clinical-trial-df","chapter":"2 Medical Background & Datasets","heading":"2.6.2 The Diabetic Foot Clinical Trial (DF)","text":"‚Äì>\r\n\r\n\r\nDiabetic foot syndrom umbrella term foot-related problems diabetes patients.\r\none four diabetes patients develop foot ulcer lifetime [8] many facing amputations next four years [9].\r\n85% foot amputations relate foot ulcers [10], [11].\r\nrate foot amputations among diabetic patients estimated 17‚Äì40 times higher general population [13].\r\nDFS patients predisposed peripheral sensoric neuropathy, , example, consequence patients unaware temperature feet, pressure apply .\r\nAffected individuals may even injure without noticing.\r\nExcessive plantar pressure loads may aggravate tissue destruction, increasing lifetime risk foot ulceration [15].\r\nHowever, understanding pathomechanisms underlying tissue destruction without trauma limited.Medical Faculty Otto von Guericke University Magdeburg, experimental study 31 healthy volunteers 30 diabetes patients diagnosed severe polyneuropathy conducted quantify pressure- posture-dependent changes plantar temperatures surrogate tissue perfusion.\r\npurpose, plantar pressure temperature changes feet recorded extended phases standing.\r\nCustom-made shoe insoles \r\nequipped eight temperature sensors eight pressure sensors preselected positions used data acquisition (2.2 ()).\r\ninsoles positioned closed protective shoes specifically developed diabetes patients.\r\nWithin shoes temperature increases time due exchange body temperature user also affected environmental temperature.\r\nclosely monitor -shoe temperature changes one sensor placed bottom insole without contact feet, denoted ‚Äúambient temperature sensor.‚Äù\r\nFigure 2.2: () Sensor positioning relation foot placement. (b) Infrared images healthy volunteer seated position without pressure application feet () following positioning 20 kg weight upper thighs. time-dependent decrease temperature detected predominantly forefoot, visualized yellow color pressure load. Within 1 min pressure release, rapid temperature rise detected.\r\ndata acquisition started immediately putting shoes.\r\nparticipants asked follow predefined sequence actions, alternating postures standing (stance phase) seating (pause).\r\nsessions consisted six stance episodes, lasting 5, 10, 20, 5, 10 20 minutes , separated seating episodes lasting 5 min .\r\n\r\nparticipants instructed apply pressure equally feet standing.\r\nParticipants receive immediate feedback actual pressure application sessions, however study nurses verbally encouraged keep pressure without release standing.\r\nseated position participants instructed release pressure 5 minutes, still keeping contact insole.\r\nparticipants explicitly asked adhere instructions, e.g.¬†temporarily release pressure stance episode.\r\nstudy protocol furthermore encompassed measurements performed twice, room temperature ca 22¬∞C outdoors ambient temperature ca. 16¬∞C.\r\ntwo measurements performed two independent days.termographic images Figure 2.2 (b) exemplarily visualize changes plantar temperature, healthy volunteer seated position, pressure application (1), placement 20 kg weight front upper thighs (2-6), removal additional weight (7-8).\r\npressure load, gradual temporal temperature decrease detected predominantly forefoot.\r\npressure release, rapid temperature rise within 1 min observed.","code":""},{"path":"background.html","id":"the-intracranial-aneurysm-angiography-image-dataset-iad","chapter":"2 Medical Background & Datasets","heading":"2.6.3 The Intracranial Aneurysm Angiography Image Dataset (IAD)","text":"Retrospective clinicalIntracranial aneurysms pathologic dilations intracranial vessel wall, often form dilation. \r\nbear risk rupture lead subarachnoidal hemorrhages often fatal consequences patient.\r\nSince treatment can also cause severe complications, extensive studies conducted assess patient-individual rupture risk based various parameters, including aneurysm symptomatology, size location, well patient age gender [16].\r\nstudies identified parameters, aspect ratio, undulation index nonsphericity index statistically significant respect aneurysm rupture status [17], [18].\r\nHowever, although studies allow retrospective analysis, clinician needs guidance case asymptomatic aneurysm (accidental finding) detected rupture risk determined.developed methods ‚ÄúIntracranial Aneurysm Angiography Image Dataset‚Äù (IAD) comprising 3D rotational angiography data 74 patients (age: 33-85 years, 17 male 57 female patients) university hospital Magdeburg, Germany, adding total 100 intracranial aneurysms.\r\nidentified two primary goals dataset: () build models can accurately predict rupture status based morphological parameters , (ii) assess importance parameters models optimal accuracy.Inspired results Baharoglu et al.¬†[19] found differences sidewall bifurcation aneurysms (cf.¬†Figure¬†2.3) regarding relationship several morphological parameters rupture status, learn distinct models subset sidewall aneurysms (9 (37.5%) 24 ruptured) subset bifurcation aneurysms (29 (46.8%) 62 ruptured).\r\nMoreover, run experiments combined group (43 100 ruptures) includes 14 samples clearly determined either sidewall bifurcation aneurysms.\r\nFigure 2.3: Sidewall bifurcation aneurysm. Illustration sidewall aneurysm side parent vessel wall (left) bifurcation aneurysm vessel bifurcation (right).\r\n","code":""},{"path":"background.html","id":"cha","chapter":"2 Medical Background & Datasets","heading":"2.6.4 The Tinnitus Patients Observational Therapy Study Dataset (CHA)","text":"‚Äì>\r\n\r\n ‚Äì>\r\nTinnitus perception phantom sound absence external sound source.\r\ncomplex multi-factorially caused maintained phenomenon, estimated affect 10% 15% adult population [20].\r\nassociated annual economic burden amounts US$19.4 billion \r\nUnited States [21] ‚Ç¨6.8 billion Netherlands alone [22].\r\nClinical assessment tinnitus challenging due patient heterogeneities respect perception tinnitus (laterality, pitch, sound characteristics, frequency, permanence, chronicity), risk factors (including hearing loss, temporomandibular joint disorder, aging), comorbidities (including hyperacusis, depression, sleep disorders), perceived distress, treatment response [23].\r\ndifferences make identification suitable effective form treatment difficult.\r\nCurrently, exist therapy gold standard: sound therapy (masking), informational counseling (minimal contact education), cognitive behavioral therapy tinnitus retraining found effective patients, also evidence patients benefit equally treatment forms [24]‚Äì[28].Due heterogeneous nature tinnitus symptom well unclear\r\nevidence-base treatment management, identification patient subgroups vital stratify individual pathophysiology treatment pathways [29]‚Äì[31].‚Äútinnitus patients observational therapy study dataset‚Äù (CHA) comprises self-report data 4,103 tinnitus patients treated Tinnitus Center Charit√© University Medicine Berlin January 2011 October 2015.\r\npatients 18 years age older suffering tinnitus least 3 months.\r\nExclusion criteria presence acute psychotic illness addiction disorder, deafness insufficient knowledge German language.\r\nTreatment comprised multimodal 7-day program included intensive daily informational counseling, detailed ear-nose-throat well psychological diagnostics, cognitive behavior therapy interventions, hearing exercises, progressive muscle relaxation physiotherapy.\r\nbaseline (T0; therapy commencement) treatment (T1), patients asked complete multiple self-report questionnaires.\r\nquestionnaires selected obtain comprehensive tinnitus assessment, including tinnitus-related distress psychosomatic background tinnitus anxiety, depression, general quality life experienced physical impairments.\r\nTable¬†2.1 provides overview questionnaires used analysis.\r\nquestionnaires contain multiple-choice items answers Likert scale. example, TQ contains 52 statements, ‚Äúunable enjoy listening music noises.‚Äù respondents can give 3 possible answers: ‚Äútrue‚Äù (encoded 0), ‚Äúpartly true‚Äù (1) ‚Äútrue‚Äù (2).\r\nquestionnaires also comprise aggregated variables, called ‚Äúsubscales‚Äù ‚Äútotal scores.‚Äù\r\nexample, TQ total score (TQ_distress) calculated sum 40 item values, 2 items used twice [34], yielding value range 0 84, higher values represent higher tinnitus-related distress.\r\ncutoff value 46 [34] used distinguish compensated (0-46) decompensated (47-84) tinnitus.\r\n, questionnaire, average time answer item recorded. Figure @ref:(fig:02-cha-patient-demographics-plot) provides graphical illustration demographics 3,803 (92.7%) patients complete data SOZ questionnaire TQ score.Table 2.1: Description questionnaires form basis CHA.\r\nFigure 2.4: Patient demographics (CHA). Overview patient demographics degree tinnitus distress measured therapy commencement.\r\n‚Äì>\r\n ‚Äì>\r\n ‚Äì>\r\n ‚Äì>","code":""},{"path":"imm.html","id":"imm","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3 Interactive Discovery and Inspection of Subpopulations","text":"chapter partly based :Uli Niemann, Henry V√∂lzke, Jens-Peter K√ºhn, Myra Spiliopoulou. ‚ÄúLearning inspecting classification rules longitudinal epidemiological data identify predictive features hepatic steatosis.‚Äù : Expert Systems Applications 41.11 (2014), pp.¬†5405-5415. DOI: 10.1016/j.eswa.2014.02.040.","code":""},{"path":"imm.html","id":"brief-chapter-summary","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"Brief Chapter Summary","text":"study separation two positive negative outcome disease epidemiological data.\r\ngoal identify compact subpopulations pure possible respect target variable.","code":""},{"path":"imm.html","id":"motivation-and-comparison-to-related-work","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1 Motivation and Comparison to Related Work","text":"Medical decisions diagnosis treatment multifactorial conditions diseases disorders based clinical epidemiological studies; latter accommodate information participants without condition allow learning discriminative models , longitudinal design, understanding progress condition.\r\nexample, several studies identified risk factors (like obesity alcohol consumption) co-morbidities (like cardiovascular diseases) associated hepatic steatosis (‚Äúfatty liver‚Äù) [63]‚Äì[67].\r\nHowever, studies identified risk factors associated outcomes pertain whole population.\r\nwork originated necessity identify factors outcomes subpopulations thus promote personalized diagnosis treatment, expected personalized medicine [68], [69].Classification subpopulations studied Zhanga Kodell [70] pointed classifier performance whole dataset can low complete population heterogeneous.\r\nTherefore, first trained ensemble classifiers, used predictions ensemble member create new feature space.\r\nperformed hierarchical clustering partitioned instances three subpopulations: one prediction accuracy high, one intermediate one low.\r\napproach, Zhanga Kodell split original dataset subpopulations easy difficult classify.\r\nmethod seems appealing general, appeared unsuitable three-class problem SHIP data exhibits skewed distribution, hence clear low classification accuracy (partially) caused skew.\r\nHence, studied dataset exploratively classification, identify less skewed subpopulations, exploratively classification, identify - inside subpopulation - variables highly associated outcome.Pinheiro et al.¬†performed association rule discovery patients liver carcinoma [71].\r\nauthors pointed early detection liver cancer may help reducing five-year mortality rate, early detection difficult, onset liver carcinoma, patient often observes symptoms [71].\r\nPinheiro et al.¬†leveraged association rule algorithm FP-growth [72] discover high-confidence association rules high-confidence classification rules regarding mortality liver cancer patients.\r\nalso considered association rules promising analysis medical data, easy compute deliver results understandable humans.\r\nTherefore, also used association rules baseline method, though epidemiological data classification rather mortality prediction.\r\nuse association rules classification, specified rule consequent target variable.\r\nZhang et al. [73] addressed increasing technical challenges medical expert-driven subpopulation discovery due increasingly large complex medical data often comprise information hundrets variables thousands patients form tables, images text.\r\nWhereas past sufficient physician working knowledge basic statistics spreadsheet software like Microsoft Excel analyze small table patient data, nowadays effective efficient approaches required management, analysis summarization large medical data.\r\nresult, domain experts usually rely technical experts help perform tasks.\r\nHowever, back forth process often slow, tedious expensive.\r\nHence, better equip domain expert technical tool allows quickly perform exploratory analyses .\r\nZhang et al. [73] presented CAVA, system incorporates miscellaneous subgroup visualizations (called ‚ÄúViews‚Äù) analytic components (called ‚ÄúAnalytics‚Äù) subgroup comparison.\r\nmain panel Figure 3.1 shows one Views: flow diagram [74] patient subgroups sequence symptoms.\r\nUsers can obtain additional summaries interacting visualization, e.g.¬†placing one boxes flow diagram onto one entries Analytics panel via drag--drop.\r\n, user can expand selected cohort letting tool search patients strictly match current inclusion criteria somewhat similar current patient subpopulation [75].\r\nFigure 3.1: CAVA‚Äôs graphical user interface. flow chart visualizes cardiac patient subgroups organized shared symptom occurrence. Color represents hospitalization risk. user can switch graphical representations data processing methods via drag--drop operations. top-right panel provides detailed information currently selected patients. bottom-right panel contains provenance graph allows user undo operations revisit previous interaction steps. (: [73])\r\nKrause et al. [76] argued model selection based solely global performance metrics like accuracy, statistics contribute better understanding model‚Äôs reasoning.\r\nFurthermore, complex yet accurate model automatically warrant actionable insights.\r\nKrause et al.¬†propose Prospector [76], system provides diagnostic components complex classification models based concepts partial dependence (PD) plots [81].\r\nPD plots popular tool visualize marginal effect feature predicted outcome probability.\r\nBriefly, point PD curve represents average prediction model observations, given observations fixed value feature interest.\r\nfeature whose PD curve high range variability considered impactful model prediction feature flat PD curve.\r\nClosely related PD plots individual conditional expectation (ICE) plots [82], display one curve every observation, thus help uncover contrasting subpopulations might ‚Äúaverage ‚Äù PD plot.\r\nProspector combines PD ICE curves depict relationship feature model prediction (global) model level (local) patient-individual level.\r\n, color bar provided compact alternative ICE curves (Figure 3.2) ().\r\nstacked bar chart shows distribution predicted risk scores study group (Figure 3.2 (b)), user can click specific decile get list individual patients exact prediciton score label.\r\nway, patients whose prediction score close decision boundary can investigated.\r\nauthors calculate feature ‚Äúimpactful feature change‚Äù: given actual feature value patient, identify near counterfactual value led large change predicted risk score, minimizing difference original feature value maximizing predicted risk score.\r\ntop-5 called ‚Äúsuggested changes‚Äù displayed ‚Äì separately inreasing decreasing disease risk ‚Äì table (cf.¬†Figure 3.2 (c)), incorporated interactive elements IC color bars (cf.¬†Figure 3.2 (d)).\r\nFigure 3.2: selection Prospector‚Äôs model diagnostics. () upper chart depicts two curves feature ‚Äúage‚Äù: gray partial dependence (PD) curve represents marginal prediction model patients whereas black individual conditional expectation (ICE) curve illustrates effect counterfactual age values predicted diabetes risk example patient. histogram shows age distribution. color bar placed beneath compact depiction ICE curve ; encircled value represents feature value selected patient. (b) Stacked bars show distribution predicted risk scores study group. Clicking one bars opens table shows ID, predicted risk true label patients belonging selected prediction risk decile. (c) Summary table ‚Äúimpactful feature changes‚Äù decreasing (upper group) increasing (lower group) predicted risk: row shows actual feature value ‚Äúsuggested change,‚Äù .e., similar counterfactual value led considerable change predicted risk. (d) Multiple PD color bars augmented suggested changes (white encircled labels). (adapted : [76])\r\nPahins et al. [83] presented COVIZ, system cohort construction large spatiotemporal datasets.\r\nCOVIZ comprises mechanisms explorative data analysis treatment pathways event trajectories, visual cohort comparison visual querying.\r\nOne COVIZ‚Äô design goals fast, e.g.¬†using efficient data structures like Quantile Data Structure [84] ensure low latency computational operations hence suitablity large datasets.Bernard et al. [85] proposed system cohort construction temporal prostate cancer cohort data comprised visualizations subgroups individual patients.\r\nguide users exploration, visual markers indicate interesting relationships attributes derived statistical tests.","code":""},{"path":"imm.html","id":"other-related-work","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1.1 Other related work","text":"IIComPath: [86]","code":""},{"path":"imm.html","id":"previous-work-on-ship","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.1.2 Previous Work on SHIP","text":"Klemm et al. [87] presented ‚Äú3D regression cube‚Äù system enables interactive exploration feature correlations epidemiological datasets.\r\nsystem generates large number multiple regression models various combinations one dependent two independent variables displays goodness fit three-dimensional heatmap.\r\nsystem allows user modify regression equation, example, changing number independent variables, specifying interaction terms fixing one variables reduce computational complexity specifically focus variable interest.\r\napproach also capable identifying variables highly associated outcome, search subpopulation-specific relationships instead generating global model whole dataset, provide predictive value ranges.Klemm et al. [90] presented system combines visual representations non-image image data.\r\nidentify clusters backpain patients SHIP data.\r\nfixed hepatic steatosis outcome, rather opted build supervised models classification rules directly captured relationships predictive variables outcome.Alemzadeh et al. [91] presented S-ADVIsED, system interactive exploration subspace clusters, incorporating various visualization types, donut charts, correlation heatmaps, scatterplot matrices, mosaic charts error bar graphs.\r\nS-ADVIsED requires user input mining results obtained advance outside system, tool allows expert-driven interactive subpopulation discovery.Hielscher et al. [94] developed semi-supervised constrained-based subspace clustering algorithm find diverse sets interesting feature subsets using SHIP data.\r\nguide search interesting feature subsets, expert can provide domain knowledge form instance-level constraints, thus forcing pairs instances assigned either different cluster.\r\nHielscher et al. [95] extended work introduced mechanism compare subpopulations independent cohorts.consequence, thesame group developed constrained-based technique, theclustering guided small set constraints, given expert[HNP*18]. example, pairs participants diagnosedwith fatty liver, expert specifies participants must inthe clusters, whereas pairs participants forcedto different clusters since one pair diagnosed withthe disorder . semi-supervised subspaceclustering turned yield relevant results epidemiologists[HNP*18].TODO: Preim: Visual analytics image-centric cohort studies epidemiology","code":""},{"path":"imm.html","id":"subpopulation-discovery-workflow-and-interactive-mining-assistant","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2 Subpopulation Discovery Workflow and Interactive Mining Assistant","text":"subpopulation discovery workflow presented hereafter.\r\ndataset used population partitioning class sepration target variable hepatic steatosis come Study Health Pomerania (SHIP) described Subsection 2.6.1.subsection 3.2.1, explains origin availability target variable.\r\nsubsection 3.2.2, motivation partitioning data partitioning steps presented.\r\n, used methods class separation whole dataset partitions discussed Subsection 3.2.3.","code":""},{"path":"imm.html","id":"imm-target","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.1 Target variable","text":"target variable derived participant‚Äôs liver fat concentration computed magnetic resonance imaging (MRI).\r\ntime writing original manuscript, MRI results available 578 SHIP-2 participants.\r\nuse data participants classifier learning, interactive ruleInspector (cf.¬†section ???) also juxtaposes data data remaining 1755 participants, MRI recordings made available.Participants liver fat concentration 10% less mapped class (‚Äúnegative‚Äù class, .e., absence disorder), values greater 10% lower 25% mapped class B (increased liver fat / fatty liver tendency), values greater 25% mapped class C (high liver fat).\r\nconsider classes B C ‚Äúpositive.‚Äù\r\nAlthough cut-value 10% higher value 5% suggested [96] separation subjects without hepatic steatosis.\r\nHowever, primary interest medical perspective identification important variables individuals likely ill.¬†\r\nselection high cut-value exacerbated class imbalance made data analysis challenging. Figure 3.3 depicts class distribution gender.\r\n578 participants, 438 belong class (ca. 76%), 108 B (ca. 19%) 32 C (ca. 6%).\r\nMen likely exhibit increased high liver fat women (30.7% vs.¬†18.8% classes B C).\r\nFigure 3.3: Gender-specific distribution target variable.\r\nNext target variable, dataset contains 66 variables extracted participants‚Äô questionnaire answers medical tests (cf. [100]).\r\nvariables sociodemographics (gender, age, etc.),\r\nvariables consumption behaviour (e.g.¬†alcohol cigarettes), SNPs (genetic information), variables extracted laboratory data (e.g.¬†sera concentrations), two variables results liver ultrasound ‚Äì stea_s2 stea_alt75_s2.\r\nvariables take symbolic values reflect likelihood participant fatty liver; latter combination former ALAT recording participant; details (cf. [100]).\r\nAlmost variables mentioned hereafter suffix _s2 indicates measurements SHIP-2 followup, opposed SHIP-0 (_s0) SHIP-1 (_s1).\r\nExceptions gender, highest school degree 10 SNP variables.","code":""},{"path":"imm.html","id":"imm-partitioning","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.2 Partitioning the Dataset into Subpopulations","text":"Since dataset imbalanced respect gender (314 women, 264 men), decided partition dataset classification.\r\nFirst, investigated class distributions two partitions gender.\r\nobserved distributions different, notably respect class B (cf.¬†Figure 3.3.\r\nsecond step, studied class distribution gender age, whereupon detected age associated PartitionF PartitionM.\r\nThird, identified cut-point age introducing heuristic identifies age value minimizes standard deviation respect target variable.\r\nAfterwards, performed supervised learning separately partitions male female participants.\r\nFurthermore, built additional learner subpopulation older female participants aged cut-point 52.understand age affects class distribution, introduced heuristic determines cutoff age value partitionF splits two bins, standard deviations liver fat concentration bin minimized.\r\nLet \\(\\mathsf{splitAge}\\) denote cutoff value \\(X_y=\\{x\\\\mathsf{PartitionF}|\\text{age } x \\leq \\mathsf{splitAge}\\}\\), \\(X_z=\\{x\\\\mathsf{PartitionF}|\\text{age } x > \\mathsf{splitAge}\\}\\) denote bins.\r\n, let \\(n\\) cardinality \\(X_y\\cup{}X_z\\) .e.¬†PartitionF.\r\n, define Sum weighted Standard Deviations (\\(SwSD\\)) \\[\\begin{equation}\r\nSwSD\\left(X_y,X_z\\right) = \\frac{|X_y|}{n}\\sigma({X_y})+\\frac{|X_z|}{n}\\sigma({X_z})\r\n\\tag{3.1}\r\n\\end{equation}\\]\\(|X_i|\\) cardinality \\(X_i\\) \\(\\sigma(X_i)\\) standard deviation original liver fat values.\r\nheuristic selects \\(\\mathsf{splitAge}\\) \\(SwSD\\) minimal.\r\n\\(\\mathsf{PartitionF}\\), minimum value 7.44 age 52,\r\n.e.¬†close onset menopause.\r\nFigure 3.4: Distribution liver fat concentration male participants, female younger older 52 years. horizontal axis shows liver fat concentration bins 5%, vertical axis shows number participants bin.\r\nhistograms Figure 3.4 depict differences liver fat concentration distributions age cutoff value 52.\r\nNext \\(\\mathsf{PartitionM}\\) (n=264), show subpartitions \\(\\mathsf{F:age\\leq{}52}\\) (n=131) \\(\\mathsf{F:age>52}\\) (n=183) \\(\\mathsf{PartitionF}\\).\r\nfemale participants \\(\\mathsf{F:age\\leq{}52}\\) 5% liver fat concentration ca. 95% 10%, .e., belong negative class .\r\ncontrast, ca. 28% participants \\(\\mathsf{F:age>52}\\) liver fat concentration 10%; belong positive classes B C.classification cohort participants concentrated algorithms deliver interpretable models, since wanted identify predictive conditions, .e., variables values/ranges models.\r\nHence, considered decision trees, classification rules regression trees.employed J4.8 decision tree classification algorithm (equivalent C4.5 algorithm [101]) Waikato Environment Knowledge Analysis (Weka) workbench [102].\r\nalgorithm builds tree successively, splitting node (subset dataset) variable maximizes information gain within node.\r\noriginal algorithm operates variables take categorical values creates one child node per value.\r\nHowever, implementation Weka library also provides option forces algorithm always create exactly two child nodes: one best separating value one values.\r\nused option experiments, delivers trees better quality.\r\nMoreover, Weka algorithm also supports variables take numeric values:\r\nnode split two child nodes partitioning value range variable two intervals.deal skewed distribution, considered following classification variants:Naive: problem imbalanced data ignored.InfoGain: keep top-30 66 variables, sorting variables information gain towards target variable.Oversampling: use SMOTE [103] resample dataset minority-oversampling: class B, 100% new instances generated, class C 300% new instances generated, resulting following distribution :438, B:216, C:128.CostMatrix: prefered misclassify negative case rather detecting positive case, penalized false negatives (FN) false positives (FP).\r\nused cost matrix depicted Table 3.1.\r\nTable 3.1: Cost matrix. Cost matrix penalizing misclassification class skew.\r\n","code":""},{"path":"imm.html","id":"imm-classification","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.3 Classification Rule Discovery","text":"Classification rules can uncover interesting relationships one features outcome [104], [105].\r\nCompared model families deep neural networks, support vector machines random forests, classification rules achieve inferior accuracy time.\r\nHowever, easier interpret reason , hence lend better interactive subpopulation discovery.\r\nepidemiological research, interesting subpopulations subsequently used formulate validate small set hypotheses just explore associations risk factors specific outcome.\r\ninteresting subpopulation phrased ‚Äúsample study, prevalence goiter 32%, likelihood subpopulation described thyroid-stimulating hormone smaller equal 1.63 mU/l body mass index greater 32.5 kg/m2 49%.‚ÄùClassification rule algorithms induce descriptions ``interesting‚Äô‚Äô subpopulations data interestingness quantified quality function.\r\nclassification rule association rule whose consequent fixed specific class value.\r\nConsider exemplary classification rule \\(r_1\\):\r\n\\[\\begin{equation}\r\nr_1: \\underbrace{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}_{\\text{Antecedent}} \\longrightarrow \\underbrace{\\vphantom{som\\_waist\\_s2 < 80 \\wedge age\\_ship\\_s2 > 59 \\left(\\wedge \\ldots \\right)}hepatic\\_steatosis = pos}_{\\text{Consequent}}\r\n\\tag{3.2}\r\n\\end{equation}\\]","code":""},{"path":"imm.html","id":"underpinnings","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"Underpinnings","text":"Classification rules expressed form \\(r: \\text{antecedent} \\longrightarrow T=v\\).\r\nconjunction conditions (.e.¬†feature - feature value pairs) left arrow constitutes rule‚Äôs \\(\\text{antecedent}\\) (left hand site).\r\n\\(\\text{consequent}\\) (right hand side), \\(v\\) requested value target variable \\(T\\).define \\(s(r)\\) subpopulation cover set \\(r\\), .e.¬†set instances satisfy antecedent \\(r\\).\r\ncoverage \\(r\\), fraction instances covered \\(r\\), defined \\(Cov(r)=|s(r)|/N\\), \\(N\\) total number instances.\r\nsupport \\(r\\) quantifies percentage instances covered \\(r\\) additionally \\(T=v\\), calculated \\(Sup(r)=|s(r)_{T=v}|/N\\).\r\nconfidence \\(r\\) (also referred precision accuracy) defined \\(Conf(r)= |s(r)_{T=v}|/|s(r)|\\) expresses relative frequency instances satisfying complete rule (.e., antecedent consequent) among satisfying antecedent.\r\nrecall sensitivity \\(r\\) respect \\(T=v\\) defined \\(Recall(r)=Sensitivity(r)=\\frac{|s(r)_{T=v}|}{n_{T=v}}\\).\r\nWeighted Relative Accuracy rule interestingness measure balances coverage confidence gain often used internal quality criterion candidate generation [105].\r\ndefined \\(WRAcc(r) = Cov(r)\\cdot \\left(Conf(r)-\\frac{n_{T=v}}{N} \\right)\\).\r\n\r\nexample, Figure 3.5 illustrates exemplary rule \\(r_2\\) dataset 10 instances binary target, circles cyan color represent instances negative class red circles positive instances.\r\ncover set \\(r_2\\) contains instances 7, 8, 9 10, hence \\(Cov(r_2)=0.40\\).\r\n, \\(Sup(r_2)=0.30\\), \\(Conf(r_2)=0.75\\) \\(WRAcc(r_2)=0.4\\cdot\\left(0.75-0.4\\right)=0.14\\).\r\nFigure 3.5: Exemplary classification rule.\r\n","code":""},{"path":"imm.html","id":"hotspot","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.4 HotSpot","text":"classification rule discovery use algorithm HotSpot1 provided Waikato Environment Knowledge Analyis (WEKA) workbench [102].\r\nHotSpot beam search algorithm implements general--specific approach extracting rules.\r\nsingle rule constructed successively adding condition \r\nantecedent locally maximizes confidence.\r\n\r\nContrary general hill-climbing considers best rule candidate iteration, HotSpot‚Äôs beam search keeps \\(b\\) highest ranked candidates refines later steps.\r\nConsequently, HotSpot reduces ‚Äúmyopia‚Äù [104] hill-climbing search typically suffers .\r\nBriefly, hill-climbing approaches consider locally optimal candidate iteration.\r\nconsequence globally optimal rule found locally optimal every iteration.\r\napplication point view, also desirable generate one rule, alternative descriptions subpopulations can facilitate hypothesis generation.\r\nbeam width can specified maximum branching factor, maximum number conditions may added rule candidate.\r\nevery iteration, rule candidates must satisfy minimum value count sensitivity threshold.\r\navoid adding condition leads marginal improvement confidence, parameter minimum improvement, .e.¬†minimum relative improvement confidence adding condition can specified.\r\nComputational complexity rule search can reduced specifying maximum rule length, number conditions antecedent.describe parametrization experiments","code":""},{"path":"imm.html","id":"interactive-medical-miner","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.2.5 Interactive Medical Miner","text":"Classification rules can provide valuable insights potentially prevalent conditions different subpopulations cohort study.\r\nHowever, number rules produced large, usually case large epidemiological data, conditions rules overlap conditions present target feature‚Äôs classes.\r\nHence, medical expert needs inspection aids decide rules informative features studied .\r\nInteractive Medical Miner (IMM) allows expert () discover classification rules subject frequency constraints, inspect frequency rules (b) towards class (c) unlabeled part cohort, (d) study statistics rule values selected variables.\r\ndescribe functionalities , referring screenshot Figure 3.6.\r\nFigure 3.6: Interactive Medical Miner: classification rules discovered class B shown bottom left panel. selected rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, distribution participants covered rule among three classes shown absolute values (top middle panel) histogram (bottom right panel) respect age (top right panel).\r\nuser interface consists six panels.\r\n‚ÄúSettings‚Äù panel (upper left) allows medical expert set parameters rule induction pressing button ‚ÄúBuild Rules.‚Äù\r\npanel, discovered rules displayed.\r\npanel ‚ÄúSorting preference‚Äù allows expert specify whether rules sorted confidence, coverage, rather alphabetically better overview overlapping rules.mining criteria include dataset (choosing whole dataset versus one partitions), class rules generated (drop-list ‚ÄúClass‚Äù) constraints respect class, .e., min value count (can also given relative number), maximum rule length, maximum branching factor minimum improvement.\r\nexample parameters affect rule search, consider selected rule Figure xxx, som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B, coverage 0.12 confidence 0.56.\r\nsensitivity 38/108 = 0.352 satisfies minimum value count threshold 0.33.\r\nApriori property, apparent two conditions rule‚Äôs antecedent, namely som_huef_s2 > 109 crea_u_s2 > 5.38 must also exceed threshold.\r\nposition condition within antecedent indicates refinement step added rule candidate.\r\nexample, first condition som_huef_s2 > 109 confidence 44/107 = 0.41 expanded second condition crea_u_s2 > 5.38, gain confidence exceeds minimum improvement threshold, .e., 38/68 - 44/107 = 0.15 > 0.05.\r\nrule, however, expanded maximum rule length set 2.\r\nmaximum branching factor set conservatively 1000, prevent potentially interesting rules generated due small beam width.\r\nparameter can lowered interactively, number discovered rules high rule induction takes long.output list execution run (area ‚ÄúSettings‚Äù) scrollable interactive.\r\nexpert clicks rule, top middle area ‚ÄúSummary Statistics‚Äù updated.\r\nfirst row shows distribution cohort participants among classes whole dataset, second row shows participants covered rule (column ‚ÄúTotal‚Äù second row) distributed among classes.\r\nHence, expert can specify discovery classification rules one classes study often antecedent rule appears among participants classes.\r\nrule covers participants selected class (class B Figure 3.6) necessarly interesting, example, covers also high number participants classes.\r\nrule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B covers 68 participants total, 38 class B.\r\nlower number covered participants classes , .e., increase confidence, user can lower minimum value count, allow generation rules lower sensitivity, homogeneity respect selected class.data might incomplete. example, participants cohort subjected liver MRI.\r\nHence, also interest know distribution unlabeled participants support antecedent given rule.\r\n‚ÄúHistogram‚Äù panel can used purpose: expert chooses feature interactive area ‚ÄúVariable selection‚Äù upper right panel can see values variable distributed among study participants - labeled ones unlabeled ones; latter marked ‚ÄúMissing‚Äù color legend.\r\nplotting histograms make use free Java chart library JFreeChart .\r\nNumeric variables discretized using ‚ÄúScott‚Äôs rule‚Äù [106] follows:\r\nlet \\(X_{s(r)}\\) set values numeric variable \\(X\\) respect cover set \\(s(r)\\).\r\nbin width \\(h\\) calculated \\(h(X_{s(r)})=\\frac{\\max{X_{s(r)}}-\\min{X_{s(r)}}}{3.49\\text{sd}_{s(r)}}\\cdot |s(r)|^{\\frac{1}{3}}\\).expert choose variable, target variable used default, distribution labeled participants visible.\r\nhistogram Figure 3.6 shows age distribution labeled unlabeled participants covered example rule som_huef_s2 > 109 & crea_u_s2 > 5.38 \\(\\longrightarrow\\) mrt_liverfat_s2 = B.\r\nvalue distribution among labeled participants reveals ageing might risk factor displayed subpopulation likelihood class B increases higher age.\r\nvisual finding suggests adding condition age_ship_s2 > 56.8 rule‚Äôs antecedent.\r\nIndeed, confidence specific rule increases 38/68 = 0.56 27/40 = 0.675.\r\nHowever, since sensitivity reduces 38/108 = 0.352 27/108 = 0.250, minimum value count threshold longer satisfied.\r\nHence, visualization participants‚Äô statistics selected rules can deliver indications subpopulations monitored closer, hints alter algorithm parameters subsequent runs, example, lower minimum value count 0.25 increase maximum rule length 3.","code":""},{"path":"imm.html","id":"experiments-and-findings","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3 Experiments and Findings","text":"","code":""},{"path":"imm.html","id":"results-of-decision-tree-classifiers","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.1 Results of Decision Tree Classifiers","text":"evaluation decision tree classifiers, consider accuracy, .e., ratio correctly classified participants, sensitivity specificity, \\(F_1\\) score, .e., harmonic mean precision recall.\r\nspecificity, precision recall, consider two classes B C together positive class.Oversampling achieved best performance accuracy ca. 80% \\(F_1\\)score 62%.\r\nbest decision trees found partition \\(\\mathsf{F:age>52}\\), followed \\(\\mathsf{PartitionF}\\), \\(\\mathsf{PartitionM}\\).\r\nlarge discrepancy accuracy \\(F_1\\)score appears also models partitions, underlying accuracy scores unreliable skewed distribution. Therefore, report accuracy hereafter.partition \\(\\mathsf{F:age>52}\\), overall best decision tree achieved oversampling variant.\r\nlarger \\(\\mathsf{PartitionF}\\), best performance achieved decision tree produced InfoGain variant, best decision tree \\(\\mathsf{PartitionM}\\) built CostMatrix variant.\r\nSensitivity specificity values trees shown Table 3.2, trees depicted Figures 3.7 - 3.9 respectively discussed Section 3.3.3.\r\nTable 3.2: Best decision trees three partitions: best separation achieved \\(\\mathsf{F:age>52}\\); \\(\\mathsf{PartitionM}\\) heterogeneous one, performance values lowest.\r\nTable 3.2 indicates decision tree variants perform differently different partitions.\r\nOversampling beneficial \\(\\mathsf{F:age>52}\\), partially compensates skew problem.\r\n\\(\\mathsf{PartitionM}\\) heterogeneous class distribution paritions, variants perform relatively poor .\r\nHence, expected insights decision trees \\(\\mathsf{F:age>52}\\) \\(\\mathsf{PartitionF}\\), better separation achieved.\r\nFigure 3.7: Best decision tree \\(\\mathsf{F:age>52}\\), achieved variant Oversampling.\r\n\r\nFigure 3.8: Best decision tree \\(\\mathsf{PartitionF}\\), achieved variant InfoGain.\r\n\r\nFigure 3.9: Best decision tree \\(\\mathsf{PartitionF}\\), achieved variant InfoGain.\r\n","code":""},{"path":"imm.html","id":"discovered-classification-rules","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.2 Discovered Classification Rules","text":"classification rules found HotSpot whole dataset conclusive class positive classes B C, omit reporting rules useful diagnostic purposes.\r\nclassification rules found partitions informative.\r\nHowever, classification rules one feature antecedent low confidence.\r\nensure high confidence, restricted output rules least two features antecedent.\r\nensure still high coverage, allowed three features.\r\nselection high confidence high coverage rules partition class shown Tables 3.3 ‚Äì 3.5, respectively.\r\ndescribe important features antecedent rules next subsection, together important features best decision trees.\r\nTable 3.3: Best HotSpot classification rules (maxLength = 3) \\(\\mathsf{PartitionF}\\) (excerpt).\r\n\r\nTable 3.4: Best HotSpot classification rules (maxLength = 3) \\(\\mathsf{F:age>52}\\) (excerpt).\r\n\r\nTable 3.5: Best HotSpot classification rules (maxLength = 3) \\(\\mathsf{PartitionM}\\) (excerpt).\r\n","code":""},{"path":"imm.html","id":"imm-important-features","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.3 Important Features for Each Subpopulation","text":"important features decision trees Figures 3.7 - 3.9 closer root.\r\nbetter readability, tree nodes figures contain short descriptions instead original variable names.\r\nthree decision trees, root node ultrasound diagnosis variable stea_s2.\r\nnegative ultrasound diagnosis points negative class , \r\npositive ultrasound diagnosis directly lead one positive classes B C.\r\ndecision trees three partitions differ nodes placed near root.best decision tree \\(\\mathsf{PartitionF}\\) (cf.¬†Figure 3.8) observe \r\nultrasound report positive HbA1C concentration 6.8%, class C.\r\nclassification rules high coverage confidence Table 3.3) point interesting features:\r\nwaist circumference 80 cm, BMI 24.82 kg/m2, hip circumference 97.8 cm less characterize participants negative class.\r\n6 participants serum glucose concentration greater 7 mmol/l TSH concentration greater 0.996 mu/l belong class C.\r\n, severe obesity (BMI value 38.42 kg/m2 points class C high confidence ‚Äì combination variables.contrast best tree \\(\\mathsf{PartitionF}\\), best decision tree subpartition \\(\\mathsf{F:age>52}\\) (cf.¬†Figure ~3.7) also contains nodes SNPs, indicating potentially genetic associations fatty liver participants.\r\nClassification rules high coverage confidence class B also contain SNPs, can seen Table ~3.4.\r\nSimilarly \\(\\mathsf{PartitionF}\\), high BMI values point positive class combined features: Table 3.4, see four participants stea_alt75_s2 = 3 (.e.¬†positive ultrasound diagnosis combined critical ALAT value) BMI larger 38.42 kg/m2 belong class C.\r\nsimilar association holds stea_alt75_s2 = 3 combined high waist circumference (> 124 cm).\r\n19 20 participants class B positive ultrasound diagnosis, genetic marker gx_rs11597390 = 1 serum HDL concentration 1.53 mmol/l.role ultrasound report predicting negative class \\(\\mathsf{PartitionM}\\) (cf.¬†Figure 3.9 \\(\\mathsf{PartitionF}\\).\r\nbest tree \\(\\mathsf{F:age>52}\\), best tree \\(\\mathsf{PartitionM}\\) contains nodes SNPs serum GGT value ranges.\r\nfeatures also antecedent top Hotspot rules (cf.¬†Table ~3.5):\r\nSerum GGT concentration 1.9,\\(\\mu\\)mol/sl combination creatinine concentration 90 mmol/l thromboplastin time ratio (quick_s2) 59% point class C.\r\nSimilarly, positive ultrasound diagnosis serum HDL concentration exceeding 0.84 mmol/l point class C.decision trees classification rules give insights features seem diagnostically important.\r\nHowever, medical expert needs additional information decide whether feature worth investigation.\r\nparticular, decision trees highlight importance feature context subtree located; subtree describes subpopulation usually small.\r\ncontrast, classification rules return information larger subpopulations.\r\nHowever, subpopulations may overlap; example, first four rules class C \\(\\mathsf{PartitionM}\\) (cf.¬†Table 3.5) may refer 6 participants.\r\nMoreover, unless classification rule confidence close 100 %, may participants classes also support .\r\nHence, decide whether features rule‚Äôs antecedent deserve investigation, expert also needs insights rule‚Äôs statistics classes well.\r\nassist expert task, propose Interactive Medical Miner, tool discovers classification rules class delivers information statistics rules classes.","code":""},{"path":"imm.html","id":"enhancements","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.3.3.1 Enhancements","text":"[107], added functionalities specify (sub-)cohort dataset, example, focus subset female participants.\r\nadditional window, expert can specify filtering queries form Variable Operator Value.\r\ndefined restrictions shown table user can undo previous constraints.\r\n\r\n, user can (de-)select variables model generation.\r\nexample, expert might exclude variable known highly correlated another variable already considered model learning.[108] added panels contain tables displaying additional rule statistics lift p-value.\r\n, mosaic chart juxtaposes class distributions subpopulation ‚Äúcomplements‚Äù subgroups participants match one conditions length-2 rule describes subpopulation.paper, study SD binary target variable problem (positive negative outcome). subpopulation \\(r\\), \\(s(r)\\) set instances satisfy antecedent rule \\(r\\), also known cover set \\(r\\). % %\r\n%subpopulation comprises instances fulfill logical condition rule‚Äôs antecedent; ae rule described :\r\n%\r\n%, \\(|s(r)_{T=v}|\\) number instances additionally belong value target variable \\(s(r)\\), support set; \\(n\\) total number instances dataset; \\(n_{T=v}\\) total number instances exhibit target variable value interest. %\r\n%conjunction feature - feature value pairs left arrow constitute rule‚Äôs antecedent (left hand site), pair target feature - target feature value right arrow rule‚Äôs consequent (right hand side).\r\nrule Equation (3.2) classification rule referring class B.Features continuous values (e.g.¬†age, waist) restricted within specific range.\r\nrange chosen algorithm way maximize support rule within cohort confidence rule‚Äôs consequent given antecedent. first rule example belongs results \\(\\mathsf{PartitionF}\\) (see Table 3.3, described next section): 20 participants supporting antecedent, 17 belong class B.\r\nNote use expressions  .classification rule discovery use Weka algorithm HotSpot. class, algorithm determines rules best confidence  optimal boundary values features antecedent. wrapped algorithm mechanism selects class rules supported least \\(\\tau\\) participants. experiments, use \\(\\tau=\\frac{1}{3}\\), interactive tool (section ??) allows expert set threshold freely.","code":""},{"path":"imm.html","id":"conclusions-on","chapter":"3 Interactive Discovery and Inspection of Subpopulations","heading":"3.4 Conclusions on ‚Ä¶","text":"[109]","code":""},{"path":"sdclu.html","id":"sdclu","chapter":"4 Identifying Distinct Subpopulations","heading":"4 Identifying Distinct Subpopulations","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Bernhard Preim, Till Ittermann, \r\nHenry V√∂lzke. ‚ÄúCombining Subgroup Discovery Clustering Identify\r\nDiverse Subpopulations Cohort Study Data.‚Äù : Proc. IEEE Int.\r\nSymposium Computer-Based Medical Systems (CBMS). 2017, pp.¬†582-587.\r\nDOI: 10.1109/CBMS.2017.15.Epidemiologists search significant relationships risk factors outcome large heterogeneous datasets encompass participant health information gathered questionnaires medical examinations.\r\nprevious chapter, described expert-driven workflow can help epidemiologists automatically detect relationships form classification rules, descriptions risk factors predictive value ranges specific subpopulation outcome interest.\r\nHowever, rule induction algorithms often produce large overlapping rule sets requiring expert manually pick interesting rules remove less interesting redundant rules.\r\npost-filtering step time-consuming tedious.\r\nchapter presents clustering-based algorithm hierarchically reorganizes large rule sets summarizes important concepts maintaining distinctiveness clusters.\r\ncluster, representative rule shown expert turn can drill-cluster members.\r\nevaluate algorithm two subsets SHIP outcomes hepatic steatosis goiter serve target variable, respectively.\r\n, report effectiveness algorithm present selected subpopulations.chapter presents SD-Clu, approach combines subgroup discovery clustering return set \\(k\\) representative classification rules.\r\nBuilding set potentially highly overlapping rules generated SD algorithm, leverage hierarchical agglomerative clustering find groups rules cover different sets instances.\r\ncluster, nominate one rule group‚Äôs representative exhibits best trade-rule confidence coverage towards target variable.\r\ndefine similarity pair subgroups based fraction mutually covered instances individually covered instances.\r\nRules covering (almost) instances likely condensed cluster thus likely represented proxy rule.\r\nevaluate algorithm two samples SHIP investigating diseases hepatic steatosis goiter.Section¬†4.1 serves motivation related field subgroup discovery redundancy classification rules.\r\n\r\nSection¬†4.2 presents SD-Clu algorithm generates distinct rules.\r\nSection¬†4.3 contains experimental setup.\r\nSection¬†4.4, describe evaluation results.\r\nSection¬†4.4.1, discuss findings regarding hepatic steatosis goiter SHIP data.\r\nSection¬†4.5, summarize contributions.\r\n","code":""},{"path":"sdclu.html","id":"brief-chapter-summary-1","chapter":"4 Identifying Distinct Subpopulations","heading":"Brief Chapter Summary","text":"study redundancy large rule sets describing subpopulations.\r\npresent workflow extracts smaller number representative rules.\r\nrules selected avoid instance overlap much possible, thus covering different concepts data space.\r\nevaluate workflow samples SHIP hepatic steatosis goiter target variables.","code":""},{"path":"sdclu.html","id":"sdclu-intro","chapter":"4 Identifying Distinct Subpopulations","heading":"4.1 Motivation and Comparison to Related Work","text":"Subgroup discovery (SD) algorithms aim uncover interesting relationships one conditions (variables value ranges) target variable form classification rules¬†[105], [110].\r\nCompared accurate predominantly opaque black-box models neural networks, support vector machines, random forests, SD algorithms provide higher confidence interpretability results, making highly suitable domain expert-guided subpopulation discovery.\r\nSD algorithms used several medical studies descriptive knowledge needed inferred, e.g., extract potential drug targets multi-relational data sources treatment dementia¬†[111], identify predictive auditory-perceptual, speech-acoustic, articulatory-kinematic features preschool children speech sound disorders¬†[112], discover discriminative features patient subpopulations different admission times psychiatric emergency departments¬†[113].However, SD methods often yield large sets rules domain experts willing tediously go manually separate interesting irrelevant redundant rules.\r\ncommon observation groups rules cover almost set instances, shown Figure¬†4.1.\r\nInstead presenting rules found SD algorithm , propose organize rule sets hierarchically domain expert able explore compact set different concepts, equipped mechanisms drill specific rules interest.\r\nFigure 4.1: Example two redundant rules. \\(r_1\\) \\(r_2\\) cover instances 7,8,9,10; \\(r_2\\) additionally covers instance 2. cover set overlap due high correlation #Teeth Age, BMI Waist circumference. rules describe concept, .e., elderly overweight people higher risk outcome.\r\nSimilar HotSpot algorithm described Section¬†3.2.4, popular SD algorithms SubgroupMiner¬†[114],\r\nSD¬†[115] CN2-SD¬†[116] use fixed beam width limit number expanded subgroup candidates iteration.\r\npost-pruning step can applied reduce cardinality rule set ‚Äì e.g., return top-k rules ‚Äì using quality criterion weighted relative accuracy p-value statistical test.\r\nEven beam-width search top-\\(k\\) pruning applied, result often still contains redundant rules.\r\ndue correlation (non-target) variables, leads large number variations given finding, cf.¬†Figure¬†4.2 illustrative example.\r\nparticular, top-\\(k\\) pruning leads different variations concepts (.e.¬†large instance overlap)¬†[117].\r\nFigure 4.2: Graphical representation 1115 HotSpot rules found SHIP data 886 labeled instances. gray cell indicates instance x-position covered rule associated y-position. Instances rules sorted agglomerative hierarchical clustering. partitioning 10 clusters based covered instances, rules cluster describe similar subpopulations.\r\nUnlike SD, generates multiple rules overlap terms coverage sets, predictive rule learning algorithms CN2¬†[118] RIPPER¬†[119] designed generate rules capture different spaces data.\r\nwork iteratively according divide--conquer strategy¬†[104]:\r\nFirst, rule maximizes quality function algorithm generated set instances yet covered.\r\nSecond, covered instances removed training set.\r\nprocess rule induction removal instances training set repeated instances covered least one rule.\r\noutput algorithm often decision list.\r\nclassify instance, prediction first rule covers instance used.\r\nalgorithms avoid problem rule redundancy, important concepts may remain uncovered.\r\nexample, algorithm might induce rule includes instances high BMI, might able find slightly weaker association income instances immediately removed instance candidate space.\r\naddition, coverage rules generally decreases iteration.\r\nRules low coverage negligible epidemiological studies may represent spurious correlations study sample.Instead simply eliminating covered instances rule induction subsequent iterations, weighted coverage approaches take account many times instances covered far rule candidate expansion step¬†[116].\r\nleniency removing instances allows larger number rules, new hyperparameter introduced control tradeoff reusing already covered instances minimum rule confidence.\r\nparameter unintuitive difficult set, especially domain experts.\r\nMoreover, traditional predictive rule learning algorithms weighted covering extensions introduce order dependencies rules: rule depends previous rules rule list instances target variable covers, may make sense interpret single rule.","code":""},{"path":"sdclu.html","id":"sdclu-method","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2 Finding Distinct Classification Rules","text":"section, present algorithm SD-Clu, combines subgroup discovery clustering return set \\(k\\) distinct classification rules.\r\nalgorithm consists three main steps.\r\nFirst, SD algorithm generates set potentially highly overlapping rules.\r\nUsing hierarchical agglomerative clustering, set rules grouped set distinct rule clusters covering different sets instances.\r\ncluster, rule best tradeoff confidence coverage target variable appointed representative group.\r\nRules covering instances grouped cluster therefore represented proxy rule.","code":""},{"path":"sdclu.html","id":"rule-clustering","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.1 Rule clustering","text":"use notation classification rule discovery Section¬†3.2.3.\r\nAgglomerative hierarchical clustering iteratively merges similar instances clusters, bottom-way.\r\norder merging two clusters depends linkage strategy: complete linkage distance two clusters defined maximum distance two instances.\r\npair clusters minimizing maximum distance selected merging.\r\ndendrogram tree representation stepwise process.\r\nOptionally, parameter \\(k\\) can specified obtain specific partitioning.\r\ndefine rule similarity clustering basis mutually covered instances adaption S√∏rensen‚ÄìDICE coefficient¬†[120].\r\ndistance two rules \\(r_1\\), \\(r_2\\) corresponding subpopulations \\(s(r_1)\\), \\(s(r_2)\\) given \r\n\\[\\begin{equation}\r\n\\text{dist}(r_1,r_2) = 1 - \\frac{2\\cdot\\left|s(r_1)\\cap s(r_2)\\right|}{\\left|s(r_1)\\right| + \\left|s(r_2)\\right|}.\r\n\\tag{4.1}\r\n\\end{equation}\\]\r\nnumber clusters can specified parameter \\(k\\).\r\nAlternatively, describe approach derives appropriate \\(k\\) rule set using cluster quality function Silhouette coefficient.\r\nclustering \\(\\xi\\) set rules \\(R\\), Silhouette coefficient calculated \r\n\\[\\begin{equation}\r\n\\text{Silh}(R,\\xi) = \\frac{1}{|R|}\\sum_{r\\R}{\\frac{b(r)-(r)}{\\max\\left\\{(r), b(r)\\right\\}}}\r\n\\tag{4.2}\r\n\\end{equation}\\]\r\n\r\n\\[\\begin{equation}\r\n(r)=\\frac{\\sum_{y\\{}Y}\\text{dist}(r,y)}{|Y|-1}\r\n\\tag{4.3}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Y\\\\xi\\) contains \\(r\\), \r\n\\[\\begin{equation}\r\nb(r)=\\frac{\\sum_{y\\{}Z}\\text{dist}(r,y)}{|Z|}\r\n\\tag{4.4}\r\n\\end{equation}\\]\r\naverage dissimilarity \\(r\\) rules cluster \\(Z\\\\xi\\) closest cluster \\(Y\\) containing \\(r\\).\r\n, traverse dendrogram bottom-, compute Silhouette set clusters \\(\\xi\\) select \\(\\xi_{opt}\\) set clusters best Silhouette value.\r\noptimal number clusters cardinality \\(|\\xi_{opt}|\\).\r\nFinally, map cluster \\(Y\\\\xi_{opt}\\) representative rule.\r\n, invoke rule interestingness measure Weighted Relative Accuracy (WRA hereafter) balances coverage confidence gain defined \r\n\\[\\begin{equation}\r\nWRA(r)=Cov(r)\\cdot\\left(Conf(r)-\\frac{n_{T=v}}{N}\\right)\r\n\\tag{4.5}\r\n\\end{equation}\\]\r\n\\(N\\) total number instances dataset \\(n_{T=v}\\) number instances target variable value interest.\r\ncompute WRA rule \\(r\\{Y}\\) select cluster proxy \\(cp(Y)\\) rule WRA maximum.","code":""},{"path":"sdclu.html","id":"representativeness-of-a-set-of-cluster-proxies","chapter":"4 Identifying Distinct Subpopulations","heading":"4.2.2 Representativeness of a set of cluster proxies","text":"rule proxies good representation total rule set.\r\nThus, instance covered equally often cluster proxies compared total rule set.\r\nHence, define representativeness difference average fraction proxy rules instances covered total average fraction rules instances covered .\r\ndifference two ratios small, representativeness cluster proxy rules high.Typically, set rule clusters \\(\\zeta\\) set rules \\(R\\) optimal set clusters, described previous subsection, can set clusters chosen user, long contains rules \\(R\\).\r\n\\(\\zeta\\), let \\(R_{\\zeta}=\\{cp(Y)|Y\\\\zeta\\}\\) denote set cluster proxy rules.\r\nquantify representative set rules , proceed follows.\r\nFirst, let \\(U\\subseteq{}R\\) arbitrary subset complete set rules, let \\(x\\) instance.\r\ncoverage rate \\(x\\) towards \\(U\\) calculated \r\n\\[\\begin{equation}\r\ncovRate(x,U) = \\frac{\\sum_{r\\{}U}isCovered(x,r)}{|U|}\r\n\\tag{4.6}\r\n\\end{equation}\\]\r\n\\(isCovered(x,r)\\) equal 1, \\(r\\) covers \\(x\\), .e., \\(x\\s(r)\\), 0 otherwise.\r\nobserve set rules \\(U\\), instance \\(x\\) covered \\(|U|\\) rules.\r\nLet \\(R_x\\) set rules cover instance \\(x\\), .e., \\(R_x=\\{r\\{}U | isCovered(x,r)=1 \\}\\).\r\nwhole set instances \\(X\\) create bins:\r\n\\[\\begin{equation}\r\nbin_i(U)= \\{x \\{}X | |R_x|=\\}.\r\n\\tag{4.7}\r\n\\end{equation}\\]\r\n, let \\(bin_0(U)= \\{x \\{}X | \\forall r\\U : isCovered(x,r)=0 \\}\\).\r\nEvidently, instance \\(x\\) can covered \\(0, 1, \\ldots, |U|\\) rules, .e.¬†\\(covRate(x,U)\\) can take one \\(|U|+1\\) values.\r\ncontrast, \\(covRate(x,R)\\) can take one \\(|R|+1\\) values, usually much larger number. therefore map possible values \\(covRate(x,R)\\) much smaller set possible values rounding, computing:\r\n\\[\\begin{equation}\r\nadjCovRate(x,U, R)=\\frac{\\lfloor covRate(x,R)\\cdot|U| \\rceil}{|U|}\r\n\\tag{4.8}\r\n\\end{equation}\\]\r\n\\(\\lfloor\\rceil\\) rounding operator.\r\n, complete set instances \\(X\\), set induced rules \\(R\\), clustering \\(\\zeta\\) \\(R\\) set cluster proxy rules \\(R_{\\zeta}\\), representativeness \\(R_{\\zeta}\\) defined \r\n\\[\\begin{equation}\r\nrepresentativeness(R_{\\zeta},R)=1-\r\n\\frac{1}{|X|}\\sum_{x\\{}X} |adjCovRate(x,U,R) - covRate(x,R_{\\zeta})|.\r\n\\tag{4.9}\r\n\\end{equation}\\]","code":""},{"path":"sdclu.html","id":"sdclu-experiments","chapter":"4 Identifying Distinct Subpopulations","heading":"4.3 Experimental Setup","text":"Datasets.\r\nevaluate method, used data SHIP study.\r\ndescription SHIP, see Section¬†2.6.1.\r\nconsidered hepatic steatosis (see section¬†3.2.1) goiter target variables.\r\nsample HepStea, derived binary outcome variable discretizing liver fat concentration obtained MRI report study participants concentration greater 10% assigned negative class values greater 10% assigned positive class indicating presence disease.\r\n886 participants MRI report SHIP-2 available time, 694 (78.3%) negative 192 (21.7%) positive.\r\nconsidered 99 variables selected exclusively SHIP-0 assess long-term effects expressed 10 years later SHIP-2.\r\nGoiter sample, outcome variable derived thyroid ultrasound.\r\npresence goiter defined thyroid volume greater 18ml women 25ml men¬†[121].\r\n4400 participants outcome variable available TREND-0, 3010 belong negative class (68.4%) 1390 (31.6%) positive class.\r\nApart target variable, use total 182 variables pre-selected medical expert potential risk factors.SD algorithms.\r\nsubgroup discovery, two algorithms HotSpot¬†[122] (cf.¬†section¬†3.2.4 SD-Map¬†[123] used.\r\nSD-Map exhaustive algorithm adapts popular FP-Growth association rule learning method¬†[72].\r\nRules fall minimum coverage threshold pruned.\r\ndeep-first search performed candidate generation.\r\nRules ranked according user-defined quality function.\r\nused implementation VIKAMINE framework¬†[124].\r\nimplementation SD-Map supports categorical variables.\r\nTherefore, numeric variable discretized using minimum description length based approach Fayyad Irani¬†[125].\r\nSD-Map, set minimum coverage threshold 0.05 avoid overfitting rules small.\r\nuse WRA quality function define minimum threshold 0.025.\r\nHotSpot, set support threshold rule 0.05.\r\nbeam width set 500.\r\naddition, avoid rather meaningless literals, restrict extension rule body another literal relative confidence gain least 0.3.\r\navoid many overly specific rules cover small number study participants, limit length rule body, .e., number literals 3.","code":""},{"path":"sdclu.html","id":"sdclu-results","chapter":"4 Identifying Distinct Subpopulations","heading":"4.4 Results","text":"Figure¬†4.3, show optimal number clusters combination study sample SD algorithm.\r\nTable¬†4.1 shows optimal \\(k\\) ratio proxy rules vs.¬†total number rules.\r\nexample, clustering optimal silhouette coefficient algorithm HotSpot Goiter 76 clusters thus 76 rule cluster proxies (cf.¬†Table¬†4.1), 21.3% total number rules.\r\nThus, cluster proxies displayed expert beginning, time needed check rules reduced.\r\nFigure 4.3: Silhouette coefficients (\\(Silh\\)) SD-Clu using complete linkage combination dataset algorithm. cardinality clustering highest \\(Silh\\) score (\\(|\\zeta_{opt}|\\)) indicated dashed vertical line.\r\n\r\nTable 4.1: Statistics best runs per dataset algorithm. Number rules \\(|R|\\), optimal Silhouette coefficient \\(Silh(\\zeta_{opt})\\), corresponding cardinality optimal clustering \\(|\\zeta_{opt}|\\) percentage cluster proxies relative total number rules every combination data sample SD algorithm.\r\noptimal cardinality \\(|\\zeta_{opt}\\) shown Figure¬†4.3 used suggestion, expert free specify number rules wishes obtain.\r\nexample, expert considers \\(|\\zeta_{opt}|\\) = 100 large HotSpot HepStea, diagram show reduction \\(|\\zeta_{opt}|\\) = 58 possible, reducing \\(Silh\\) slightly 0.48 0.37.\r\nAlso direction: \\(|\\zeta_{opt}|\\) rather low, diagram shows small increase change \\(Silh\\) much; therefore, added rules may also important.\r\nexpert even analyze diagram derive range instead single value, e.g., range \\(Silh\\) 90% maximum.assess representativeness cluster proxies, compare three baseline criteria return top \\(k\\) rules according odds ratio (baseline 1), coverage (baseline 2), WRA (baseline 3).\r\nFigure¬†4.4 juxtaposes representativeness SD-Clu three baselines different numbers rules \\(k\\) returned expert HotSpot algorithm sample HepStea.\r\nplots arranged rule selection method (rows) number representative rules \\(k\\) returned expert (columns).\r\ngraph shows \\(adjCovRate\\) (y-axis) instances (x-axis) respect \\(R\\) (solid black curve).\r\ninstances sorted number rules \\(R\\) covered , respective \\(covRate\\) shown dots.\r\ndotted curve represents locally weighted scatterplot smoothing (LOWESS) points.\r\nIdeally, curves close , meaning instances covered approximately proportion rules cluster proy proportion rules general.\r\nFigure 4.4: Evaluation \\(representativeness\\) HepStea using HotSpot algorithm. \\(representativeness\\) SD-Clu three baseline approaches different numbers clusters \\(k\\) HepStea sample using HotSpot. Points depict instance‚Äôs \\(adjCovRate\\) respect set representative rules approach (row) \\(k\\) (column). Instances sorted \\(covRate\\) respect \\(R\\) , .e., set rules) descending order, shown solid black curve. dotted curve depicts LOWESS regression fit points. similarity solid dotted curve illustrates \\(representativeness\\) top-\\(k\\) rules respective approach. illustrated dark gray area -solid curve dotted curve smaller areas better reflect higher \\(representativeness\\) values.\r\napproaches, \\(representativeness\\) improves increasing number representative rules \\(k\\).\r\nexample, \\(representativeness\\) increases 0.87 0.96 SD-Clu \\(k=10\\) \\(k=50\\) means absolute difference \\(adjCovRate\\) \\(\\zeta\\) \\(covRate\\) \\(R\\) instances successively decreases.\r\n, given \\(k\\), representative rules baselines less representative SD-Clu‚Äôs cluster proxy rules, e.g.¬†0.91, 0.92, 0.91 vs.¬†0.96 \\(k=50\\), respectively (cf.¬†5th column plot matrix Figure¬†4.4).","code":""},{"path":"sdclu.html","id":"sdclu-discussion","chapter":"4 Identifying Distinct Subpopulations","heading":"4.4.1 Discussion of Findings","text":"Tables¬†4.2 ¬†4.3 show antecedent, support, confidence cluster proxy rules found two algorithms HepStea Goiter \\(k\\)=5.\r\nprevalence hepatic steatosis goiter significantly higher subpopulations described rules corresponding overall population.\r\nsubpopulations characterized known risk factors hepatic steatosis, large waist circumference BMI, blood pressure hypertension, advanced age, high values medical tests (ALAT LDL).\r\nFurthermore, apolipoprotein A1 (ApoA1), major protein component high-density lipoprotein (HDL) particles plasma, found associated outcome elderly patients (see fourth hotspot rule Tables¬†4.2).\r\nLipoprotein metabolism considered main process contributing development fatty liver¬†[126].\r\naddition, Poynard et al¬†[127] found patients hepatic steatosis higher levels ApoA1 patients hepatic fibrosis, turn higher levels patients cirrhosis.\r\nfifth hotspot rule describes subpopulation elevated levels liver high-sensitivity C-reactive protein (CRP) (approximately 0.80-quantile) elevated levels uric acid (approximately 0.36-quantile).\r\nLizardi-Cervera et al.¬†[128] reported increased levels ultra-sensitive CRP subjects hepatic steatosis independent metabolic states.\r\nSimilarly, Keenan et al.¬†[129] found elevated uric acid levels patients hepatic steatosis independent metabolic syndrome.\r\nTable 4.2: Representative rules (HepStea). Proxy rules \\(k\\)=5 HepStea sample learned positive outcome target.\r\nSimilarly, identified subpopulations goiter (see Table¬†4.3) characterized common risk factors increased weight body mass index participants prescribed angiotensin II receptor blockers (see second HotSpot rule).\r\nFurthermore, first HotSpot rule describes participants intima-media thickness greater 0.73 mm (approximately 0.80-quantile).\r\nPrevious studies found associations intima-media thickness thyroid-stimulating hormone¬†[130] subclinical hypothyroidism\r\n[131], [132].\r\ncondition third HotSpot rule describes duration ECG phase.\r\nJabbar et al¬†[133] summarize review pathological thyroid hormone levels increase risk cardiovascular disease.\r\nassociation appears especially true elderly¬†[134].\r\nfourth rule suggests certain thrombocyte levels indicate increased thyroid volume, confirms Erikci et al¬†[135] found hypothyroid patients higher platelet volume platelet distribution width control group.\r\nTable 4.3: Representative rules (Goiter). Proxy rules \\(k\\)=5 Goiter sample learned positive outcome target.\r\n","code":""},{"path":"sdclu.html","id":"sdclu-conclusions","chapter":"4 Identifying Distinct Subpopulations","heading":"4.5 Conclusions on identifying distinct subpopulations","text":"SD-Clu tackles problem high instance overlap sets rules generated subgroup discovery algorithms.\r\nlimiting number rules time spent rule inspection reduced.\r\nSD-Clu nominates representative rule hierarchical clustering large set rules, thus returns rules express distinct concepts, .e., rules cover different sets instances.\r\nintroduced representativeness measure assesses whether instances similarly often covered representatives total rule set.\r\nSD-Clu evaluated two samples epidemiological study optimal set proxy rules selected () contains considerably less rules total rule set (ii) representative compared baseline approaches, respectively.","code":""},{"path":"phenotypes.html","id":"phenotypes","chapter":"5 Visual Identification of Informative Features","heading":"5 Visual Identification of Informative Features","text":"chapter partly based :Uli Niemann, Petra Brueggemann, Benjamin Boecking, Matthias Rose, Myra Spiliopoulou, Birgit Mazurek. ‚ÄúPhenotyping chronic tinnitus patients using self-report questionnaire data: cluster analysis visual comparison.‚Äù : Scientific Reports 10.1 (2020), p.¬†16411. DOI: 10.1038/s41598-020-73402-8.supervised methods classification subgroup discovery described previous chapters show great potential applications one small number well-defined target variables.\r\nHowever, medical conditions heterogeneous appearances well understood yet.\r\nexample, chronic tinnitus complex, multi-factorial heterogeneous symptom.\r\nClinical assessment selection suitable therapy strategies difficult patients benefit equally treatment.\r\nDue large number heterogeneity available assessment tools, unsupervised methods like cluster analysis appear promising approaches extract clinically relevant tinnitus phenotypes.\r\nClinical acceptance empirical results can strengthened comprehensive visualization intuitively illustrate major characteristics phenotype differences multiple phenotypes.\r\nchapter, describe workflow (1) identify distinct tinnitus phenotypes parameter-free clustering algorithm (2) visualize subgroups explore phenotype idiosyncrasies.\r\nSection¬†5.1, describe clinical value patient subgroup stratification, briefly review previous approaches list requirements clustering solution.\r\nspecify features used clustering Section¬†5.2, give overview clustering algorithm Section¬†5.3, introduce cluster visualization Section¬†5.4 interactive web application Section¬†5.5.\r\nSection¬†5.6, present phenotypes describe major characteristics.\r\nmedical interpretation findings developed together tinnitus experts provided Section¬†5.7.\r\n, discuss strength weaknesses workflow Section¬†5.8.\r\nFinally, conclude chapter summary Section¬†5.9.","code":""},{"path":"phenotypes.html","id":"brief-chapter-summary-2","chapter":"5 Visual Identification of Informative Features","heading":"Brief Chapter Summary","text":"identify distinct tinnitus phenotypes clustering screening data using parameter-free algorithm leverages Bayesian information criterion determine suitable number subgroups.\r\npresent novel radial bar radial line visualizations juxtapose phenotypes high-dimensional feature space explore phenotype-specific idiosyncrasies.\r\nprovide web application enhanced versions visualizations also depict treatment effects.","code":""},{"path":"phenotypes.html","id":"phenotypes-motivation","chapter":"5 Visual Identification of Informative Features","heading":"5.1 Motivation and Comparison to Related Work","text":"Challenges management treatment tinnitus caused large extend clinical heterogeneity, includes individual perception, risk factors, comorbidities, degrees perceived stress treatment response (cf.¬†Section¬†2.6.4).\r\nfactors make difficult clinicians () choose treatment effective individual patient, (b) design unified treatment strategy patients equally benefit .\r\nawareness existence distinct patient subgroups may stimulate development effective therapy modules.\r\nSince clinically relevant subgroups established yet, clustering emerges promising approach identify characteristic tinnitus phenotypes data-driven hypothesis-free way.Previous studies found subgroups tinnitus patients cluster analysis based small number audiometric features¬†[29], combination features extracted self-reports, audiometry psychoacoustics¬†[30], neuroimaging data \r\nsocio-demographics¬†[136].\r\nAlthough studies provided insights tinnitus subgroup patterns, acceptance among medical scholars may increased presenting clustering results intuitive visualizations show individual subgroup patterns enable visual juxtaposition multiple subgroups respect high-dimensional data.Addressing requirement, Schlee et al.¬†[137] proposed compact radar chart visualization allows compare degree health burden individuals subgroups based measurements self-report questionnaires.\r\nvisualization applied disease domain, Schlee et al.¬†demonstrated efficacy showing subgroup differences respect measurements tinnitus distress associated comorbidities.\r\nHowever, aim visualize clustering results, restricted pre-defined cohorts graphically comparing female male patients, patients low tinnitus frequency patients high tinnitus frequency.","code":""},{"path":"phenotypes.html","id":"phenotypes-features","chapter":"5 Visual Identification of Informative Features","heading":"5.2 Selection of Measurement Instruments","text":"Discussions tinnitus experts selection measurement instruments (hereafter denoted features) clustering resulted two main requirements:\r\n(1) features cover clinical heterogeneity tinnitus great degree, \r\n(2) available, robust compound scores preferred single items questionnaire.\r\nroutine questionnaire assessment battery (cf.¬†Section¬†2.6.4), selected total 64 features2 14 questionnaires.\r\ninclude questionnaire total scores, questionnaire subscale scores items questionnaires neither subscales total scores.\r\nfeatures measure\r\n(general) tinnitus characteristics,\r\nphysical quality life,\r\nexperiences pain,\r\nsomatic expressions,\r\naffective symptoms,\r\ntinnitus-related distress,\r\ninternal resources,\r\nperceived stress, \r\nmental quality life.total 4,103 patients, data 2,875 (70.1%) incomplete therefore excluded.\r\nN=1,228 patients included final sample slightly, yet significantly younger excluded ones (\\(\\mu\\)included = 50.0, \\(\\sigma\\)included = 11.9; \\(\\mu\\)excluded = 51.7, \\(\\sigma\\)excluded = 13.6; \\(t\\)(2630.8) = 4.0, p \\(<\\) 0.01).\r\nAdditionally, 989 included patients patients (80.5%) post-treatment data also available used visually explore treatment effect differences clusters.\r\nSince features greater scores higher health burden, reversed remaining features greater scores higher quality life.\r\nfeature \\(X\\) reversed \\(X_{reversed} = \\max{(X)} - X\\).\r\nasterisk suffix feature name (e.g.¬†ACSA_qualityoflife*) denotes reversed feature.\r\nDue widely differing value ranges, feature standardized via z-score normalization.\r\nfeature \\(X\\) expected value \\(E(X)=\\mu\\) variance \\(Var(X) = \\sigma^2\\) standardized \\(Z = \\frac{X - \\mu}{\\sigma}\\).\r\n\\(Z\\), holds true \\(\\mu\\) = 0 \\(\\sigma^2\\) = 1.","code":""},{"path":"phenotypes.html","id":"phenotypes-xmeans","chapter":"5 Visual Identification of Informative Features","heading":"5.3 Identification of Tinnitus Phenotypes using Clustering","text":"Practical considerations data clustering include set number clusters \\(K\\).\r\nSince ground truth often available, several heuristics automatically determine \\(K\\) proposed.\r\npopular approach called ‚Äúelbow‚Äù method involves running clustering algorithm different values \\(K\\) (cf.¬†Section¬†6.2.1 application elbow method density-based clustering).\r\nnumber clusters plotted cluster compactness.\r\npopular \\(K\\)-means algorithm, cluster compactness quantified total within-sum squares (WSS), sum squared distances observation centroid clusters.\r\nWSS similar goodness fit measures increase monotonically increasing \\(K\\), idea elbow method identify curve‚Äôs characteristic ‚Äúknee point‚Äù first point adding another cluster leads minor improvement compactness.\r\nplot guaranteed exhibit distinctive knee point universal compactness thresholds exist, approach sometimes impracticable.\r\nAnother popular clustering evaluation measure Silhouette coefficient favor clusterings similar objects assigned cluster dissimilar objects assigned different cluster.Instead evaluating clustering quality post-hoc, decided chose algorithm internally identifies appropriate number clusters.\r\nX-means¬†[138] parameter-free adaption popular K-means algorithm incorporates Bayesian information criterion¬†[139] (BIC) find good trade-low total sum squares small number clusters.Let \\(\\mathcal{D}\\) dataset \\(d\\) dimensions let \\(D\\) subset \\(\\mathcal{D}\\), .e., \\(D\\subseteq \\mathcal{D}\\).\r\nK-means clustering \\(D\\) creates set clusters\r\n\\(\\mathcal{C}=\\left\\{C_1,\\ldots,C_k,\\ldots,C_K\\right\\}\\), \\(c_k\\) centroid cluster \\(k\\), \\(r_k\\) number objects \\(D\\) assigned \\(C_k\\) \\(p\\) number free parameters, .e., \\(p = (d+1) \\cdot K\\).\r\nBIC cluster \\(C_k\\) using Schwarz criterion calculated \\[\\begin{equation}\r\n\\text{BIC}(C_k) = \\hat{l}_k(\\mathcal{D}) - \\frac{p_k}{2} \\cdot \\log |\\mathcal{D}| \r\n\\tag{5.1}\r\n\\end{equation}\\]\\(\\hat{l}_k(\\mathcal{D})\\) log-likelihood \\(\\mathcal{D}\\) according \\(C_k\\).\r\npoint probabilities computed \r\n\\[\\begin{equation}\r\n\\hat{P}(x_i)=\\frac{r_{()}}{|\\mathcal{D}|}\\cdot \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}}\\text{exp}\\left(\\frac{1}{2\\hat{\\sigma}^2}||x_i-c_{()}||\\right)\r\n\\tag{5.2}\r\n\\end{equation}\\]\r\n\r\nmaximum likelihood estimate variance (identical spherical Gaussian assumption) \r\n\\[\\begin{equation}\r\n\\hat{\\sigma}^2=\\frac{1}{|\\mathcal{D}|-K}\\sum_{=1}^{|\\mathcal{D}|}\\left(x_i - \\mu_{()}\\right)^2.\r\n\\tag{5.3}\r\n\\end{equation}\\]\r\nlog-likelihood \\(\\mathcal{D}\\) according \\(\\mathcal{C}\\) \r\n\\[\\begin{equation}\r\nl(\\mathcal{D})=\\log\\prod_{=1}^{|\\mathcal{D}|} P(x_i)=\\sum_{=1}^{|\\mathcal{D}|}\\left(\\log \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}} - \\frac{1}{2 \\sigma^2} ||x_i-c_{()}||^2 + \\log \\frac{r_{()}}{|\\mathcal{D}|}  \\right).\r\n\\tag{5.4}\r\n\\end{equation}\\]main steps X-means algorithm summarized Figure¬†5.1.\r\nstart, initial partitioning generated ordinary K-means \\(K\\) = \\(K\\)lower, \\(K\\)lower lower bound number clusters.\r\n, cluster bisected; resulting two child centroids placed opposite direction along randomly chosen vector distance proportional cluster radius.\r\npair child clusters, local K-means clustering \\(K=2\\) run.\r\nBIC score new partitioning exceeds BIC score parental one, child centroids kept, otherwise parent centroid retained.\r\niterative steps repeated cluster whose bisection leads better BIC score, number clusters exceeds optional upper bound \\(K\\)upper.\r\nused R implementation Ishioka¬†[140].\r\nSince aim restrict solution space respect number clusters, set \\(K\\)lower, .e., lowest possible value, set \\(K\\)upper.\r\n\r\ninternal validation, recorded number clusters produced \\(X\\)-means 500 bootstrap samples.\r\nFigure 5.1: Principal steps X-means (simplified). Adapted ¬†[138].\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-visualization","chapter":"5 Visual Identification of Informative Features","heading":"5.4 Visualisation of Phenotypes","text":"Visualizing clusters high-dimensional data challenging.\r\nScatterplot matrices (SPLOMs) can intuitively represent \r\nrelationship pairs features matrix two-dimensional scatterplots¬†[87], [141].\r\nHowever, number features increases, number scatterplots grows quadratically, leading scalability problems overplotting.\r\nSeveral advanced visualization techniques proposed remedy, simply adding transparency colors points sophisticated density contours, hexagon binning, layers aggregated geometric features (Minimal Spanning Trees, Alpha Shape, Convex Hull), animation, combinations several techniques \r\nsplatterplots¬†[142].\r\nHowever, SPLOMs traditional visualization techniques parallel coordinate graphs¬†[143] still suitable low-dimensional data.Dimensionality reduction (DR) techniques often used project original data onto low-dimensional projection allows visualization types used.\r\nIdeally, projection preserves important structures original data, relative pairwise distance, clusters, outliers, correlations.\r\nPrincipal Component Analysis¬†[144] (PCA) seminal DR algorithm generates linear, orthogonal combinations original dimensions.\r\nnew dimension, called principal component, contains loading indicating much variability data covers.\r\nTypically, first two three dimensions carry highest charges selected visualization.\r\nPCA robust outliers capture nonlinear relationships.\r\nMultidimensional scaling¬†[145] (MDS) another early DR technique emphasizes preservation pairwise distances, ie,\r\nObjects close high-dimensional space also close low-dimensional projected space.\r\ncomplex, arbitrarily shaped structures, pairwise distances may subject curse dimensionality, leading poor results.\r\n\\(t\\)-stochastic neighborhood embedding¬†[146] (\\(t\\)-SNE) Uniform Manifold Approximation Projection¬†[147] (UMAP) nonlinear dimensionality reduction methods represent matrix pairwise similarities.\r\nidea obtain global structures clusters local structures distances neighbors.\r\n\\(t\\)-SNE UMAP can produce superior projections compared traditional linear techniques, provided hyperparameters appropriately tuned.\r\nHowever, shortcoming techniques mapping original features quantified.\r\nMoreover, projection applied new observations; instead, new projection must recomputed.\r\nstochasticity, different runs hyperparameters may yield different results.\r\nSince semantics original dimensions lost, decided DR methods suitable application.Discussions tinnitus experts led following cluster visualization requirements:keep original (interpretable) features,represent high-dimensional data dozens features,compactly compare multiple clusters glance,Contrast cluster characteristics overall patient mean.Following requirements, implemented () radial bar chart visualization single cluster (Figure¬†5.2) (b) radial line chart visualization comparing multiple clusters ¬†5.3.\r\nradial bar chart used compare observations assigned single cluster overall population.\r\nmean values features within cluster represented bars arranged radial layout.\r\nbar begins black ‚Äúzero line,‚Äù represents feature means entire population, .e., subjects used clustering.\r\nfeatures standardized (.e., z-scored) prior clustering, bars inclined outside represent feature averages population average bars inclined inside represent feature averages population average.\r\nexample, bar whose top positioned -1 characterizes feature average within cluster 1 standard deviation smaller population average.\r\naddition combination bar height bar direction, within-cluster averages also mapped bar color sequential color gradient dark blue (lower boundary) yellow (population average) light red (upper boundary).\r\nGray error lines top bar represent within-cluster standard error.\r\nallow quick feature localization, features grouped expert-defined categories, displayed inner circle along cluster name number subjects assigned.\r\nFigure 5.2: Radial bar graphs 4 phenotypes (PT). () PT 1 characterizes subgroup lowest health burden. (b) PT 2 includes suffering subjects, mean values psychosomatic somatic characteristics exceed population mean 0.5 standard deviations (SD). (c) PT 3 indicates somatic indicator scores population mean. (d) PT 4 indicates subjects elevated distress scores, including subjective stress perceived quality life. Bars arranged circular layout, height direction bar representing within-cluster feature average gray line centered top bar illustrating 95% confidence interval. characteristics grouped 9 categories defined tinnitus experts. categories shown inside inner circle. See Appendix¬†feature description. Adapted ¬†[148].\r\nradial line chart (5.3) allows comparison clusters single display.\r\nInstead bars, feature mean values represented points.\r\nPoints cluster feature category connected line segments.\r\nPoints line segments colored according respective cluster.\r\nFigure 5.3: Radial line chart phenotypes comparison. Points show within-phenotype feature averages. Points depicting features category connected line segments. Points lines colored cluster. See Figure¬†5.2 description phenotypes Appendix¬†description features. Adapted ¬†[148].\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-app","chapter":"5 Visual Identification of Informative Features","heading":"5.5 Interactive Components and Exploration of Treatment Effects","text":"provide interactive demo cluster solutions visualizations web application3 (Figure¬†5.4).\r\nfollowing interactive components added:\r\nhovering bars, lines, feature labels visualizations opens tooltips additional cluster summaries compact feature descriptions.\r\nClicking feature triggers additional plot showing original (unscaled) distribution selected feature stratified clusters , selected, also treatment.\r\ncontinuous features, semi-transparent boxplots displayed violin plot¬†[149] layers.\r\ncategorical traits, points error lines represent category proportions 95% confidence intervals, respectively.\r\nclustering performed baseline data , included treatment effect indicators allow visual detection potential differences clusters treatment efficacy.\r\nend, extended radial bar chart plotting cluster averages treatment (T0) treatment (T1) adjacent bars.\r\nends pair bars connected line arrowhead pointing T0 score T1 score.\r\nhighlight large treatment effects, connecting lines feature labels colored according relative decrease T0 T1.\r\nGiven user-defined parameter \\(\\Delta\\), .e., minimum relative magnitude difference T0 T1 considered change, elements colored () red T1 score greater T0 score \\(\\Delta\\) , (b) green T0 score less T0 score \\(\\Delta\\) less, (c) black, respectively.\r\nFigure 5.4: User interface interactive demo. Radial bar charts enhanced interactive components: hovering bar feature label opens tooltip additional cluster summaries compact feature description. Clicking feature updates right plot showing distribution selected feature stratified cluster, selected, also treatment. Continuous features shown using semi-transparent boxplots placed violin plot¬†[149] layers whereas nominal features, category proportions alongside 95% confidence intervals displayed points error lines, respectively.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-results","chapter":"5 Visual Identification of Informative Features","heading":"5.6 Results","text":"Four clusters (also referred phenotypes hereafter) represent optimal solution present data according \\(X\\)-means (cf.¬†Figure¬†5.5).\r\nFigure 5.5: Results internal validation. Bars show distribution number clusters generated \\(X\\)-means 500 bootstrap samples. common cluster number 4, occurred 82 samples (16.4%).\r\nPhenotype 1 (PT 1) represents largest subgroup (697 1,228 patients; 56.8%), characterized substantially -average symptom expression tinnitus-related general psychosomatic symptom indices, including affective symptoms, perceived stress, tinnitus-related distress, somatic symptoms, well (-average) quality life internal resources (Figure¬†5.2 ()).\r\ngroup patients potentially help-seeking, presents clinic frequently, wishes participate multimodal treatment, can assumed experience psychological distress strive present unburdened possible.\r\nTherefore, phenotype referred ‚Äúavoidance group‚Äù.\r\nPatients subgroup comparatively high levels education, employment, duration illness psychotherapeutic treatment (Figure¬†5.6).PT 2 included 173 patients (14.1%) reported highest emotional somatic burden among PTs (Figure¬†5.2 (b)).\r\nspecifically, PT 2 represents patient subgroup high\r\npsychosomatic comorbidity therefore referred ‚Äúpsychosomatic group‚Äù.\r\npatient subgroup shows high tinnitus burden addition clinically relevant impairment affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear consistent somatoform expressions distress, including somatic symptoms.\r\nPatients subgroup report greatly reduced quality life coping behaviors, pessimism, less experienced self-efficacy, optimism.\r\nRelative overall population, subgroup higher proportion women, patients live alone, unemployed, overall lower educational status.\r\nPatients cluster also report consulting physicians, taking sick days, using psychotherapy.\r\nPT 2 patients reported tinnitus noise audible throughout head (.e., unilateral) higher percentage groups.PT 3 contains 187 patients (15.2%) characterized -average scores traits measuring somatic complaints near-average scores affective symptoms (Figure¬†5.2 (c)).\r\npain scores SF8_bodilyhealth* SSKAL_painfrequency similar magnitude PT2, patient subgroup referred ‚Äúsomatic group‚Äù.\r\nPT 3 includes oldest subgroup, largest proportion female patients largest reported time since tinnitus onset.contrast PT3, PT 4 (n=171; 13.9%) -average values affective scores, quality--life components, perceived stress (Figure¬†5.2 (d)), e.g., mental component summary score (SF8_mentalcomp*; 0.85) anxious depression score (BSF_anxdepression; 0.79).\r\nTherefore, PT 4 referred ‚Äúdistress group‚Äù.\r\nPT 4 represents youngest 4 subgroups (mean 47.3 years), largest proportion male patients (Figure¬†5.6).\r\nFigure 5.6: Inter-phenotype comparison demographic characteristics. Summaries given means [95% confidence interval] entire population 4 phenotypes. Confidence intervals estimated using nonparametric Basic Bootstrap Sampling¬†[150] 2,000 samples . Kruskal-Wallis test used statistical comparison differences phenotypes continuous features (age), Pearson‚Äôs chi-square test used categorical features (sex). asterisk indicates statistical significance (\\(\\alpha\\) = 0.05). Correction multiple comparisons performed due exploratory nature study. Adapted ¬†[148].\r\nFigure¬†5.7 depicts top-10 features highest average change T1 T0 per cluster.\r\nPT 1 PT 3, BSF_elevatedmood* decreased , namely 0.48 \\(\\pm\\) 0.75 0.65 \\(\\pm\\) 0.85 (\\(Z\\) units), respectively.\r\nPT 2 PT 4, top-ranked features ADSL_depression average difference T1 T0 0.73 \\(\\pm\\) 0.88 0.74 \\(\\pm\\) 0.83, respectively.\r\nSix ten features among top-10 features clusters, including BSF_elevatedmood*, TQ_cogintivedistress, TQ_psychodistress, TQ_emodistress, TQ_distress BSF_fatigue.\r\nFigure 5.7: Cluster-specific top-10 features highest average treatment effect magnitude. Bars depict intra-cluster average differences measurements T1 T0 based standardized values. Lower values represent better treatment effectiveness. symbols right feature names indicate whether feature among top-10 features cluster position . example, character string ‚úó ‚úì ‚úó ‚úì TQ_intrusiveness (ranked 2nd PT 1) means feature among top-10 PT 1 PT 3, PT 2 PT 4.\r\n","code":""},{"path":"phenotypes.html","id":"phenotypes-clinical-interpretation","chapter":"5 Visual Identification of Informative Features","heading":"5.7 Clinical Value","text":"discussed clinical relevance phenotypes three five tinnitus experts co-authored original publication¬†[148].PT 1 (avoidant group) represents half patient sample.\r\nBesides actual tinnitus symptom, patients subgroup reported affective psychosomatic symptoms.\r\nbiased presentation patients (‚Äúeverything fine weren‚Äôt tinnitus‚Äù), clinicians might easily led believe assessment possible factors contributing individual distress unnecessary.\r\nHowever, clinical experience suggests thorough assessment psychosocial stressors.\r\npsychosocial resourcefulness subgroup enables patients seek help quickly solution-oriented manner.\r\nAdequate tinnitus-specific counseling individualized (online) therapy modules include audiological, psychological, relaxation techniques may represent adequate treatment strategy patient subgroup.PT 2 (psychosomatic group) represents 15% patients high tinnitus distress clinically relevant impairment across affective indices, including depression, anxiety, perceived stress.\r\naffective symptoms appear interact strongly somatoform expressions distress, including physical complaints somatic symptoms.\r\nPatients subgroup reported greatly reduced quality life coping behaviors, higher pessimism, lower experienced self-efficacy, optimism.\r\nfrequently asked question whether increased tinnitus-related distress contributes increase depression vice versa.\r\ngroup, depressive anxious symptoms may considered crucial underlying factor overall symptom distress, treatment must initially focus improving mood alleviating depression.\r\n, tinnitus-related distress may need viewed broader context medical psychological contributing factors require patient-specific conceptualization.PT 3 (somatic group) appears represent patient subgroup \r\ncharacterized somatopsychic symptom expression, .e., physical symptoms may reflect stress /underlying medical conditions.\r\nmeet needs patient subgroup, multimodal interventions may include proportion body-oriented procedures relaxation exercises physiotherapy, effects interpreted terms direct indirect psychological effects (e.g., increased well-affection others).Patients PT 4 (stress group) reported higher average perceived stress, accompanied physical exhaustion anxious-depressed mood.\r\ngroup likely include younger, employed, male patients reported chronic distress may susceptible burnout syndrome subjectively reduced mental performance (‚Äúhamster wheel‚Äù), used describe life situations even absence tinnitus distress.\r\nMultimodal therapy initially focus stress regulation techniques, including relaxation individually tailored behavior modification approaches.\r\nSimilar PT 2, high psychosomatic burden, patients PT 4 also benefit longer psychotherapeutic multimodal treatment procedures (inpatient rehabilitative).","code":""},{"path":"phenotypes.html","id":"phenotypes-discussion","chapter":"5 Visual Identification of Informative Features","heading":"5.8 Discussion","text":"Although dataset optimal number clusters four, expect number may different different sample tinnitus patients even clustering algorithm.\r\nFigure¬†5.5 shows high variance terms number clusters returned \\(X\\)-means different bootstrap samples.\r\nConsidering slightly lower occurrence 5 6 clusters, clustering result certainly set stone.Nonetheless, comparison clusters showed questionnaires characteristics differed considerably patient phenotypes.\r\nparticular, patient subgroups differed substantially terms coping behaviors, stress, tinnitus burden, perceived pain, discomfort, perception quality life.\r\ncontrast, patients appear differ respect localization noise.\r\nRegarding separability phenotypes, mostly high correlations feature within category suggest phenotyping also possible fewer questionnaires, especially since questionnaires overlap semantically, e.g.¬†PHQK_depression, ISR_depression, ADSL_depression, etc.Without ‚Äúground truth‚Äù different sets available measurements, difficult compare results similar studies.\r\ngreatest strength workflow inclusion wide range self-report questionnaire assessments.\r\nstudies also used audiometry¬†[29], [30] cardiac imaging data¬†[136].\r\nNevertheless, PT 2 (psychosomatic distress group) seems associated ‚Äúconstant distressing tinnitus‚Äù subgroup reported Tyler et al.¬†[30], mean scores tinnitus-related health distress much larger subgroups.\r\nClearly, selection meaningful set characteristics central effectiveness cluster analysis.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nclosest radial bar chart visualization radar chart proposed Schlee et al.¬†[137], whose solution tends overplot two subsets need displayed simultaneously.\r\nTherefore, fill areas spanned connected points color \r\navoid one polygon completely overlaps another.\r\nFurthermore, inferences radar map¬†[137] depend heavily arrangement features, since main criterion comparison shapes polygons.\r\nsolution compute arrangement yields areas achieve maximum mean area difference subgroups minimum area variance within subgroups partially solves problem, since still moderate number 20 features can represented.\r\nchose organize 64 features according expert-determined categories, e.g., quality life, makes easier find features compare similar features.addition, visualization tinnitus-specific can used display compact summary condition subset index symptoms.\r\nWhether visualizations adopted clinicians guide appropriate tinnitus management strategies adopted remains tested.\r\npreliminary user study, clinicians suggested graphical summaries possible patient subtypes ease challenge assigning appropriate treatment strategy specific combinations symptom presentations.\r\nexcluding patients complete questionnaires T0, may selection bias.\r\nPossible reasons noncompletion include unfamiliarity technical equipment used record item responses, loss motivation due relatively large number questionnaires, collisions baseline studies laboratory.\r\nNevertheless, analysis 15 questionnaires led insights \r\ncontributions questionnaires phenotyping, possibly allowing reduction number questionnaires.\r\nresults reflect snapshot patients‚Äô situation baseline, patient may transition one phenotype another later stages life, depending tinnitus management.\r\nTherefore, next step investigate effects treatment phenotypes detail find whether patient phenotypes benefit others.","code":""},{"path":"phenotypes.html","id":"phenotypes-conclusions","chapter":"5 Visual Identification of Informative Features","heading":"5.9 Conclusions","text":"presented workflow find distinct tinnitus phenotypes clustering using algorithm internally determines optimal number clusters.\r\nvisualized characteristics phenotype differences multiple phenotypes using radial bar line graphs.\r\nalso provided web application phenotypes can explored treatment effects.\r\nknowledge, first approach combines clustering tinnitus patients comprehensive visualization subgroups high-dimensional data interactive components exploration patterns treatment effects.","code":""},{"path":"evo.html","id":"evo","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6 Constructing Evolution Features to Capture Study Participant Change over Time","text":"chapter partly based :Uli Niemann, Tommy Hielscher, Myra Spiliopoulou, Henry V√∂lzke, Jens-Peter K√ºhn. ‚ÄúCan classify participants longitudinal epidemiological study previous evolution?‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2015, pp.¬†121-126. DOI: 10.1109/CBMS.2015.12.begin chapter work related construction temporal representations medical data (Section¬†6.1).\r\nSection¬†6.2 present evolution feature framework including full workflow encompasses steps extraction evolution features, dealing class imbalance feature selection. Subsequently, describe evaluation setup (Section¬†6.3) present results (Section¬†6.4).\r\nFinally, Section¬†6.5 conclude chapter give answers aforementioned research questions.","code":""},{"path":"evo.html","id":"brief-chapter-summary-3","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"Brief Chapter Summary","text":"present framework cohort analysis longitudinal cohort studies constructs ‚Äúevolution features‚Äù latent temporal information describing change cohort participants time.\r\nshow exploiting novel features improves generalization performance classification models.\r\nreport results SHIP outcome ‚Äúfatty liver.‚Äù","code":""},{"path":"evo.html","id":"evo-intro","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.1 Motivation and Comparison to Related Work","text":"Epidemiological studies serve basis identification risk factors associated medical condition.\r\nMachine learning still rather little used epidemiology, mainly due hypothesis-driven nature research.\r\nHowever, examples machine learning applications identification health failure subtypes¬†[151] discovery factors (including biomarkers) modulate medical outcome¬†[152], [153].\r\nlongitudinal cohort studies measurements performed multiple study waves, hence researchers obtain access sequences recordings.\r\ncontext machine learning, extracting leveraging inherent temporal information sequences may increase model performance thus, understanding medical condition interest.clinical applications temporal information often exploited, predominantly analysis patient records.\r\nexample, Pechenizkiy et al.¬†[154] analyzed streams recordings predict patient rehospitalization health failure treatment.\r\nSun et al.¬†[155] computed similarity streams patients patient monitoring data.\r\nCombi et al.¬†[156] reported streams life signals, particular temporal analysis timestamped medical records hospital patients.\r\nHowever, participants epidemiological, population-based study hospital patients ‚Äì random sample studied population, often skewed class distribution.\r\nlongitudinal study kind, recordings cohort member made moment.\r\nHielscher et al.¬†[157] presented feature engineering approach extract temporal information multiple, patient recordings longitudinal epidemiological study.\r\nFirst, assessment clusters feature-value sequences associated target variable found.\r\nAfterwards, original sequence features used conjunction classification.\r\nHielscher et al.¬†[157] showed classification performance increases features temporal information incorporated feature space.\r\nInstead modeling individual change measurement values, approach involves deriving multivariate change descriptors.\r\n\r\n\r\n\r\n\r\nPatient evolution clustering studied Siddiqui et al. [158] proposed method predicts evolution patient timestamped data clustering similarity predicting cluster movement multi-dimensional space. However, patient data considered [158] labeled moment.\r\nworkflow combines labeled unlabeled timestamped data longitudinal study improve classification performance skewed data.\r\ntarget variable, study multi-factorial disorder hepatic steatosis (fatty liver) sample participants longitudinal population-based ‚ÄúStudy Health Pomerania‚Äù (SHIP)¬†[5], see Section¬†2.6.1.\r\nSHIP cohort, assessments (interviews, medical tests, etc.) recorded three moments (SHIP-0, SHIP-1 SHIP-2), ca. 5 years apart.\r\nTemporal information often used analyzing patient data hospital, time granularity different.\r\nexample, intensive care unit, timestamped data collected fast pace, .e., every minute even every second.\r\ncontrast, participants longitudinal epidemiological study monitored period months even years.\r\nimplies measurements assessment epidemiological dataset possibly far apart.\r\nlarge time span two consecutive recordings complicates application methods designed data arrive higher frequency.\r\nexample, participant may exhibit alcohol abuse become pregnant, stop smoking start , take antibiotics affect liver, experience lifestyle changes turn medical recordings taken 5 years ago irrelevant learning participant‚Äôs current health state.\r\nAnother patient may changes lifestyle illnesses, past data reflect aging.\r\nchallenge recordings SHIP-2 labeled.\r\nreliable estimate fat accumulation liver computed magnetic resonance tomography images.\r\nSHIP-0, MRT unavailable.\r\nInstead, liver fat accumulation derived ultrasound ‚Äì procedure lower clinical accuracy.\r\nSHIP-1, calculation omitted altogether.\r\nconsequence, given SHIP participant class label available SHIP-2, label SHIP-1 partially reliable indicator SHIP-0.\r\nSince hepatic steatosis reversible disorder, label imputation ‚Äì means growth model¬†[159] ‚Äì possible; participant evolution must learned one moment labeled data.address challenges follows.\r\nFirst, group study participants moment similarity, thus building clusters cohort members similar recordings one three moments.\r\n, connect clusters across time, thus capturing transition cluster one study wave next.\r\ntransitions reflect evolution subgroups, individuals.\r\nHence, next single labeled recording per cohort participant, also exploit earlier, unlabeled recordings, description cluster assigned information clusters evolve time.\r\nshow new, augmented dataset, combining labeled unlabeled data individuals subgroups, improves classification delivers additional insights factors associated hepatic steatosis.","code":""},{"path":"evo.html","id":"evo-concept","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2 Evolution Features","text":"leverage latent temporal information longitudinal cohort study dataset extracting informative features based individual change participants transition respective clusters time.\r\npurpose, exploit similarity among participants moment, surrogate labels available first two moments, assuming similar participants evolve similarly.\r\n\r\ncall new features ‚Äúevolution features‚Äù.\r\napproach illustrated Figure 6.1¬†().\r\nmonitor individual change participants across study waves, trace change clusters separately, extract new features (labeled unlabeled data) augment original data space new descriptors change.\r\ncomplete classification workflow depicted Figure 6.1¬†(b).\r\nFigure 6.1: Concept evolution feature extraction classification performance improvement. () Clustering longitudinal cohort data subsequent generation evolution features change individuals (red) whole clusters (green). (b) Overview classification workflow. TODO: explain parentheses\r\nfollowing, describe clustering study participants (Section¬†6.2.1), generation evolution features (Section¬†6.2.2), strategy undersampling majority class balance class distribution (Section¬†6.2.3) feature selection strategy extract subset informative features input classification (Section¬†6.2.4).","code":""},{"path":"evo.html","id":"evo-concept-clustering","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.1 Clustering","text":"clustering, prefer density-based clustering partitional algorithms (like K-means), data contain extreme cases, clusters may arbitrarily shaped different sizes, determine number advance.\r\nmoment \\(t\\), run DBSCAN¬†[160] algorithm cluster set \\(Z(t)\\) recordings cohort members observed \\(t\\).\r\nparticipant \\(x\\), \\(v(x,f,t)\\) denotes value \\(x\\) feature \\(f\\\\) feature-set \\(F\\) \\(t\\), \\(obs(x,F,t)\\) set feature recordings \\(x\\) \\(t\\) (cf.¬†notation Table¬†6.1).\r\nTable 6.1: Symbols basic functions.\r\nDistance function.\r\ndistance participants \\(x,z\\) \\(t\\), use adjusted heterogeneous Euclidean overlap metric¬†[157], [161], weights difference two values \\(x,z\\) feature \\(f\\) feature‚Äôs information gain \\(G(f)\\), scaled largest observed value \\(G^*\\), defined :\r\n\\[\\begin{equation}\r\nd(x,z,t)=\\sqrt{\\sum_{f \\F} \\left(\\frac{G(f)}{G^*}\\cdot \\delta\\left(v(x,f,t),v(z,f,t)\\right)\\right)^2}.\r\n\\tag{6.1}\r\n\\end{equation}\\]\r\ncontinuous features, \\(\\delta(,b)\\) min-max-scaled difference values \\(,b\\), .e., \\((-b)/(\\max(f)-\\min(f))\\).\r\nnominal features, \\(\\delta(,b)\\) 0 \\(=b\\), 1 otherwise.DBSCAN Parameter setting.\r\nDBSCAN relies two parameters: radius \\(eps\\) neighborhood around data point, minimum number \\(minPts\\) neighbors point core point.\r\nuse ‚Äúelbow‚Äù heuristic ¬†[160] determines suitable \\(eps\\) value given \\(minPts\\) value, illustrated Figure¬†6.2.\r\nspecifically, define parameter \\(k\\), compute \\(x\\{}Z(t)\\) distance k-dist\\((x,k)\\) k-th nearest neighbor.\r\nsort distances, draw k-dist\\((x,k)\\) graph g span line l connecting smallest k-dist() value largest one.\r\n, set \\(eps\\) k-dist value maximum distance g l.\r\nFigure 6.2: Setting \\(eps\\) based k-dist graph given \\(minPts\\). k-dist graph g depicts sorted distances points‚Äô k-th next neighbors. suitable \\(eps_{opt}\\) can identified position maximum distance k-dist line l connects first last point g. DBSCAN clustering \\(minPts\\) = k, \\(eps_{opt}\\), points k-dist \\(\\leq\\) \\(eps_{opt}\\) become core points, else border noise points.\r\n","code":""},{"path":"evo.html","id":"evo-concept-evo-features","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.2 Constructing Evolution Features","text":"Table¬†6.2 provides description evolution features.\r\ncohort member \\(x\\) moment \\(t\\), record cluster containing \\(x\\) (feature 1 Table¬†6.2),\r\n(2) distance \\(x\\) cluster‚Äôs centroid,\r\n(3) fraction positively labeled participants among \\(k\\) nearest neighbors \\(x\\),\r\n(4) (graph-based) cohesion¬†[162] (5) Silhouette coefficient¬†[162] \\(x\\), (6) (graph-based) separation¬†[162] \\(x\\) cohort participants outside cluster.\r\ncompute difference cohesion, silhouette separation values \\(t\\) later moments \\(\\{t' \\T|t'>t\\}\\) (7-9), also check much values metrics change \\(x\\) moves \\(c(x,t)\\) \\(c(x,t')\\) (10-12).\r\nrecord whether \\(x\\) outlier, .e., DBSCAN noise point moment (13).\r\n\\(t\\) \\(\\{t' \\T|t'>t\\}\\), compute fraction cohort members cluster \\(x\\) \\(t\\) \\(t'\\) (14), fraction common \\(k\\) nearest neighbors (15), change distance \\(x\\) centroid \\(t\\), \\(t\\) \\(t'\\) (16).\r\nrecord changes sequence values feature, including real (17), absolute (18) relative (19) differences values two moments.\r\nmeasure cluster shrinks/grows \\(t\\) \\(t'\\) (20), much members move (average) closer far apart previous positions (21-23).\r\nway, extract information evolution participants, distinguishing among evolve smoothly switch among clusters.\r\ntransfer information evolution features, thus enriching feature space information unlabeled moments.\r\nTable 6.2: Overview extracted features. first group features () comprises cluster membership aggregated distance information participant moment; feature group II changes participant‚Äôs position (hyperspace) relative cluster closest neighbors; feature group III captures changes values participant‚Äôs recordings; feature group IV refers changes clusters.\r\n","code":""},{"path":"evo.html","id":"evo-concept-undersampling","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.3 Undersampling","text":"imbalanced data, feature selection classification often biased favor majority class¬†[163].\r\navoid problem, prior application CFS undersample majority class generate balanced data set select features informative respect classes.\r\n\r\n\r\n\r\n\r\n\r\n\r\n","code":""},{"path":"evo.html","id":"evo-concept-feature-selection","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.2.4 Feature Selection","text":"use feature selection method Hielscher et al.¬†[164] follows.\r\nFirst, invoke correlation-based feature selection¬†[166] (CFS), builds feature set iteratively inserting feature adds ‚Äúmerit‚Äù .\r\nmerit \\(M_F\\) feature set \\(F\\) computed calculating information gain pair features \\(F\\) (lower gain corresponds low correlation thus preferred) feature \\(F\\) towards target variable (higher gain better).\r\nContinuous features first discretized entropy-based method Hielscher et al.¬†[125].\r\ndiscretize feature selection; clustering classification use original values.shown Figure¬†6.1, perform feature selection twice.\r\nfirst time, consider features recorded moments.\r\nessential evolution tracing: can compute distances objects clusters located topological space.\r\ngenerating evolution features, build complete set features, also considering recorded moment.\r\nset perform feature selection , discard unpredictive (original evolution) features.\r\nfinal feature set used classification.","code":""},{"path":"evo.html","id":"evo-evaluation","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.3 Evaluation Setup","text":"evaluate workflow 10-fold cross-validation four --shelf classification algorithms: random forest¬†[171] (RF), C4.5 decision tree¬†[172], Na√Øve Bayes¬†(NB) k-nearest neighbor (kNN).\r\nNext, compare generalization performance algorithm used alone (baseline variant) vs.¬†incorporated workflow (worflow-enhanced variant).\r\n, study impact different combinations three workflow components undersampling (U), feature selection (F) incorporation generated evolution features (G).\r\nshown Table¬†6.3, Baseline simply invokes classification algorithm; use classification algorithms achieves highest F-measure scores.\r\nvariant U-G performs undersampling uses generated evolution features classification.\r\nSince undersample feature selection build classification models original dataset, U-G identical --G U-- identical Baseline variant, omit explicityl list U-G U--.main parameter \\(k\\) number neighbors data point: set \\(minPts\\) = \\(k\\) use \\(k\\) derive values DBSCAN parameter \\(eps\\) (cf.¬†Section¬†6.2.1) parameters features same_kNN_\\(k\\)_t_1_t_2 fraction_Of_POS_kNN_\\(k\\)_t (cf.¬†Table¬†6.2).\r\n, number nearest neighbors k-NN classification algorithm also set \\(k\\).\r\nvary \\(k\\) measure impact classification performance.Following findings Chapters¬†?? ¬†?? differences female male participants respect outcome, run experiments whole dataset (Partitionall), partitions female (Partitionf) male (Partitionm) participants.\r\nFinally, list important features found Partitionall two subsets.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 6.3: Workflow variants. UFG complete workflow.\r\n","code":""},{"path":"evo.html","id":"evo-results","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.4 Results","text":"Figure¬†6.3 shows sensitivity (left), specificity (center) F-measure (right) simple classifiers (gray curves) workflow-enhanced counterparts (line style, colored), different \\(k\\).\r\n\r\n\r\nOverall, workflow-enhanced variant outperforms simple counterpart respect sensitivity F-measure, outperforms performs slightly worse respect specificity.\r\nworkflow-enhanced Naive Bayes performs best respect sensitivity \\(k\\) best \\(k=31\\).\r\nDecision trees exhibit highest F-measure, improvements sensitivity F-Measure compared simple variant, albeit specificity sligthly worse; improvements less large \\(k\\).\r\nRandom Forests benefit workflow, absolute improvement F-measure 30% (green vs gray ‚Äú+‚Äù curves right part Figure¬†6.3).\r\nOne explanation rather poor sensitivity simple RF variant large number trees (100) learned data samples containing positive examples, RF trapped many majority class examples.\r\nconsistent specificity curve (almost straight line around 95%) simple RF, F-measure slightly 40%.\r\nworkflow improves RF sensitivity (63%) F-measure (65%), specificity remains high (90%).\r\nOverall, impact \\(k\\) three measures limited algorithms except workflow-enhanced baseline k-NN naturally affected stronger value \\(k\\) algorithm.\r\nTherefore, workflow-enhanced variants outperform simple counterparts terms sensitivity F-measure. algorithms, workflow prevents overfitting negative class.\r\nFigure 6.3: Comparison classification performance workflow baseline. Sensitivity (true positive rate), specificity (false positive rate) F-measure scores different classifiers varying number k neighbors cohort member impacts clustering result. classifier two performance curves shown: colored one workflow-enhanced version gray one baseline counterpart. Higher values better measures. [173].\r\nworkflow component-specific analysis results Figure¬†6.4 show complete workflow UFG variants UF- --G outperform --- sensitivity F-measure.\r\nvariants -F- -FG perform well regarding specificity, suggests feature selection may fruitful without undersampling datasets class imbalance.\r\nFigure 6.4: Comparison classification performance workflow components. Sensitivity, specificity F-measure scores workflow variant baseline using decision tree learning. [173].\r\nImportant features. performance workflow variants include feature selection indicates small number features sufficient class separation.\r\nHereafter, partition report evolution features selected classification among top-15 features according information gain.\r\nFigure¬†6.5 shows PartitionAll 3 15 features generated evolution features.\r\nboxplots () (c) Figure¬†6.5 refer differences values recorded two moments.\r\nfeature separationDelta_g_1_2 measures difference cluster separation participant based cluster assignment moment 1 2, corresponds entry #9 Table¬†6.2.\r\nParticipants positive exhibit higher median separationDelta_g_1_2 participants negative class indicating clusters harboring mostly positive participants cover larger, sparse areas.\r\nfeature relative_Difference_som_huef_g_0_1 (#19) quantifies difference participant‚Äôs hip circumference SHIP-0 SHIP-1, relative value SHIP-0.\r\naverage study participants classes lose weight grow older, negative participants reduce weight compared positive participants (cf.¬†Figure 6.5¬†(c)), general reflects differences life styles.\r\nmosaic chart Figure 6.5¬†(b) feature fraction_of_Positives_kNN_1_g_2 (#3) indicates nearest neighbor participant fatty liver likely also exhibit disorder non- fatty liver participant.\r\nFigure 6.5: Visualization selected evolution features contribute class separation whole dataset PartitionAll.\r\nPartitionF, 5 top-15 features evolution features, cf.¬†Figure¬†6.6.\r\nCompared female participants without disorder, female subjects fatty liver exhibit larger distance centroid cluster SHIP-1 (#2), lower silhouette coefficient SHIP-1 (#5), higher difference waist circumference SHIP-0 SHIP-2 (#19), lower relative difference serum triglycerides concentration SHIP-0 SHIP-1 (#19).\r\nFigure 6.6: Visualization selected evolution features contribute class separation PartitionF.\r\nPartitionM, 2 top-15 features evolution features (Figure¬†6.7), including relative difference waist circumference SHIP-1 SHIP-2 (#19) difference separation SHIP-0 SHIP-1 (#6).\r\nfeatures, patients exhibiting disorder greater values.\r\nFigure 6.7: Visualization selected evolution features contribute class separation PartitionM.\r\n","code":""},{"path":"evo.html","id":"evo-conclusions","chapter":"6 Constructing Evolution Features to Capture Study Participant Change over Time","heading":"6.5 Conclusions from Exploiting Study Participant Evolution","text":"TODO:LINK RESEARCH QUESTIONS\r\nTODO: say original submission, also report static featuresWe proposed workflow classification longitudinal cohort study data exploits inherent temporal information clustering cohort participants moment, linking clusters tracing participant evolution moments study.\r\nclusters transitions extract evolution features, added feature space subsequently used classification.\r\nworkflow improves generalization performance respect sensitivity F-measure scores.\r\ngenerated evolution features contribute improvement, even used alone without undersampling skewed data.\r\nshow change values somatographic variables cluster quality indices time predictive.","code":""},{"path":"diabfoot.html","id":"diabfoot","chapter":"7 Feature Extraction From Short Temporal Sequences for Clustering","heading":"7 Feature Extraction From Short Temporal Sequences for Clustering","text":"chapter partly based :Uli Niemann, Myra Spiliopoulou, Fred Samland, Thorsten Szczepanski, Jens Gr√ºtzner, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. ‚ÄúLearning Pressure Patterns Patients Diabetic Foot Syndrome.‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2016, pp.¬†54‚Äì59. DOI: 10.1109/CBMS.2016.31.Uli Niemann, Myra Spiliopoulou, Thorsten Szczepanski, Fred Samland, Jens Gr√ºtzner, Dominik Senk, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, Peter R. Mertens. ‚ÄúComparative Clustering Plantar Pressure Distributions Diabetics Polyneuropathy May Applied Reveal Inappropriate Biomechanical Stress.‚Äù : PLOS ONE 11.8 (2016), pp.¬†1‚Äì12. DOI: 10.1371/journal.pone.0161326.Uli Niemann, Myra Spiliopoulou, Jan Malanowski, Juliane Kellersmann, Thorsten Szczepanski, Silke Klose, Eirini Dedonaki, Isabell Walter, Antao Ming, Peter R. Mertens. ‚ÄúPlantar temperatures stance position: comparative study healthy volunteers diabetes patients diagnosed sensoric neuropathy.‚Äù : EBioMedicine 54 (2020), p.¬†102712. DOI: 10.1016/j.ebiom.2020.102712.written","code":""},{"path":"diabfoot.html","id":"brief-chapter-summary-4","chapter":"7 Feature Extraction From Short Temporal Sequences for Clustering","heading":"Brief Chapter Summary","text":"present approach build representations short temporal sequences via clustering example pressure- posture-dependent plantar temperature pressure patients diabetic foot syndrome.","code":""},{"path":"iml.html","id":"iml","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8 Post-Hoc Interpretation of Classification Models","text":"chapter partly based :Uli Niemann, Philipp Berg, Annika Niemann, Oliver Beuing, Bernhard Preim, Myra Spiliopoulou, Sylvia Saalfeld. ‚ÄúRupture Status Classification Intracranial Aneurysms Using Morphological Parameters.‚Äù : Proc. IEEE Int. Symposium Computer-Based Medical Systems (CBMS). 2018, pp.¬†48-53.\r\nDOI: 10.1109/CBMS.2018.00016.Uli Niemann, Benjamin Boecking, Petra Brueggemann, Wilhelm Mebus, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúTinnitus-related distress multimodal treatment can characterized using key subset baseline variables.‚Äù : PLOS ONE 15.1 (2020), pp.¬†1-18.\r\nDOI: 10.1371/journal.pone.0228037.Uli Niemann, Petra Brueggemann, Benjamin Boecking, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúDevelopment internal validation depression severity prediction model tinnitus patients based questionnaire responses socio-demographics.‚Äù : Scientific Reports 10.1 (2020), p.4664.\r\nDOI: 10.1038/s41598-020-61593-z.medical applications, understanding clearly communicating results machine learning model critical deriving actionable knowledge can ultimately used improve disease prevention, diagnosis, treatment.\r\n\r\n\r\nObtaining results easily understood data scientists medical experts helps formulate new hypotheses regarding relationship potential risk protective factors target; significance relationships can tested follow-studies.\r\nCurrent state---art machine learning algorithms produce models superior performance compared simpler interpretable models, decision trees, rule lists, linear regression fits.\r\nHowever, opaque black boxes involve many complex feature interactions decisions, nonlinear, often difficult explain understandable way.\r\nArising need provide understandable insights otherwise opaque models, interpretability community machine learning gained traction goal resolving dilemma choosing moderately accurate interpretable models highly accurate opaque black-box models.chapter, describe comprehensive data analysis workflow high-dimensional medical data includes classification, feature elimination, post-learning analysis steps addition application-specific preprocessing steps.\r\nvariety learners used classification, simple interpretable models complex black boxes.\r\nbest classifier, explore different post-hoc interpretation methods derive model-, observation-, subpopulation-level insights.\r\nreport results evaluate approach based via experiments three applications.\r\nchapter organized follows.\r\nSection¬†8.1, describe reasons using interpretable machine learning methods provide methodological underpinnings selection pioneering methods.\r\nSubsequently, present components mining workflow Section¬†8.2, includes correlational analysis, image preprocessing, feature selection, classification high-dimensional medical data, hyperparameter tuning, model evaluation approach putting interpretation methods use.\r\nSection¬†8.3, report findings three applications: prediction () tinnitus-related distress (ii) depression treatment tinnitus patients, well (iii) rupture status classification intracranial aneurysms.\r\ndiscuss results Section¬†8.4 conclude chapter Section¬†8.5.","code":""},{"path":"iml.html","id":"brief-chapter-summary-5","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"Brief Chapter Summary","text":"present machine learning workflow combines classification high-dimensional medical data model explanation using post-hoc interpretation methods.\r\nend, use Shapely value explanations (SHAP), LASSO coefficients, partial dependency graphs.\r\napproach provides statistics visualizations representing global feature importance, instance-individual feature importance, subpopulation-specific feature importance, help illuminate complex black-box machine learning models.\r\nreport results three applications: () tinnitus-related distress tinnitus patients, (ii) depressivity tinnitus patients, (iii) rupture risk intracranial aneurysms.","code":""},{"path":"iml.html","id":"iml-motivation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.1 Motivation and Methodological Underpinnings","text":"Current state---art machine learning algorithms, gradient boosting¬†[81] tabular data deep learning¬†[174] unstructured data (images, videos, audio recordings), widely used support medical decision-making.\r\nmethods produce models typically achieve better predictive performance simpler models decision trees, rule lists, linear regression fits.\r\nHowever, also complex, making difficult understand prediction made.\r\nThus, practitioners may face dilemma choosing either opaque black-box model high predictive power simple, less accurate model can least explained domain expert.\r\nEspecially high-risk domains healthcare, misconceptions can serious consequences, ability explain reasoning model highly desirable, essential, property decision-support system¬†[175], [176].result, methods explain predictions complex machine learning models attracted increasing attention recent years¬†[177], [178].\r\nExisting methods classified according different criteria¬†[176].\r\nexample, distinction made intrinsically interpretable models post-hoc explanations.\r\nformer often entails limiting model complexity choosing algorithms produce transparent models, decision trees linear regression models.\r\nDecision trees, example, can intuitively visualized node-link diagrams.\r\nFeatures split conditions near tree root generally higher impact predictions features occurring lower tree levels within leaf nodes.\r\nQuantitative measures calculate overall importance feature decrease impurity variance nodes feature occurs compared parent nodes¬†[179].\r\nFurthermore, data partitions created decision tree can described understandable conditions ‚Äúbody mass index > 30,‚Äù decision paths root leaf nodes provide insights feature interactions.\r\naddition, can used contrasting predictions individual instances, e.g., considering alternative feature values effects model prediction (‚Äúpatient body mass index 25 instead 30, difference terms prediction .‚Äù).\r\nDisadvantages decision trees able capture linear, non-axis-parallel relationships predictors response, can unstable respect small changes training data¬†[180].\r\nTherefore, may unsuitable complex learning tasks.sophisticated model trained instead, post-hoc methods can applied examine model training.\r\noutput methods can feature summary statistics, model internals, individual observations, feature summary visualization¬†[176].\r\ngeneral, feature summary statistics individual scores express overall importance feature model prediction strength feature‚Äôs interaction features.\r\nExamples model internals coefficients linear model weight vectors neural network.\r\nIndividual observations can describe representatives (prototypes) observation subgroups model provides consistent predictions subgroup members.\r\nIndividual observations can also used provide counterfactual explanations, e.g., determine minimum change cause model predict different class particular observation interest.Partial dependence plots. Visualizations feature summaries typically depict trends relationship subset features predicted response, often form curves surface plots.\r\npartial dependence plot¬†[81] (PDP) widely used tool visually depicting marginal effect one predictors predicted response model.\r\nexample, PDP Figure 8.1¬†() shows roughly S-shaped relationship values predictor values response estimated model.\r\n\r\nFigure 8.1: Illustrations partial dependence plot (PDP) individual conditional expectation (ICE) plot artificial data. () PDP predictor artificial dataset. Points represent sample predictor distribution. (b) PDP augmented ICE curves. 2 distinct subsets observations PD different upper half predictor distribution.\r\nLet \\(\\zeta\\) classification model (general function returns single real value) let \\(F=Q\\cup R\\) total set features, \\(Q\\) chosen subset features \\(R\\) complement subset.\r\npartial dependence \\(PD\\) model \\(\\zeta\\) \\(Q\\) can represented \\[\\begin{equation}\r\nPD(Q)=\\mathbb{E}_{R}\\left[\\zeta(X)\\right]=\\int\\zeta(Q,R)p_R(R)\\,dR.\r\n\\tag{8.1}\r\n\\end{equation}\\], \\(p_R(R)\\) marginal probability density \\(R\\), .e., \\(p_R(R)=\\int p(X)\\,dQ\\), \\(p(X)\\) joint density dataset \\(X\\).\r\ncomplement marginal density \\(p_R(R)\\) estimated training data, \\(PD\\) can approximated \\[\\begin{equation}\r\nPD(Q)=\\frac{1}{N}\\sum_{=1}^N \\zeta(Q,R_{})\r\n\\tag{8.2}\r\n\\end{equation}\\]\\(R_i\\) actual values complementary features observation \\(\\), \\(N\\) total number observations training data.\r\ncardinality \\(Q\\) usually chosen either equal 1 2.\r\nresults visualized line chart (\\(|Q|=1\\)) contour chart (\\(|Q|=2\\)).\r\npractice, random sample often drawn \\(Q\\) reduce computation time.averaging across observations removes information variability, PD curves can obscure potentially distinct observation subgroups substantially different effects predictors model output.\r\nremedy, Goldstein et al.¬†[82] proposed individual conditional expectation (ICE) plots showing curve observation.\r\nFigure 8.1¬†(b) illustrates example small number observations (black curves) differ rest PD constant second half predictor distribution.\r\nLIME. Another criterion distinguishing model interpretation methods whether explanations global local, .e., whether explanations apply observations one small number selected observations.\r\nLocal Interpretable Model-Agnostic Explanations¬†[181] (LIME) popular local post-hoc interpretation method.\r\nmain assumption LIME complex model linear local scale¬†[181].\r\nThus, explain predictions black-box model particular observation interest \\(\\), LIME generates surrogate model intrinsically interpretable whose predictions similar predictions black-box model proximity \\(\\).\r\nmain ideas LIME shown Figure¬†8.2.\r\nFigure 8.2¬†() shows decision boundary black-box model.\r\nSince form nonlinear decision boundary quite complex, model predictions explained simple terms.\r\nLIME attempts approximate behavior black-box model creating linear surrogate model performs well, especially near user-selected instance interest.\r\nend, perturbed training set created repeatedly randomly changing values instance interest.\r\nFigure 8.2¬†(b) shows instance interest perturbed instances, glyph size represents proximity instance interest.linear surrogate model trained dataset, observation weights proportional distance instance interest.\r\nFigure 8.2¬†(b), decision boundary surrogate model shown dashed line.\r\nFinally, model internals displayed user explanation, coefficients logistic regression model.\r\nFigure 8.2¬†(c) shows feature importance ranking, bar height represents model coefficient feature.\r\nLIME provides intuitive interpretations applicable tabular non-tabular data, several design decisions make hyperparameters tune, including neighborhood kernel width, surrogate model family, feature selection mechanism, number features considered surrogate model, among others.\r\nstability results LIME questioned¬†[182], [183].\r\nFigure 8.2: Illustration LIME‚Äôs main ideas. () data set two-class problem, represented two-dimensional scatterplot simplicity. nonlinear decision boundary black-box model easily explained. (b) LIME aims approximate predictions black-box model vicinity instance interest intrinsically interpretable model, logistic regression model. dashed line shows linear decision boundary surrogate model. (c) feature importance ranking can derived model coefficients.\r\nModel-specific interpretation methods limited specific model families, model-agnostic interpretation methods can applied type model.\r\nModel-specific methods based model internals widely used neural networks¬†[184], e.g.¬†layered relevance propagation¬†[185], explicitly uses layered structure neural network infer explanations.\r\ncontrast, model-agnostic methods decoupled actual learning process access algorithmic internals.\r\nSince consider output models, .e., predictions, model-agnostic methods also post-hoc.\r\nexample, LIME representative model-agnostic, post-hoc interpretability method.SHAP. Closely related LIME Shapley Additive Explanations¬†[186] (SHAP) framework, derives additive feature attributions predictions model.\r\nSHAP based Shapley values¬†[187]‚Äì[189], originally developed game theory.\r\nterm ‚Äúadditive‚Äù denotes given observation, model output equal sum attributions feature.\r\nspecifically, observation \\(x\\), model output \\(\\zeta(x)\\) \r\n\\[\\begin{equation}\r\n\\zeta(x)=\\phi(\\zeta,x)_0 + \\sum_{j=1}^M \\phi(\\zeta,x)_j\r\n\\tag{8.3}\r\n\\end{equation}\\]\\(\\phi(\\zeta,x)_0=E(\\zeta(x))\\) expected value model training data, \\(\\phi(\\zeta,x)_j\\) attribution feature \\(j\\) \\(x\\), \\(M\\) total number features.\r\n, combination feature \\(j\\) observation \\(x\\), Shapely value \\(\\phi\\) represents impact predictor added, aggregated weighted average possible feature subsets \\(S\\subseteq S_{}\\):\\[\\begin{equation}\r\n\\phi_{j}(x)=\\sum_{S\\subseteq S_{}\\setminus\\left\\{j\\right\\}}\\frac{|S|!(M-|S|-1)!}{M!}\\left(\\zeta_{S\\cup{j}}(x)-\\zeta_S(x)\\right).\r\n\\tag{8.4}\r\n\\end{equation}\\]SHAP feature importance estimates offer several practical properties.\r\nFirst, sum feature attributions observation equal difference average prediction model actual prediction observation (local accuracy).\r\nSecond, feature important one model another, regardless features also present, importance attributed feature also higher (symmetry / monotonicity).\r\nThird, feature value missing, associated feature importance 0 (missingness).\r\nSeveral approaches proposed reduce complexity Shapley value estimation exponential polynomial time, including KernelSHAP¬†[186], works model type, TreeShap¬†[190] tree-based models.Feature Selection.\r\ncontext predictive modeling, feature selection (FS) methods generally aim reduce number predictor features either () maximize model performance (b) affect model performance little possible.\r\nmodeling families sensitive predictors irrelevant target feature, support vector machines¬†[193] neural networks¬†[174], [195].\r\nOthers, linear logistic regression models, susceptible correlated predictors.\r\nOften domain experts require intrinsically interpretable models, requires eliminating predictors contribute substantially model performance.Traditionally, FS methods broadly classified three categories: embedded, filter, wrapper¬†[197].\r\nEmbedded FS refers internal mechanisms modeling algorithms evaluate usefulness features.\r\nExamples algorithms include tree- rule-based models¬†[199], [200], regularization methods Least Absolute Shrinkage Selection Operator¬†[203] (LASSO) Ridge¬†[205] regression.\r\nFiltering methods rank predictors based measure importance, e.g., correlation target feature.\r\n\r\nPopular examples include correlation-based feature selection¬†[206] Relief¬†[207].\r\nWrapper methods rank refine candidate feature subsets iterative search driven model performance.\r\nExamples include sequential forward search, recursive backward elimination, genetic search¬†[208].","code":""},{"path":"iml.html","id":"iml-workflow","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2 Overview of the Mining Workflow","text":"section, describe components mining workflow (Figure¬†8.3).\r\napply workflow three classification tasks: prediction () tinnitus-related distress (ii) depression treatment tinnitus patients, well (iii) rupture status classification intracranial aneurysms.\r\nrefer tasks CHA-Tinnitus, CHA-Depression AneurD, hereafter.TODO: COMPLETEFor CHA dataset, selected patients complete data two classification tasks. AneurD, segmented aneurysms raw image data, performed automated centerline neck curve extraction, generated morphological features. performed correlation analysis identify relevant correlations predictors, correlations predictors response, significant differences correlation predictors response T0 T1. embedded model training iterative feature elimination wrapper retained predictors identified important model. selected best overall model based AUC used post-hoc interpretation methods identify predictors highest attribution model prediction global, subpopulation observation level.\r\nFigure 8.3: mining workflow.\r\n","code":""},{"path":"iml.html","id":"preprocessing-of-raw-image-data","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.1 Preprocessing of Raw Image Data","text":"AneurD, necessary extract morphological features first.Segmentation neck curve extraction.\r\nAneurysms vessels segmented using threshold-based approach¬†[209] digital subtraction data reconstructed 3D rotational angiography images.\r\nSubsequently, centerline vessel extracted using Vascular Modeling Toolkit (VMTK, vmtk.org)¬†[210].\r\nSubsequently, plane separating aneurysm parent vessel determined using automatic ostium detection Saalfeld et al.¬†[211].Morphological feature extraction.\r\n3D surface mesh, obtained neck curve, dome point \\(D\\), two base points \\(B_1\\) \\(B_2\\).\r\ndescribed ¬†[211], \\(B_1\\) \\(B_2\\) approximated points centerline largest distance rays \\(B_1\\) \\(B_2\\) \\(D\\) intersect surface mesh.\r\nFigure¬†8.4 illustrates extracted parameters, \\(H_{max}\\), \\(W_{max}\\), \\(H_{ortho}\\), \\(W_{ortho}\\), \\(D_{max}\\) (see Figure¬†()) describe aneurysm shape¬†[17], [212].\r\nangle parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) (Figure 8.4¬†(b)) extracted based \\(B_1\\), \\(B_2\\), \\(D\\), respectively.\r\nabsolute difference \\(\\alpha\\) \\(\\beta\\) denoted \\(\\Delta_{\\alpha\\beta}\\).\r\nseparating aneurysm parent vessel neck curve, able derive surface area \\(A_A\\) volume \\(V_A\\) aneurysm (Figure 8.4¬†(c)).\r\nprovide two variants surface area ostium, \\(A_{O1}\\) \\(A_{O2}\\) (see Figure 8.4¬†(d)).\r\n\\(A_{O1}\\) area ostium, .e., area triangulated ostium surface resulting connection neck curve points centroid \\(C_{NC}\\), \\(A_{O2}\\) denotes area neck curve projected plane (cf.¬†[211]).\r\nTherefore, \\(A_{O2}\\) extracted parameter comparable studies often use cutting plane determine ostium.\r\nhighly lobulated aneurysms, method achieves local optimum considers one many dome points.\r\nAlthough estimated positions \\(B_1\\) \\(B_2\\) may vary slightly, neck curve detection still performed morphological parameters calculated. Table¬†(tab:09-morphological-features) provides overview extracted morphological features.\r\nFigure 8.4: Illustration extracted morphological features. () Features describe aneurysm width, height, diameter. (b) angles \\(\\alpha\\), \\(\\beta\\) \\(\\gamma\\) extracted base points \\(B_1\\), \\(B_2\\) dome point \\(D\\). (c) separating aneurysm parent vessel via neck curve, area \\(A_A\\) volume \\(V_A\\) computed. (d) area ostium \\(A_{O1}\\) area projected ostium \\(A_{O2}\\) extracted estimating center neck curve \\(C_{NC}\\).\r\n\r\nTable 8.1: Overview morphological features extracted AneurD.\r\n","code":""},{"path":"iml.html","id":"correlational-analysis","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.2 Correlational Analysis","text":"CHA tinnitus, calculate pairwise Spearman correlation predictors part exploratory data analysis.\r\nUsing agglomerative hierarchical clustering complete linkage, arrange predictors correlation heat map potential subgroups predictors similar intra-group correlations similar inter-group correlations can visually identified.\r\nFurthermore, calculate median correlation response predictors questionnaire T0 T1 obtain potential candidate predictors important later modeling step.\r\naddition, identify predictors highest absolute correlation respect response T0 T1, respectively.\r\nFinally, examined predictors whose correlation TQ distress score differed T0 T1.","code":""},{"path":"iml.html","id":"classification-algorithms","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.3 Classification Algorithms","text":"order limited particular classification algorithm, create model highest possible predictive power, examined total eleven classifiers:Least absolute shrinkage selection operator¬†[203] (LASSO) Ridge¬†[205] extensions ordinary least squares (OLS) regression perform feature selection regularization improve predictive performance interpretability.\r\ndataset \\(n\\) observations, \\(p\\) predictor features target \\(y\\), objective LASSO Ridge solve\r\n\\[\\begin{equation}\r\n\\underset{\\beta}{\\text{argmin}} \\underbrace{\\sum_{=1}^n \\left( y_i - \\left( \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j \\right) \\right)^2}_{\\text{Residual Sum Squares}} + \\alpha \\lambda \\underbrace{\\sum_{j=1}^p |\\beta_j|}_{\\text{L1 Penalty}} + (1-\\alpha) \\lambda \\underbrace{\\sum_{j=1}^p \\beta_j^2}_{\\text{L2 Penalty}}\r\n\\tag{8.5}\r\n\\end{equation}\\]\r\n\\(\\beta\\) determined model coefficients, \\(\\lambda\\) tuning hyperparameter controls amount regularization.\r\nLASSO uses L1 norm penalty term, .e., \\(\\alpha\\) = 1, shrinks absolute values coefficients, often forcing exactly equal 0.\r\nRidge uses L2 norm penalty term, .e., \\(\\alpha = 0\\), shrinks coefficient magnitudes.\r\ngeneral, LASSO performs better Ridge relatively small number predictors substantial coefficients remaining predictors coefficients close equal zero.\r\nRidge performs better settings response depends many predictors, approximately equal importance.\r\nperspective interpretability, LASSO advantage producing sparser models reducing values predictors‚Äô coefficients exactly zero.Partial least squares another derivative OLS regression first performs projection extract latent variables capture much variability among predictors possible modeling response well.\r\nlinear regression fit preferably small number latent features projection.\r\nuse generalized partial least squares (GPLS) implementation Ding Gentleman¬†[215].\r\nsupport vector machine (SVM)¬†[193] learns linear nonlinear decision boundaries feature space separate classes.\r\ndecision boundary represented training observations difficult classify, .e., support vectors.\r\ngoal find maximum margin hyperplane separating hyperplane maximum margin support vectors.\r\ncase linear decision boundary exist, nonlinear SVM approaches can used apply called kernel trick transform original feature space new, higher-dimensional space linear hyperplane can found separate classes.artifical neural network (NNET) consists structure nodes connected directed edges.\r\nnode performs basic unit computation.\r\nNodes supplied data values passed via incoming edges nodes.\r\nedge holds weight controls impact node forwards values .\r\nmain goal NNET adjust weights edges relationship predictors response underlying data represented.\r\nNeural networks extract new useful features original predictors relevant classification.\r\ncombining interconnected nodes complex predictive features, NNETs capable extracting classification-relevant feature sets compared expert-driven feature engineering dimension reduction techniques.\r\nNNETs undergone widespread adoption last decade led various success stories computer vision natural language processing¬†[174]. used feed-forward NNET one intermediary layer (hidden unit)¬†[218].Weighted k-nearest neighbor¬†[220] (WKNN) variant KNN classification.\r\nclassify observation unknown response value, k nearest training observations identified modus response values taken prediction.\r\nProximity observations quantified distance measure Euclidean distance.\r\nWhereas ordinary KNN neighbors equal influence prediction, weighted KNN takes account actual distance magnitudes.\r\nresult, WKNN assigns weights training observations inversely proportional distance observation classified.Na√Øve Bayes classifier (NB) uses Bayes‚Äô theorem calculate class membership probabilities.\r\n\r\nnaive property refers assumption class-conditional independence among predictors, employed reduce computational complexity obtain reliable class-conditional probability estimates.Classification regression trees¬†[223] (CART),\r\nC5.0¬†[199],\r\nrandom forests¬†[171] (RF) \r\ngradient boosted trees (GBT)¬†[81] tree-based models.\r\nAlgorithm model family partition predictor space set non-overlapping hyperrectangles based combinations predictor-value conditions, ‚Äúage \\(>\\) 52 & body-mass index \\(<\\) 25.‚Äù\r\nnew observation classified based majority class training data associated hyperrectangle belongs.\r\nRandom forests gradient boosted trees ensembles several different decision trees, tree casting vote final prediction.\r\nrandom forest, base trees created independently.\r\ngradient boosted model, base trees constructed added composite model way new tree reduces error current set trees.","code":""},{"path":"iml.html","id":"classifier-evaluation-and-hyperparameter-tuning","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.4 Classifier Evaluation and Hyperparameter Tuning","text":"use 10-fold stratified cross-validation (CV) classifier evaluation.\r\nk-fold CV, observations split k disjunct partitions.\r\npartition serves test set model trained remainder partitions.\r\nk performance estimates aggregated obtain overall performance score.\r\nperformed grid search hyperparameter selection (cf.¬†Table¬†8.2).\r\nthree applications dichotomous responses different skew, accuracy might inappropriate estimate generalization performance.\r\nInstead, used area receiver operating characteristic curve (AUC) performance measure.\r\nreceiver operating characteristic curve (ROC) shows relationship sensitivity (true positive rate (TPR)) false positive rate (FPR) binary classifier.\r\narea ROC curve (AUC) takes values 0 (0% TPR, 100% FPR) 1 (100% TPR, 0%FPR).\r\nhigher AUC suggests classifier better separating classes.Table 8.2: Overview hyperparameter tuning grid. classifiers implemented statistical programming language R [225] using package mlr [228], provides uniform interface listed machine learning algorithms R packages. grid search used tune hyperparameters using area ROC curve (AUC) evaluation measure. table provides overview classifier, including R package used, tuned hyperparameters value ranges. hyperparameters set default values. * = {linear, polynomial, radial, sigmoid}","code":""},{"path":"iml.html","id":"iterative-feature-elimination","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.5 Iterative Feature Elimination","text":"developed feature selection wrapper successively eliminates subset predictors positively contribute performance model.\r\ncontribution predictor computed using model reliance¬†[246], generalization random forest permutation feature importance¬†[171].\r\nModel reliance estimates merit predictor \\(f\\) toward model \\(\\zeta\\) comparing classification error \\(\\zeta\\) original training set \\(\\mathbf{X}_{orig}\\) classification error \\(\\zeta\\) modified version training set \\(\\mathbf{X}_{perm}\\) values \\(f\\) randomly permuted.\r\nparticular, model reliance \\(MR\\) model \\(\\zeta\\) predictor \\(f\\F\\) calculated \\[\\begin{equation}\r\nMR(f,\\zeta) = \\frac{CE(y,\\zeta(\\mathbf{X}_{perm}))}{CE(y,\\zeta(\\mathbf{X}_{orig}))}\r\n\\tag{8.6}\r\n\\end{equation}\\]\\(CE\\) classification error function takes true class labels \\(y\\) vector predicted class labels, returns fraction incorrectly classified observations.\r\nhigh MR score represents high dependence model \\(f\\), since shuffling values \\(f\\) increases classification error.\r\nConversely, \\(MR\\) score smaller 1 suggests \\(f\\) potentially adversarial model performance, removal increase model performance. Thus, feature elimination wrapper starts training model full set predictors, followed iterative step subset adversarial predictors according model reliance removed, new model remaining predictors trained.\r\nfirst iteration \\(=1\\), initial model \\(\\zeta_1\\) calculated full set predictors \\(F_1 = F\\).\r\npredictor \\(f \\F_i\\), model reliance \\(MR(f,\\zeta_i)\\) calculated.\r\nPredictors \\(f\\F_i:MR(f,\\zeta_i)>1\\) kept iteration \\(+1\\) remaining predictors removed.\r\nprocedure repeated either \\(MR\\) smaller equal 1, .e., \\(\\forall f \\F_i: MR(f,\\zeta_i) \\leq 1\\), \\(F_{+1}=F_i\\).\r\nrandom feature permutation introduces statistical variability, compute mean \\(MR\\) 10 runs obtain stable estimate.","code":""},{"path":"iml.html","id":"post-hoc-interpretation","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.2.6 Post-Hoc Interpretation","text":"SHAP. facilitate model interpretation, model-agnostic post-hoc framework SHAP¬†[186], [190] used assess feature importance CHA data.\r\nBriefly, SHAP value \\(\\phi_f(\\zeta,x)\\) expresses estimated importance feature \\(f\\) prediction model \\(\\zeta\\) instance \\(x\\) change expected value prediction \\(f\\) feature vector \\(x\\) observed instead random.\r\nSHAP framework composes model prediction sum SHAP values feature, .e., \\(\\zeta(x)=\\phi_0(\\zeta,x)+\\sum_{=1}^M \\phi_i(\\zeta,x)\\), \\(\\phi_0(\\zeta,x)\\) expected value model (bias) \\(M\\) number features.SHAP values calculated best model \\(\\zeta_{opt}\\) according AUC.\r\nranking T0 feature attribution towards \\(\\zeta_{opt}\\) determined calculating average SHAP value magnitude instances, .e., \\((j)=\\sum_{=1}^N |\\phi_j(\\zeta_{opt},x)|\\),\r\n\\((j)\\) attribution \\(j\\)-th feature.\r\n\\(N\\times M\\) SHAP matrix clustered agglomerative hierarchical clustering identify subgroups patients similar SHAP values.PDP feature importance. derive global feature importance PD predictor.\r\nassumption predictors high PD variability important.\r\nexample, consider two PD curves Figure 8.1¬†(c): predicted response changes considerably different predictor values blue PD curve whereas green PD curve basically flat line.\r\nTherefore, predictor blue PD curve higher importance score predictor green PD curve.\r\ndefine partial dependence importance \\(\\) predictor \\(f\\) average magnitude differences consecutive values along distribution \\(f\\), .e.,\\[\\begin{equation}\r\nI_f = \\frac{1}{k-1}\\sum_{}^{k-1} |PD(Q=s_i) - PD(Q=s_{+1})|\r\n\\tag{8.7}\r\n\\end{equation}\\]\\(k\\) number (sampled) values distribution \\(f\\).\r\n\r\nFigure 8.5: Illustration partial dependence importance. Partial dependence importance \\(I_f\\) 2 toy PD curves.\r\n","code":""},{"path":"iml.html","id":"iml-results","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3 Results","text":"section, report results three classification tasks, .e., regarding CHA-Tinnitus (Section¬†8.3.1),\r\nCHA-Depression (Section¬†8.3.2) \r\nAneurD (Section¬†8.3.3).","code":""},{"path":"iml.html","id":"iml-results-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.1 Results for CHA-Tinnitus","text":"Correlational analysis. Figure 8.6¬†() shows pairwise correlations among predictors T0.\r\nidentified two major subgroups moderate high intra-group correlations low negative inter-group correlations.\r\nlarger group (cf.¬†upper black square Figure 8.6¬†()) comprises 114 predictors (ca. 55.6%) representing negatively worded items scores higher values represent higher disease burden, e.g.¬†ADSL_depression BI_overallcomplaints.\r\nConsequently, smaller group (cf.¬†lower black square Figure 8.6¬†()) contains 47 predictors (ca. 22.9%) positive wording, e.g.¬†SF8 mental health score (SF8_mental) BSF elevated mood score (BSF_mood).\r\nPredictors one two subgroups exhibit moderate high negative correlation predictors subgroup.\r\n\r\n\r\nFigure 8.6¬†(b) compares correlation predictors TQ_distress (x-axis) treatment (y-axis).\r\nOverall, low moderate bivariate correlations observed, values -0.6 +0.6.\r\naverage change absolute correlation T0 T1 0.031.\r\nchange absolute correlation smaller 0.067 ca. 95% predictors (compare distance points diagonal line Figure 8.6¬†(b)).\r\n137 205 predictors (66.8%), absolute correlation decreased T0 T1.\r\nMedian target-correlation questionnaires ADSL, BSF BI (SF8) greater (smaller) +0.3 (-0.3) moments, respectively, thus greater remaining questionnaires.\r\nFigures 8.6¬†(c) (d) reveal predictors ADSL, BSF, BI, SF8, TINSKAL PSQ among top-20 predictors ranked absolute correlation TQ_distress T0 T1.\r\ngeneral depression score ADSL_depression shows largest correlation magnitude (\\(\\rho\\) = 0.630) treatment (\\(\\rho\\) = 0.564).\r\nFigure 8.6¬†(e) shows 10 predictors largest differences correlation magnitudes T0 T1.\r\nCorrelation treatment larger predictors.\r\nFigure 8.6: Spearman correlation among predictors correlation predictors TQ_distress T0 T1. () heatmap depicts correlation coefficients pairs predictors T0. Predictors arranged result agglomerative hierarchical clustering complete linkage. two black squares depict two major subgroups correlated predictors. (b) relationship predictor TQ_distress T0 (x-axis) T1 (y-axis). diamond symbol represents median correlation predictors questionnaire. (c) Top-20 predictors exhibit highest absolute correlation TQ_distress T0. (d) Top-20 predictors exhibit highest absolute correlation TQ_distress T1. (e) Top-10 predictors highest change absolute correlation TQ_distress T0 T1.\r\nPredictive performance classification models. performances 11 classifiers across feature elimination iterations shown Figure¬†8.7.\r\ngradient boosted trees model (GBT) yields highest AUC (iteration = 7, AUC = 0.890 \\(\\pm\\) 0.04; mean\\(\\pm\\)SD), using 26 predictors (ca. 13%).\r\n\r\nRIDGE classifier achievs second-best performance (=2, AUC: 0.876 \\(\\pm\\) 0.05), relying 127 features, followed random forest model (=3, AUC: 0.872 \\(\\pm\\) 0.05) using 77 features.\r\nClassification using best model (GBT, =7) based probability threshold 0.5 resulted accuracy 0.86, true positive rate (sensitivity) 0.72, true negative rate (specificity) 0.88, precision 0.48 negative predictive value 0.95.\r\nFigure 8.7: Classification results CHA-Tinnitus. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier‚Äôs run maximum AUC. Classifiers ordered maximum AUC left right.\r\ntrained using smaller feature space, classifier produce least one model similar even improved performance compared respective model learned whole set predictors.\r\nfact, exception WKNN, classification methods benefit feature elimination produce best model predictor subset (cf.¬†Figure¬†8.7).\r\nGBT, gain AUC 185 features 26 features (= 11) 0.01.\r\nmodel achieves maximum AUC good trade-high predictive performance low model complexity, thus decided investigate model.Feature importance. best model, attributions 26 selected features shown Figure 8.8¬†().\r\nAmong 26 features 6 scores, 12 single items, 4 demographic features (number visited doctors, university-level education, lower secondary education, tinnitus duration) 4 features measuring average time spent completing item.\r\nTINSKAL tinnitus impairment score (TINSKAL_impairment) represents predictor highest model attribution exhibits highest average absolute SHAP value (change log odds) 0.448.\r\nADSL depressivity score (ADSL_depression) single question ADSL (ADSL_adsl11: ‚Äúpast week sleep restless.‚Äù) ranked second third important, respectively.\r\nRemarkably, 9 questionnaires least 1 feature selected.\r\nFigure 8.8¬†(b) shows patient-individual SHAP values predictor point color depicts predictor value magnitude.\r\nhigh attribution TINSKAL_impairment highlighted wide spread SHAP value distribution.\r\npredictor, high values generally correspond increased predicted probability tinnitus decompensation.\r\nHowever, trend non-linear, since small values (light green yellow) associated SHAP value just slightly smaller equal 0.\r\nMoreover, large spread SHAP value ca. 0.7 1.2 patients high TINSKAL_impairment values opposed somewhat dense bulk points representing patients SHAP values ca. -0.7 -0.4.\r\nindicate patients report high tinnitus impairment difficult classify.\r\n, may suggest visual analog scales robust enough quantify tinnitus-related distress.\r\ninference supported SHAP feature dependence plot Figure 8.9¬†(1) juxtaposes actual values predictor corresponding SHAP values patients reveals J-shaped relationship .\r\nspecifically, predicted tinnitus-related distress decreasing 0 2.5, remains plateau 2.5 4 increasing 4 maximum value 10.\r\nBesides TINSKAL_impairment, features ADSL_depression, TINSKAL_loudness, BI_overallcomplaints, BSF_timestamp SWOP_pessimism also showed non-linear relationship respect SHAP values.\r\nFigure 8.8: SHAP analysis results best model (GBT, feature elimination iteration = 7). () Global feature importance based mean absolute SHAP magnitude observations. Values depict absolute change log odds higher values indicate higher feature attribution towards model. (b) Patient-individual SHAP values. Points represents SHAP value predictor (y-axis) individual patient. afar point vertical 0-baseline, larger attribution corresponding predictor value model prediction. Vertically offset points depict regions high density (similar violin plot), .e., greater number patients similar SHAP values. Actual predictor values mapped point color. (c) Stacked patient-individual SHAP values 6 predictors highest mean absolute SHAP values. Patients ordered according hierarchical clustering Ward linkage. Black horizontal lines depict average sum SHAP values cluster members k = 5 clusters. inset plot shows Bayesian information criterion minimal number clusters.\r\n\r\nFigure 8.9: SHAP feature dependence. relationship actual values predictor (x-axis) corresponding SHAP values (y-axis) shown points representing patient locally weighted scatterplot smoothing (LOWESS) curves indicating overall trend. Predictors ordered mean absolute SHAP value (see Figure8.8¬†()). Gray histograms bar charts depict distributions predictors.\r\nEven though several predictors exhibit low moderate global importance, high attribution towards model prediction specific subgroups.\r\nexample, considering SOZK_lowersec, patients lower secondary education average SHAP value +0.5, whereas patient different education levels average SHAP value -0.1 hence close population average (cf.¬†Figure 8.8¬†(b), Figure 8.9¬†(13)).\r\nfeatures show monotonic relationship actual values SHAP values.\r\nexample, increasing values SF8 physical component score (SF8_physicalcomp) exhibit decreasing likelihood predicted decompensated tinnitus increasing physical health (cf.¬†Figure 8.8¬†(b), Figure 8.9¬†(14)).investigate whether subgroups patients similar respect model prediction can explained , clustered patients based SHAP values.\r\nFigure 8.8¬†(c) shows stacked patient-individual SHAP values six predictors highest average absolute SHAP values remaining predictors combined.\r\nAccording Bayesian information criterion (cf.¬†insetted plot Figure 8.8¬†(c)), optimal number patients clusters similar SHAP value patterns 5.\r\nClusters 1 5 comprise subgroups sum SHAP values predictors positive, see horizontal lines Figure 8.8¬†(c).\r\nHence, patients likely predicted decompensated tinnitus.\r\ncomparison subgroups, patients clusters 1 5 reported higher degrees tinnitus impairment, depressivity, anxiety, tinnitus loudness, sleeplessness, pessimism, psychosomatic complaints well perceived levels stress social isolation.\r\ngeneral, patients cluster 1 slightly higher values across predictors patients cluster 2.\r\naddition, cluster 1 contains higher fraction patients lower secondary education (‚ÄúHauptschule‚Äù), report frequently occuring headaches, higher levels fears future longer tinnitus duration.\r\nCluster 3 largest subgroup comprising 39.6% patients.\r\nTogether cluster 2, subgroups lowest predicted probability tinnitus decomposition.\r\nPatients cluster 2 3 report highest physical health levels determination.\r\nCluster 4 somewhat close prediction average positive negative SHAP values nearly even .\r\nrespect average patient-sum SHAP values, cluster 3 lies cluster 2 cluster 4.\r\n","code":""},{"path":"iml.html","id":"iml-results-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.2 Results for CHA-Depression","text":"Predictive performance classification models.\r\nFigure¬†8.10 depicts performance classification methods across iterations.\r\nLASSO classifier achieved maximum AUC classification algorithms (iteration = 1, AUC = 0.867 \\(\\pm\\) 0.037; mean \\(\\pm\\) SD), followed Ridge (= 1, AUC = 0.864 \\(\\pm\\) 0.040) GBT (= 1, AUC = 0.862 \\(\\pm\\) 0.038).\r\nconsidering best model per classifier, models similar performance, ranging AUC 0.809 (C5.0) 0.867 (LASSO).\r\nFigure 8.10: Classification results CHA-ADSL_depression. Average cross-validation AUC relative number retained predictors classifier optimal hyperparameter configuration feature selection iteration. Yellow ribbons depict standard deviation. Points highlight classifier‚Äôs run maximum AUC. Classifiers ordered maximum AUC left right.\r\nbest model (Lasso, = 1) achieves accuracy 79%, true positive rate (sensitivity) 61%, true negative rate (specificity) 88%, precision 72% negative predictive value 82% based probability threshold 0.5.\r\nfinal model includes 40 predictors non-zero coefficients.\r\nFigure¬†8.11 shows median model coefficient features across 10 cross-validation folds.\r\nADSL questionnaire alone, 16 single items included final model, including indicators depressivity (ADSL_adsl09, ADSL_adsl18, ADSL_adsl12) perceived antipathy received people (ADSL_adsl19), sleeplessness (ADSL_adsl11), dejectedness (ADSL_adsl03), lack appetite (ADSL_adsl02), confusion (ADSL_adsl05), anxiety (ADSL_adsl10, ADSL_adsl08), absence self-respect (ADSL_adsl04, ADSL_adsl09), lack vitality (ADSL_adsl09, ADSL_adsl09), taciturnity (ADSL_adsl13) irritability (ADSL_adsl01).\r\nThus, questionnaire contributed highest number predictors model. tinnitus-distress-oriented TQ, 5 predictors selected.\r\n, model used 5 predictors socio-demographics questionnaire (SOZK), including German nationality (SOZK_nationality) highest absolute model coefficient, university level graduation (SOZK_graduate), tinnitus duration (SOZK_tinnitusdur), employment status (SOZK_job), marital status (SOZK_unmarried) partnership status (SOZK_partnership).\r\nFigure 8.11: Coefficients LASSO model. Cross-validation (CV) median (points) \\(\\pm\\) median absolute deviation (line ranges) coefficients best LASSO model (= 1). frequency non-zero coefficients 10-fold CV given parentheses right predictor name. 185 features total, 40 features exhibit non-zero model coefficient least one CV fold.\r\nTable¬†8.3 provides description predictors Figure¬†8.11.Effect feature elimination classification performance. classifiers SVM show high stability performance smaller feature subsets.\r\nFigure¬†8.10, see LASSO difference AUC trained 185 features (= 1) vs.¬†trained 6 features (= 7) -0.017.\r\nSeveral classifiers benefited feature selection terms predictive performance.\r\nGPLS, NNET, CART, C5.0 RF, max. AUC achieved feature subset.\r\ndecision tree variants CART C5.0 gain performance feature removal, since respective max. AUC obtained smallest predictor subset, cardinality 22 10, respectively.\r\nTable 8.3: important features LASSO model. Predictors highest absolute coefficient final LASSO model (iteration = 1). 185 predictors total, 40 predictors exhibit non-zero model coefficient least one ten cross-validation folds.\r\n","code":""},{"path":"iml.html","id":"iml-results-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.3.3 Results for AneurD","text":"Figure¬†8.12 shows classification results data subset.\r\nGBT achieves maximum AUC (cross-validation average 67.2% \\(\\pm\\) 1.8% standard deviation), followed C5.0 (AUC 64.6% \\(\\pm\\) 1.9%) GPLS (AUC 63.3% \\(\\pm\\) 1.2%).\r\nsubset SW, SVM comes best 75.2% \\(\\pm\\) 5.7% AUC, slightly superior GPLS (AUC 73.6% \\(\\pm\\) 4.4%) NNET (AUC 71.6% \\(\\pm\\) 5.5%).\r\nBF, WKNN yields best (AUC 64.0% \\(\\pm\\) 1.1%) model, GPLS (AUC 62.9% \\(\\pm\\) 2.6%) RF (AUC 62.7% \\(\\pm\\) 2.3%) similar yet slightly inferior generalization performances.\r\nresults indicate classifiers yield better performance subset sidewall aneurysms.\r\nOverall, none classification algorithms outperforms others across three subsets.\r\nFigure 8.12: Classification results AneurD. combination data subset classification algorithm, performance run preprocessing transformation achieves highest AUC shown. SW = sidewall; BF = bifurcation.\r\nrespect PD importance, Figure¬†8.13 illustrates high attribution angle parameter \\(\\gamma\\) towards rupture status classification, feature ranked first third best models BF.\r\nSVM model trained SW subset, ellipticity index (EI) important.\r\nFigure 8.13: Relative PD importance (AneurD). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\nFigure¬†8.14 shows PDP ICE curves important predictors according \\(I_f\\) best models data subset.\r\nICE curves GBT model SVM model (Figure 8.14¬†() ¬†(b)) exhibit nearly identical trend different intercept.\r\ncontrast, ICE curves WKNN (Figure8.14¬†(c)) appear jittery.\r\nApparently, plots summarize major characteristics different model families.\r\nspecifically, GBT model (Figure 8.14¬†()) produces jagged curves distinct vertical cuts, representing splits base decision trees tree ensemble.\r\nexample, plot \\(\\gamma\\) shows 4 splits {16.54, 49.26, 54.42, 64.35}.\r\nSVM classifier (Figure 8.14¬†(b)) linear model, hence ICE PD curves just lines fixed slope.\r\nmarginal model posterior aneurysm rupture increases higher values ellipticity index, max. width aneurysm body, aspect ratio \\(H_{ortho}/N_{avg}\\) max. aneurysm diameter, whereas area ostium (variant 2), lower values indicative high rupture likelihood.\r\nWKNN, PDP better able clearly show marginal posteriors individual predictors ICE curves.\r\ndue property WKNN ‚Äúlazy‚Äù learner produce actual model makes predictions based observation-individual similarity.\r\nFigure 8.14: Relative PD importance (AneurD). PD importance best model data subset. Values relative maximum PD importance. SW = sidewall; BF = bifurcation.\r\n","code":""},{"path":"iml.html","id":"iml-discussion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4 Discussion","text":"section, discuss findings respect three classification tasks, .e., regarding\r\nCHA-Depression (Section¬†8.4.1), CHA-Tinnitus (Section¬†8.4.2) \r\nAneurD (Section¬†8.4.3).","code":""},{"path":"iml.html","id":"iml-discussion-depression","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.1 CHA-Depression","text":"Machine learning used build predictive models depression severity based structured patient interviews¬†[248], [250].\r\nrefrain quantitative comparison studies due differences population characteristics measurements.\r\ngood best models actually?\r\nreasonable baseline classifier simply predicts depression status time T0, yields 79% accuracy.\r\nmodels outperform baseline, although likely provide good fit sample, patient subgroups centers yet studied.\r\nHowever, models promising first step supporting timely prediction depression severity selection appropriate treatment questionnaire items.Consistent previous studies¬†[252], found strong association tinnitus distress depression severity.\r\nFurthermore, predictors measuring perceived stress demands found significant contributor depression tinnitus patients¬†[254].\r\nfact predictors selected different questionnaires confirms multifactoriality depression, whose assessment requires inclusion different measurements.\r\nTherefore, concomitant emotional symptoms comorbidities must taken account meet patient-specific needs.\r\nprevious study¬†[256], high sensitivity detecting depression achieved using two-item questionnaire.\r\nOne two items ‚Äúpast month, often bothered feeling , depressed, hopeless.‚Äù;[256] similar ADSL_adsl06 (‚Äúpast week, felt depressed.‚Äù), second largest absolute coefficient best LASSO model.Generally, care must taken interpreting model coefficients:\r\nexample, identified strong relationship non-German citizenship depression severity (cf.¬†Figure¬†8.11 Table¬†8.3).\r\nAlthough studies reported ethnic differences depression¬†[258], [260], occurrence item tends suggest higher perceived social stress patients predominantly Turkish origin, due higher unemployment rates, larger families, poorer housing conditions, etc. demographic group.\r\n5.0% cohort population non-German citizens, results also effect overfitting.\r\nSince associated predictor first iteration feature elimination model reliance score less 1.0, consequently omitted sparser models.Regarding stability models smaller feature sets, results show simpler models slightly inferior predictive model.\r\nspecifically, classification methods show improvement AUC number predictors decreases.\r\nfact, 5 11 classifiers improved even feature selection, .e., AUC second later iteration superior AUC first iteration (205 predictors used).\r\nexample, two decision tree variants achieved highest performance smallest feature subset case.\r\nRegarding LASSO classifier, showed best performance, encouraging 6 predictors 4 questionnaires showed similar performance (AUC = 0.850) compared best overall model (AUC = 0.867).\r\nnoteworthy neither predictors tinnitus localization quality sociodemographic predictors included model.\r\nfinding used reduce number questions entire questionnaires patients must answer treatment.\r\npsychological physical stress subject undergoing examination (e.g., painful biopsy vs.¬†blood test).\r\nexample, Yu et al.¬†[261] perform feature selection budget, cost feature acquisition derived suggestions medical experts based total financial burden, patient privacy, patient inconvenience.\r\nKachuee et al.¬†[262] derive feature costs based convenience answering questions, performing medical exams, blood urine tests.terms clinical relevance, results first step guide clinicians making treatment decisions regarding clinical depression patients chronic tinnitus.\r\nmodels used design appropriate treatment pathway.\r\nHowever, using models practice, one must aware trained cross-sectional data, .e., models separate subclinical clinical depression based questionnaire responses sociodemographic data treatment.\r\nAlso, one must keep mind treatment 7-day treatment response depression status treatment.also limitations approach.\r\nFirst, models might subject selection bias patients complete seven questionnaires admission treatment excluded analyses.\r\nHowever, consider data ‚Äúmissing values‚Äù lead problematic suggestion using imputation methods.\r\nuse imputation () proportion patients complete entire questionnaire (rather individual items) (ii) know whether data missing random.\r\nHowever, number patients large, believe results sufficiently robust.\r\nfuture work, investigate possible systematic differences included excluded patients.\r\nexclusion patients dropped completing questionnaires prematurely, partly gradual loss motivation, technical unfamiliarity computer, possible interruptions staff complete baseline assessments, lead selection bias.\r\npatient population one hospital, future work involves external validation models data different populations hospitals.\r\nuse cross-sectional data limits interpretation prediction depression severity beyond end therapy, future work need validate models longitudinal data.Another potential limitation greedy process iterative feature selection wrapper, can miss global optima result.\r\niteration, predictors prevent model classifying correctly removed feature set.\r\npredictor eliminated, included subsequent iteration.\r\nHowever, possible including predictor removed early iteration lead better model later iteration.\r\npossible solution mechanism backtrack revisit earlier iterations turns removed predictors actually contributed positively model performance.\r\nAlternatively, \\(MR\\) cutoff value discarding features (set 1 experiments) subjected hyperparameter tuning.\r\nFuture work therefore includes comparison feature selection algorithms.","code":""},{"path":"iml.html","id":"iml-discussion-tinnitus","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.2 CHA-Tinnitus","text":"trained classification models predict tinnitus-related distress multimodal treatment (T1) patients chronic tinnitus based self-report questionnaires data acquired treatment (T0).\r\ngradient boosted trees model uses 26 (12.7%) total 205 predictors separates patients ‚Äúcompensated‚Äù vs.¬†‚Äúdecompensated‚Äù tinnitus best AUC.\r\n\r\nAmong features measurements describe variety psychological psychosomatic patient characteristics well socio-demographics therefore confirming multi-factorial nature tinnitus-related distress.\r\ncharacteristics used phenotyping Chapter¬†5.\r\nAdditionally, predictors can investigated followup studies characteristics influence treatment success.\r\n\r\nexpected, predictors directly linked tinnitus quality show high model attribution, degree perceived tinnitus impairment loudness.\r\ntime, depression, attitudinal factors (self-efficacy, pessimism, complaint tendency), sleep problems, educational level, tinnitus location duration emerged highly important model prediction well.Quantitative predictors, tinnitus impairment loudness, show non-monotonic relationships respect predicted outcome.\r\nNotably, low self-reported impairment loudness measured visual analogue scales generally indicate low tinnitus-related distress measured TQ.\r\nOne explanation simple measurements like TINSKAL_impairment TINSKAL_loudness less robust show higher variability compound scale combines multiple single questionnaire items.\r\nfindings investigated , e.g., whether relationship towards subgroup patients fatigued thus less thoroughly filling large number questionnaires.results confirm intricate interplay depression tinnitus-related distress elucidated numerous previous studies¬†[264], [266], [268], [271], [273].\r\nbest model, ADSL score 20 associated increased predicted risk tinnitus decompensation (cf.¬†Figure 8.9¬†(2)) close cutoff clinical relevance depression¬†[41].context parsimonious learning, general strategy determine set predictors small possible inclusion predictor yield considerable improvement performance.\r\nmany predictors really necessary accurate tinnitus distress prediction?\r\nFigure 8.15¬†() illustrates change performance GBT classifier predictors iteratively added feature space order SHAP values respect best model.\r\nmodel uses TINSKAL_impairment achieves AUC = 0.79 \\(\\pm\\) 0.06.\r\nAdding ADSL_depression leads improvement AUC 0.06.\r\nHowever, none remaining 24 predictors results improvement 0.01, respectively.\r\nMoreover, 3 predictors necessary model AUC = 0.85, 8 predictors model AUC = 0.87 15 predictors model AUC = 0.89 (cf.¬†Figure 8.15¬†()).\r\nFigure 8.15: Cumulative feature contribution & correlation network. () Cross-validation AUC (average \\(\\pm\\) standard deviation) GBT model trained feature subset comprising predictors denoted y-axis iteration. ordering features according mean absolute SHAP value (cf.¬†Figure 8.8~()). (b) Network illustrating 3 groups features among 26 selected predictors best model high intra-group correlation (\\(|\\rho| \\geq\\) 0.5). 8 predictors (predominantly SOZK) without moderate high pairwise correlation shown.\r\nOne potential explanation multicollinearity among groups predictors.\r\nFigure 8.15¬†(b) shows network 3 predictor groups among 26 features best model.\r\nexample, features TINSKAL_impairment TINSKAL_loudness moderately correlated (Spearman correlation \\(\\rho\\) = 0.69), raises question whether one two predictors removed without considerable loss AUC.\r\nlargest subgroup spanning 14 features involves descriptors depression, perceived stress reported physical health.\r\nfuture work, investigation possible interaction effects among moderately strongly correlated features investigated, better understand selected determine whether removed achieve better trade-model accuracy complexity.workflow leverages potential machine learning identifying key predictors variety features collected treatment post-treatment tinnitus compensation, ensuring every potential predictor included analysis, internal validation classification models using cross-validation hyperparameter tuning.\r\nFurthermore, selecting variety classification algorithm families, linear nonlinear relationships feature outcome identified.\r\nlimitation hypothesis-free approach learned models contain features quantify similar patient characteristics.\r\nexample, best model study included two highly correlated features ADSL_depression BSF_anx_depression (anxious depressiveness score).\r\ninclusion features contributed model performance, medical perspective, predictive model certain features might beneficial. Preselecting features avoid multicollinearity direction future work.Finally, exclusion 2,701 4,117 patients (65.6%) complete 10 questionnaires resulted selection bias.\r\nMany patients spent one hour completing questionnaire dedicated minicomputer therefore likely drop completion process.\r\n\r\nCompleters slightly younger non-completers (mean age 49.8 \\(\\pm\\) 12.2 vs.¬†51.7 \\(\\pm\\) 13.8), likely highest German school degree ‚ÄúAbitur‚Äù (48.2% vs.¬†42.0%) suffering tinnitus longer (\\(>\\) 5 years: 33.3% vs.¬†25.1%).\r\n\r\n\r\n\r\nfuture work intend investigate extent insights completers can used subsamples non-completers.\r\nTherefore, can use DIVA framework Hielscher et al.¬†[274].\r\nHowever, psychological treatment approaches likely benefit report psychological problems prior tinnitus perception association tinnitus perception.","code":""},{"path":"iml.html","id":"iml-discussion-aneur","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.4.3 AneurD","text":"classification results promising, morphological parameters alone can provide models moderate power.\r\nprevious studies found hemodynamic parameters also predictive¬†[275], [276], future work includes exploring potential combining morphologic hemodynamic features classification rupture status.\r\naddition, focus now quantifying merit morphologic parameters, ignored demographic characteristics, age sex, also correlate strongly aneurysm rupture¬†[277].\r\nexpect adding patient characteristics improve classification performance.\r\nPDP analysis showed best model (gradient boosted trees), parameters angle dome point \\(\\gamma\\), ellipticity index \\(EI\\), maximum aneurysm width \\(W_{max}\\), nonsphericity index \\(NSI\\), aneurysm area \\(A_{O2}\\) highest attribution (see Figure 8.13¬†()).\r\ndiffered found two subsets sidewall bifurcation aneurysms, respectively.\r\nFigure¬†8.14 shows none features appear among top 5 predictors sidewall aneurysms, bifurcation aneurysms, overall data set.\r\nalso partly due fact family best model different subsets.\r\nConsequently, Figure¬†8.16 shows PDP curves top 5 features differ substantially.\r\nTherefore, argue PDPs appropriate intra-model comparisons feature attributions.\r\nFigure 8.16: PDP curves top-5 predictors - GBT best models data subset.\r\nobserved classification performance consistently higher subset sidewall aneurysms vs.¬†bifurcation aneurysms, different parameters found high model attribution.\r\npartially due rather small sample size already mentioned differences model families.\r\nHowever, Baharoglu et al.¬†[19] identified significant differences sidewall bifurcation aneurysms respect morphological parameters, parameters can predict rupture status.findings also suggest form higher-level interactions groups features.\r\nexample, ellipticity index (\\(EI\\))found second important - GBT important SW - SVM, although differences \\(EI\\) unruptured ruptured aneurysms significant (p = 0.323, Wilcoxon rank sum test, \\(\\alpha\\) = 0.01).limitations analysis.\r\nsmall sample size, especially subset sidewall aneurysms (N=24), lead overfitting.\r\nfuture work, like retrain models larger number datasets, incorporate wider variety predictors, hemodynamic demographic features, mentioned .\r\nlimitation concerns validity class label.\r\nSamples labeled unruptured ruptured later moment.\r\n, like investigate samples high classification error detail.\r\n, goal derive descriptions aneurysms subgroups hard classify, order better understand reasons misclassification, signalize medical expert manual diagnosis necessary.","code":""},{"path":"iml.html","id":"iml-conclusion","chapter":"8 Post-Hoc Interpretation of Classification Models","heading":"8.5 Conclusion","text":"medical applications, black-box models becoming increasingly popular due high predictive power.\r\nHowever, due opacity, post-modeling step required extract actionable insights .\r\npresented machine learning workflow classification post-hoc interpretation alongside dataset-specific steps three medical applications.\r\nvariety classification algorithms created identical datasets, chose interpretation method based opacity model family achieves maximum performance, dimensionality.\r\nCHA-tinnitus, gradient-boosted trees performed best, used Shapely value explanations obtain feature importance values model, subpopulation, observation levels.\r\nCHA depression, LASSO found achieve best generalization performance, used intrinsically interpretable model coefficients.\r\nAneurD, used PD importance instead SHAP values, although gradient-boosted trees provide best model number observations small produce reliable importance scores subpopulation observation level.TODO: discussion robustness","code":""},{"path":"gender.html","id":"gender","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","text":"chapter partly based :Uli Niemann, Benjamin Boecking, Petra Brueggemann, Birgit Mazurek, Myra Spiliopoulou. ‚ÄúGender-Specific Differences Patients Chronic Tinnitus ‚Äì Baseline Characteristics Treatment Effects.‚Äù : Frontiers Neuroscience 14 (2020), p.¬†487. DOI: 10.3389/fnins.2020.00487.","code":""},{"path":"gender.html","id":"brief-chapter-summary-6","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"Brief Chapter Summary","text":"present workflow examine subpopulations differ respect predictive characteristics temporal data. , derive post-hoc interpretation measure assess difference association predictors two subpopulations. report results CHA gender differences (subpopulations female male patients) two outcomes tinnitus-related distress depression, effect treatment outcomes.","code":""},{"path":"gender.html","id":"gender-intro","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.1 Motivation and Comparison to Related Work","text":"written","code":""},{"path":"gender.html","id":"gender-measure","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.2 Comparing Differences in Feature Importance between Two Subpopulations","text":"written","code":""},{"path":"gender.html","id":"gender-workflow","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3 Workflow","text":"","code":""},{"path":"gender.html","id":"learning-tasks","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3.1 Learning Tasks","text":"written","code":""},{"path":"gender.html","id":"model-evaluation-and-hyperparameter-tuning","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.3.2 Model Evaluation and Hyperparameter Tuning","text":"","code":""},{"path":"gender.html","id":"gender-results","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.4 Results","text":"written","code":""},{"path":"gender.html","id":"gender-conclusions","chapter":"9 Subpopulation-Specific Learning and Post-Hoc Model Interpretation","heading":"9.5 Conclusions on Subpopulation-Specific Differences in Feature Importance","text":"written","code":""},{"path":"summary.html","id":"summary","chapter":"10 Conclusion and Future Work","heading":"10 Conclusion and Future Work","text":"written","code":""},{"path":"summary.html","id":"summary-results","chapter":"10 Conclusion and Future Work","heading":"10.1 Research Results for Medical Expert-Guided Knowledge Discovery","text":"written","code":""},{"path":"summary.html","id":"summary-future-work","chapter":"10 Conclusion and Future Work","heading":"10.2 Future Work","text":"written","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"abbreviations.html","id":"abbreviations","chapter":"Abbreviations","heading":"Abbreviations","text":"","code":""},{"path":"appx-pheno.html","id":"appx-pheno","chapter":"A Variables selected for phenotyping","heading":"A Variables selected for phenotyping","text":"ACSA_qualityoflife*: Quality life last 2 weeksADSL_depression: Depressive disorder sum scoreBI_abdominalsymptoms: Abdominal symptoms scoreBI_fatigue: Fatigue scoreBI_heartsymptoms: Heart symptoms scoreBI_limbpain: Limb pain scoreBI_overallcomplaints: Overall complaints sum scoreBSF_anger: Anger scoreBSF_anxdepression: Anxious depression scoreBSF_apathy: Apathy scoreBSF_elevatedmood*: Elevated mood scoreBSF_fatigue: Fatigue scoreBSF_mindset*: Positive mindset scoreISR_additionalitems: Additional items scoreISR_anxiety: Anxiety scoreISR_compulsivesyn: Obsessive-compulsive syndrome scoreISR_depression: Depression scoreISR_eatingdisorder: Eating disorder scoreISR_somatosyn: Somatoform syndrome scoreISR_totalpsychiatricsyn: Total psychiatric syndrome scorePHQK_depression: Presence depressionPHQK_panicsyn: Presence panic syndromePSQ_demand: Demand scorePSQ_joy*: Joy scorePSQ_stress: Total perceived stress sum scorePSQ_tension: Tension scorePSQ_worries: Worries scoreSES_affectivepain: Affective painSES_sensoricpain: Sensoric painSF8_bodilyhealth*: Bodily health scoreSF8_mentalcomp*: Mental component summary scoreSF8_mentalhealth*: Mental health scoreSF8_overallhealth*: Overall health scoreSF8_physicalcomp*: Physical component summary scoreSF8_physicalfunct*: Physical functioning scoreSF8_roleemotional*: Role emotional scoreSF8_rolephysical*: Role physical scoreSF8_socialfunct*: Social functioning scoreSF8_vitality*: Vitality scoreSSKAL_painfrequency: Visual analog scale pain frequencySSKAL_painimpairment: Visual analog scale pain impairmentSSKAL_painseverity: Visual analog scale pain severitySWOP_optimism*: Optimism scoreSWOP_pessimism: Pessimism scoreSWOP_selfefficacy*: Self-efficacy scoreTINSKAL_frequency: Tinnitus frequencyTINSKAL_impairment: Tinnitus impairmentTINSKAL_loudness: Tinnitus loudnessTLQ_01_bothears: Tinnitus location: earsTLQ_01_entirehead: Tinnitus location: entire headTLQ_01_leftear: Tinnitus location: left earTLQ_01_rightear: Tinnitus location: right earTLQ_02_hissing: Tinnitus noise: hissingTLQ_02_ringing: Tinnitus noise: ringingTLQ_02_rustling: Tinnitus noise: rustlingTLQ_02_whistling: Tinnitus noise: whistlingTQ_auditoryperceptdiff: Auditory perceptual difficulties scoreTQ_cognitivedistress: Cognitive distress scoreTQ_distress: Total tinnitus distress scoreTQ_emodistress: Emotional distress scoreTQ_intrusiveness: Intrusiveness scoreTQ_psychodistress: Psychological distress scoreTQ_sleepdisturbances: Sleep disturbances scoreTQ_somacomplaints: Somatic complaints score","code":""}]

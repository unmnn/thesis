% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  oneside]{book}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Intelligent Assistance for Expert-Driven Subpopulation Discovery in High-Dimensional Time-Stamped Medical Data},
  pdfauthor={Uli Niemann},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[left = 4cm, right = 3cm, top = 3cm, bottom = 3cm]{geometry}
\usepackage{longtable,booktabs}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{pifont} % for checkmark and cross symbols

\setlength{\tabcolsep}{0.4em} % horizontal space between table columns

% reduce font size of tables
\usepackage{etoolbox}
\AtBeginEnvironment{tabular}{\small}

% \usepackage[fontsize=11pt]{scrextend}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{tcolorbox}
\usepackage{xcolor}

\definecolor{mygray}{HTML}{F5F5F5}

\newtcolorbox{chapter-summary}{
  colback=mygray,
  colframe=mygray,
  coltext=black,
  boxsep=5pt,
  arc=0pt
}

\newenvironment{infobox}[1]
  {
  % \begin{itemize}
  % \renewcommand{\labelitemi}{
    % \raisebox{-.7\height}[0pt][0pt]{
      % {\setkeys{Gin}{width=3em,keepaspectratio}
        % \includegraphics{images/#1}}
    % }
  % }
  % \setlength{\fboxsep}{2em}
  \begin{chapter-summary}
  % \item
  }
  {
  \end{chapter-summary}
  % \end{itemize}
  }
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{thesis.bib}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Intelligent Assistance for Expert-Driven Subpopulation Discovery in High-Dimensional Time-Stamped Medical Data}
\author{Uli Niemann}
\date{18.01.2021}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\begin{comment}

# Preface {.unnumbered}

This is a preview of the current status of Uli's thesis.

Use the navigation bar at the top to download the complete manuscript as PDF or Word document.



<!-- ## Timeline {.unnumbered} -->

<!-- <iframe width="600px" max-width="100%" height="300px" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPYVbwFC1moei3kda9ONwnIonZLVXU1rczkQZ-1wS2rfHxdgN5FSfwE-G5sHv33WZUcdnTXuictySO/pubhtml?gid=0&amp;single=true&amp;widget=true&amp;headers=false"></iframe> -->

<!-- [Google Spreadsheet](https://docs.google.com/spreadsheets/d/1GYhEgQLxlNm5qJtD7QhNaAR1s7wsV_A-FDOLP6hFAA0/edit?usp=sharing) -->

<!-- [Google Doc](https://docs.google.com/document/d/1dElyZIQOyGReac6V_NEfME0YL9mac7SSeLlnOKgIJOE/edit?usp=sharing) -->


## Outline / List of Chapters {.unnumbered}

1 &#128994; Introduction\
2 &#x1F535; Medical Background\

PART I SUBPOPULATION DISCOVERY IN HIGH-DIMENSIONAL DATA

3 &#x1F535; Interactive Discovery and Inspection of Subpopulations\
4 &#128994; Identification of Distinct Subpopulations\
5 &#128994; Visual Identification of Informative Features\

PART II EXPLOITING DYNAMICS

6 &#x1F535; Extraction of Evolution Features for Cohort Data\
7 &#x1F534; Extraction of Features From Short Temporal Sequences\

PART III POST-MINING FOR INTERPRETATION

8 &#128994; Post-hoc Interpretation of Classification Models\
9 &#x1F534; Subpopulation-specific Learning and Post-hoc Model Interpretation\

PART IV SUMMARY

10 &#x1F534; Conclusion and Future Work\

Legend:  
&#x1F3C1;&#x1F3C1; = submission ready  
&#x1F3C1; = feedback from reviewers incorporated  
&#128994; = draft is ready  
&#x1F535; = maturing  
&#x1F534; = unwritten

\end{comment}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\hypertarget{motivation-and-objectives}{%
\section{Motivation and Objectives}\label{motivation-and-objectives}}

Common objectives of data analysis in medical research include (i) identifying long-term determinants and protective factors of a medical condition of interest, (ii) discovering subpopulations with increased outcome prevalence, and (iii) generating robust statistical models that can explain relationships between one or more independent variables and the target variable.
For example, epidemiologists attempt to discover associations between multiple risk factors and an outcome in cohort studies by collecting data that include extensive information about participants obtained from questionnaires, medical examinations, laboratory analyses, and imaging.
Often these data are collected repeatedly over time, for example in longitudinal studies.
This means that there is latent but often overlooked temporal information in such data, the investigation of which can potentially lead to new insights.

To find associations between variables, medical researchers usually first carefully derive some hypotheses from clinical practice, experimental studies, or extensive literature reviews to then test them formally for statistical significance.
However, with the ever-increasing volume and heterogeneity of medical data, traditional hypothesis-driven workflows are becoming increasingly impractical, as for this reason some important inherent associations between variables may go undetected.
Machine learning can improve medical research by discovering understandable descriptions of patient or study participant subpopulations that are similar in outcome, and thus can be used to derive new hypotheses.

The proliferation of medical machine learning applications is triggered by several reasons, such as the desire to make automated use of the plethora of information collected about study subjects, but sometimes just based on the ubiquity of deep learning success stories in the media.
However, the ease of creating complex data-driven models is no guarantee that insights can be effortlessly derived.
Most state-of-the-art machine learning algorithms such as deep neural networks and gradient boosting machines generate so-called black-box models with multiple layers of complexity that involve many multivariate, nonlinear interactions between variables that are difficult to represent intuitively.
It is critical that the application expert, who is not a practitioner but a scientist working in a clinical or epidemiological setting, be equipped with tools to understand, explore, and visualize the models so that they can drill down to specific individual patterns and gain actionable insights that ultimately contribute to prevention, diagnosis, and treatment in clinical practice.
Because medical data come from a wide variety of sources key characteristics of the collected datasets vary, requiring adaptation of methods to the specifics of each application scenario.

The goal of our work is to develop methods that serve as intelligent assistance to medical researchers in the analysis of high-dimensional, temporal medical data.
Hence, the core research question of the thesis is: How to derive accurate yet understandable patterns for subpopulation discovery in high-dimensional temporal medical data?
Both before, during, and after the generation of machine learning models, several challenges must be overcome in order for the domain expert to be able to derive actionable knowledge.
These challenges can be translated into the following four requirements.
(1) \emph{Comprehensibility of patterns}: the extracted models, including clusters, rules, and other patterns, must be made understandable; preferably, the model generation process is also comprehensible.
(2) \emph{Exploitation of time}: latent temporal information must be exploited, while satisfying requirement 1.
(3) \emph{Minimization of redundancy}: redundancy must be minimized, while satisfying requirement (1).
Patterns may overlap in terms of the topics they cover.
This leads to a redundancy of patterns, which has a negative impact on the perceived quality of the model.
Our task is to extract, process and display the most relevant (temporal) patterns for expert-driven model exploration.

\hypertarget{structure-and-contributions-of-this-thesis}{%
\section{Structure and Contributions of This Thesis}\label{structure-and-contributions-of-this-thesis}}

This thesis presents solutions to support medical researchers in the data-driven analysis of high-dimensional, temporal medical data.
Design decisions and developments were partly inspired by suggestions from the respective domain experts and cooperation partners, including a specialist for internal medicine, an epidemiologist with statistical expertise, a diabetes expert and three tinnitus specialists.
The thesis is organized into three parts and ten chapters tackling the aforementioned research question and challenges.
PART I covers methods for discovering subpopulations in high-dimensional data.
PART II focuses specifically on temporal aspects of medical datasets and provides approaches that extract informative representations from latent temporal data.
PART III addresses post-hoc analysis of machine learning models and includes solutions to derive model-, observation-, and subpopulation-level insights from otherwise ``opaque'' black boxes.

\begin{itemize}
\tightlist
\item
  Chapter~\ref{background} (\emph{Medical Background}) presents relevant medical background information, a brief comparison of medical study types and an overview of the case studies our solutions are tailored to.
\item
  Chapter~\ref{imm} (\emph{Interactive Discovery and Inspection of Subpopulations}) presents a workflow for the \textbf{TODO}
\item
  Chapter~\ref{sdclu} (\emph{Identifying Distinct Subpopulations}) examines redundancy in large rule sets describing subpopulations. We present a workflow that extracts a smaller number of representative rules. These rules are selected to avoid instance overlap as much as possible, thus covering different concepts in the data space. We evaluate our workflow on two samples of the longitudinal cohort study for each of two target variables, respectively.
\item
  Chapter~\ref{phenotypes} (\emph{Visual Identification of Informative Features}) describes our approach to identify distinct tinnitus phenotypes with parameter-free clustering, and presents novel visualizations to juxtapose phenotypes in a high-dimensional feature space and to explore phenotype-specific characteristics.
\item
  Chapter~\ref{evo} (\emph{Constructing Evolution Features to Capture Study Participant Change over Time}) present a framework for cohort analysis in longitudinal cohort studies which constructs ``evolution features'' from latent temporal information describing the change of cohort participants over time. We show that exploiting these novel features improves the generalization performance of classification models and report on results for the longitudinal cohort study.
\item
  Chapter~\ref{diabfoot} (\emph{Feature Extraction From Short Temporal Sequences for Clustering}) present an approach to build representations from short temporal sequences via clustering by the example of pressure- and posture-dependent plantar temperature and pressure in patients with diabetic foot syndrome.
\item
  Chapter~\ref{iml} (\emph{Post-Hoc Interpretation of Classification Models}) focuses on making already learned, complex classification models understandable to the domain experts. We provide a workflow that combines classification of high-dimensional medical data and model explanation using post-hoc interpretation methods.
  To this end, we use Shapely value explanations (SHAP), LASSO coefficients, and partial dependency graphs.
  Our approach delivers statistics and visualizations representing global feature importance, instance-individual feature importance, and subpopulation-specific feature importance, all of which help illuminate complex black-box machine learning models.
  We report our results on three applications: (i) tinnitus-related distress in tinnitus patients, (ii) depressivity in tinnitus patients, and (iii) rupture risk in intracranial aneurysms.
\item
  Chapter~\ref{gender} (\emph{Subpopulation-Specific Learning and Post-Hoc Model Interpretation}) describes an approach that examines how subpopulations differ with respect to their most predictive characteristics in temporal data. To do so, we derive a post-hoc interpretation measure to assess the difference in association of predictors between two subpopulations. We report results for CHA on gender differences (subpopulations of female and male patients) for the two outcomes of tinnitus-related distress and depression, and the effect of treatment for both outcomes.
\item
  Chapter~\ref{summary} (\emph{Conclusion and Future Work}) concludes the thesis by giving a summary of the contributions and detailed perspectives for the presented work.
\end{itemize}

\hypertarget{background}{%
\chapter{Medical Background \& Datasets}\label{background}}

\textbf{Outline}

\begin{itemize}
\tightlist
\item
  Types of medical studies
\item
  Short introduction to the medical applications
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{fundamental-terms-and-study-types}{%
\section{Fundamental terms and study types}\label{fundamental-terms-and-study-types}}

Typical goals of medical research include identifying long-term determinants and protective factors for an outcome of interest, discovering sub-populations with increased disease prevalence, studying intervention effects by generating statistical (causal) models that explain cause-effect relationships.

Traditional data analysis pipelines are usually structured as follows:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  A medical scholar formulates a hypothesis based on observations in clinical practice or current research. Examples: how does a risk factor like alcohol abuse affects the prevalence of a certain outcome? Which effect does a novel therapy have on patients with depressive symptoms?
\item
  To investigates this hypothesis, a small set of relevant variables is chosen, which might be controlled for confounders.
  Adequate data are collected.
  Variable selection may incorporate controlling for confounders.
\item
  The strength of the associations between the selected variables and the outcome is assessed with regression models and statistical methods.
\item
  Based on the results, inferential statistics are computed and conclusions are drawn which may foster the implementation of novel preventive measures or the application of suited treatment forms to high-risk patients.
\end{enumerate}

Primary medical research can be classified into basic, clinical and epidemiological studies.

\hypertarget{basic-research}{%
\section{Basic research}\label{basic-research}}

Basic medical research (or experimental research) aims to improve understanding of cellular, molecular and physiological mechanisms of the human health and diseases by conducting cellular and molecular investigations, animal experiments, studies on drug and material properties in strictly controlled laboratory environments \autocite{Roehrig:Arztebl2009}.
To investigate (causal) effects of one or more variables of interest on the outcome, all other variables are usually kept constant and only those variables of interest are varied.
The carefully standardized experimental conditions of basic medical studies ensure a high internal validity, but these conditions often cannot be easily transferred to clinical practice without a loss of generalizability of the results.

!!! Basic research also includes the development and improvement of analytical (e.g., analytical determination of enzymes, markers, genes) and imaging measurement procedures (e.g., computer tomography, magnetic resonance tomography) as well as gene sequencing (e.g., the relationship between eye color and a specific gene sequence), and the development of biometric procedures such as statistical test procedures, modeling and statistical evaluation strategies.

A brief overview of important medical study types is given (see also \autocite{thiese2014observational,Roehrig:Arztebl2009}.

\hypertarget{clinical-studies}{%
\section{Clinical studies}\label{clinical-studies}}

Clinical studies are generally classified into \emph{interventional} (also: \emph{experimental}) studies and \emph{non-interventional} (also: \emph{observational}) studies.

The general goal of an interventional study is to compare different treatments within a patient population whose members differ as little as possible, except for the treatment branch.
A common example is a pharmaceutical study which aims to validate the efficacy and harmlessness of a drug by investigating or establishing main and adverse effects, resorption, metabolization and excretion of the drug.
Selection bias can be avoided by appropriate measures, in particular by randomly assigning patients to the groups.

The treatment can be a drug, a surgical procedure or the therapeutic use of a medical product (e.g.~stent), but also physiotherapy, acupuncture, psychosocial intervention, rehabilitation, training or diet.

A \emph{randomized controlled trial} (RCT) is considered as study design ``gold-standard'' as it minimizes selection bias a. by randomly allocating patients to treatment and control group, and b.by ensuring equal distribution of known and unknown influencing variables (confounders), such as risk factors, comorbidities and genetic variabilities.
Thus, RCTs are suitable for obtaining an unambiguous answer to a clear question and proving causality.

In contrast, non-interventional clinical studies are patient-related observational studies in which patients receive an individually defined therapy (or all patients receive the exact same therapy).
Data analysis often takes place retrospectively.
Observational studies are often used to generate hypotheses.
- example: study that investigates the regular use of drugs in therapies
the treatment, including diagnosis and monitoring, does not follow a pre-defined trial protocol, but only medical practice

\hypertarget{prospective}{%
\subsubsection{prospective}\label{prospective}}

Prospective studies can be recognized by the chronological sequence of hypothesis generation and data collection.
First, the hypotheses to be tested are determined, for example with respect to a new treatment procedure.
Then, data are collected specifically for hypothesis testing.
By formulating testable hypotheses first, it can be ensured that the research questions can actually be answered with the measured data.

\hypertarget{retrospective}{%
\subsubsection{retrospective}\label{retrospective}}

data collection has already taken place before the start of the study

examples:

digitalized medical databases

\hypertarget{epidemiological-studies}{%
\section{Epidemiological studies}\label{epidemiological-studies}}

TODO: Siehe Buch

Epidemiological studies are usually interested in the distribution and temporal change of the frequency of diseases and their causes in the general population or in sub-populations.
Analogous to clinical studies, epidemiology also distinguishes between experimental and observational studies. ???

A \emph{cross-sectional} study (or prevalence study) is conducted only once whereas in a \emph{longitudinal} study, the same study is carried out at several points in time and the results of the individual \emph{waves} of investigation are compared.
Longitudinal studies are further categorized into \emph{trend} and \emph{panel} design.
In a \emph{trend} study, each wave can involve a different participant sample, i.e., an individual participant is not followed over time.
In contrast, a \emph{panel} study investigates the same population at multiple points in time which allows to also measure \emph{intra-individual} temporal changes.

\hypertarget{controlled-vs.-uncontrolled}{%
\subsubsection{controlled vs.~uncontrolled}\label{controlled-vs.-uncontrolled}}

\begin{itemize}
\tightlist
\item
  epidemiological vs clinical vs experimental study
\item
  what is a cohort
\item
  samples (clinical samples, medical research samples) -\textgreater{} validity?
\end{itemize}

\hypertarget{challenges}{%
\section{Challenges}\label{challenges}}

\begin{itemize}
\tightlist
\item
  heterogeneity (numeric, categorical)
\item
  pre-processing of raw image data (non-tabular data)
\item
  different requirements for different studies
\end{itemize}

\hypertarget{medical-applications}{%
\section{Medical Applications}\label{medical-applications}}

\hypertarget{ship}{%
\subsection{The Study of Health in Pomerania (SHIP)}\label{ship}}

After the reunification of Germany, it was found that life expectancy was considerably lower in the east than in the west \autocite{volzke2015prevalence}.
Furthermore, there were regional differences within the former East Germany, with the lowest life expectancy in the northeast \autocite{volzke2015prevalence,wiesner2004life}.
To investigate the causal relationship between the high mortality in the northeastern German population and its risk factors, the research center for community medicine established the \emph{Study of Health in Pomerania} (SHIP) \autocite{Voelzke:SHIP11}, a longitudinal epidemiological study of two independent cohorts in the northeast of Germany.
SHIP attempts to describe a wide spectrum of health-related conditions rather than focusing on a specific target disease \autocite{Voelzke:SHIP11}.
In particular, major study goals comprise investigations regarding prevalence of common diseases and their risk factors, correlation and interaction between risk factors and diseases, progression from subclinical to manifest diseases, identification of subgroups with elevated health risk, prediction of incidental diseases, as well as utilization and costs of medical devices. TODO: shorten

Cohort inclusion criteria were age from 20 to 79 years, main residency in the study region and German nationality.
Participants of SHIP underwent an extensive, recurring (ca. every 5-6 years) examination program that encompasses personal interviews, body measurements, exercise electrocardiogram, laboratory analysis, ultrasound examinations and full body magnetic resonance tomography (MRT).
Figure @ref:(fig:ship-sankey-plot) illustrates participant response and age distribution of show-ups across study waves.
Baseline examinations for the first cohort were performed between 1997 and 2001 (SHIP-0, N=4308).
Followup examinations were done in 2002-2006 (SHIP-1, n=3300), 2008-2012 (SHIP-2, n=2333), 2014-2016 (SHIP-3, n=1718) and since 2019 (SHIP-4).
Baseline information for a second, independent cohort (SHIP-Trend-0, N=4420) was collected between 2008 and 2012 and a followup was conducted between 2016 and 2019.
Major strengths of SHIP are a high level of quality assurance, standardized examination protocols, and a high cohort representativeness.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/02-ship-sankey} 

}

\caption{\textbf{Participation response and age distribution of show-ups across study waves in SHIP.} Average population age is shown as black vertical line in the histograms.}\label{fig:02-ship-sankey}
\end{figure}

The examination program changed across waves.
For example, magnetic resonance imaging (MRI) was only conducted since SHIP-2; liver ultrasound was carried out in SHIP-0 and SHIP-2, but not in SHIP-1; dermatological examinations were conducted in SHIP-1 and SHIP-2, but not in SHIP-0.

In our analyses, we focus on the disorder hepatic steatosis, also known as fatty liver, a condition which is characterized by a high accumulation of fat in the liver and present in ca. 30\% of all adults \autocite{Voelzke:SHIP11,volzke2005hepatic}.
Risk factors include alcohol abuse, obesity, metabolic syndrome, diabetes, and hyperlipidemia \autocite{antunes2019fatty}.
A liver biopsy is considered as diagnosis gold standard \autocite{antunes2019fatty}, but is associated with moderate risk for the patient.
Non-invasive diagnosis forms include MRT, CT and ultrasound.
Since hepatic steatosis is mostly asymptomatic, it often remains undiscovered which may develop into a more serious disease, such as steatohepatitis, cirrhosis, liver cell carcinoma or liver failure.

\hypertarget{the-diabetic-foot-clinical-trial-df}{%
\subsection{The Diabetic Foot Clinical Trial (DF)}\label{the-diabetic-foot-clinical-trial-df}}

--\textgreater{}

\emph{Diabetic foot syndrom} is an umbrella term of foot-related problems in diabetes patients.
Up to one out of four diabetes patients develop a foot ulcer during their lifetime \autocite{RN1} with many of them facing amputations in the next four years \autocite{RN2}.
More than 85\% of foot amputations relate to foot ulcers \autocite{RN3,RN4}.
The rate of foot amputations among diabetic patients has been estimated to be 17--40 times higher than in the general population \autocite{fard2007assessment}.
DFS patients are predisposed to a peripheral sensoric neuropathy, which, for example, have the consequence that patients are unaware of the temperature of their feet, or the pressure they apply on them.
Affected individuals may even injure themselves without noticing.
Excessive plantar pressure loads may aggravate tissue destruction, increasing the lifetime risk of a foot ulceration \autocite{SinghEtAl:PreventingFootUlcers2005}.
However, the understanding of the pathomechanisms underlying tissue destruction without trauma is limited.

At the Medical Faculty of the Otto von Guericke University Magdeburg, an experimental study with 31 healthy volunteers and 30 diabetes patients diagnosed with severe polyneuropathy was conducted to quantify pressure- and posture-dependent changes of plantar temperatures as a surrogate of tissue perfusion.
For this purpose, plantar pressure and temperature changes in the feet were recorded during extended phases of standing.
Custom-made shoe insoles
equipped with eight temperature sensors and eight pressure sensors at preselected positions were used for data acquisition (\ref{fig:02-df-infrared-sole-sensor-positions-plot} (a)).
The insoles were positioned into closed protective shoes specifically developed for diabetes patients.
Within such shoes the temperature increases over time due to exchange with the body temperature of the user and is also affected by the environmental temperature.
To closely monitor the in-shoe temperature changes one sensor was placed at the bottom of the insole without contact to the feet, which was denoted ``ambient temperature sensor''.



\begin{figure}
\includegraphics[width=1\linewidth]{figures/02-df-infrared-sole-sensor-positions} \caption[fig.scap]{(a) Sensor positioning in relation to foot placement. (b) Infrared images of a healthy volunteer in a seated position without pressure application to the feet (before) and following positioning of a 20 kg weight on both upper thighs. A time-dependent decrease of temperature was detected predominantly in the forefoot, visualized by yellow color during the pressure load. Within 1 min of pressure release, a rapid temperature rise was detected.}\label{fig:02-df-infrared-sole-sensor-positions-plot}
\end{figure}

The data acquisition started immediately after putting on the shoes.
The participants were asked to follow a predefined sequence of actions, that are alternating postures of standing (stance phase) and seating (pause).
The sessions consisted of six stance episodes, lasting 5, 10, 20, 5, 10 and 20 minutes each, separated by seating episodes lasting 5 min each.
The participants were instructed to apply pressure equally on both feet while standing.
Participants did not receive immediate feedback of the actual pressure application during the sessions, however study nurses verbally encouraged to keep the pressure without release while standing.
In the seated position the participants were instructed to release pressure for 5 minutes, while still keeping contact to the insole.
The participants were explicitly asked to adhere to these instructions, e.g.~they should not temporarily release pressure during a stance episode.
The study protocol furthermore encompassed that the measurements were performed twice, once at room temperature of ca 22°C and once outdoors with an ambient temperature of ca. 16°C.
The two measurements were performed on two independent days.

The termographic images in Figure \ref{fig:02-df-infrared-sole-sensor-positions-plot} (b) exemplarily visualize changes of plantar temperature, for a healthy volunteer in a seated position, before pressure application (1), after placement of a 20 kg weight on the front of the upper thighs (2-6), and after removal of the additional weight (7-8).
During pressure load, a gradual temporal temperature decrease was detected predominantly in the forefoot.
After pressure release, a rapid temperature rise within 1 min was observed.

\hypertarget{the-intracranial-aneurysm-angiography-image-dataset-iad}{%
\subsection{The Intracranial Aneurysm Angiography Image Dataset (IAD)}\label{the-intracranial-aneurysm-angiography-image-dataset-iad}}

Retrospective clinical

Intracranial aneurysms are pathologic dilations of the intracranial vessel wall, often in form of a dilation.
They bear a a risk of rupture which then lead to subarachnoidal hemorrhages with often fatal consequences for the patient.
Since treatment can also cause severe complications, extensive studies were conducted to assess the patient-individual rupture risk based on various parameters, including aneurysm symptomatology, size and location, as well as patient age and gender \autocite{Wermer2007}.
Further studies identified parameters, such as aspect ratio, undulation index and nonsphericity index as statistically significant with respect to aneurysm rupture status \autocite{Dhar2008,Xiang2011}.\\
However, although these studies allow for a retrospective analysis, the clinician needs further guidance in case an asymptomatic aneurysm (as accidental finding) was detected and the rupture risk should be determined.

We developed methods for the \emph{``Intracranial Aneurysm Angiography Image Dataset''} (IAD) comprising 3D rotational angiography data from 74 patients (age: 33-85 years, 17 male and 57 female patients) of the university hospital of Magdeburg, Germany, adding up to a total of 100 intracranial aneurysms.
We identified two primary goals for this dataset: (i) build models that can accurately predict rupture status based on morphological parameters only, and (ii) assess the importance of these parameters to the models with optimal accuracy.

Inspired by the results of Baharoglu et al.~\autocite{BaharogluEtAl:Neurosurgery2012} who found differences between sidewall and bifurcation aneurysms (cf.~Figure~\ref{fig:02-aneurd-sw-bf}) regarding the relationship of several morphological parameters and rupture status, we learn distinct models for the subset of sidewall aneurysms (9 (37.5\%) of 24 ruptured) and for the subset of bifurcation aneurysms (29 (46.8\%) of 62 ruptured).
Moreover, we run experiments on a combined group (43 of 100 ruptures) which includes 14 samples that could not be clearly determined to be either sidewall or bifurcation aneurysms.



\begin{figure}

{\centering \includegraphics[width=0.67\linewidth]{figures/02-aneurd-sw-bf} 

}

\caption{\textbf{Sidewall and bifurcation aneurysm.} Illustration of a sidewall aneurysm at the side of the parent vessel wall (left) and a bifurcation aneurysm at a vessel bifurcation (right).}\label{fig:02-aneurd-sw-bf}
\end{figure}

\hypertarget{cha}{%
\subsection{The Tinnitus Patients Observational Therapy Study Dataset (CHA)}\label{cha}}

--\textgreater{}
--\textgreater{}

Tinnitus is the perception of a phantom sound in absence of an external sound source.
It is a complex multi-factorially caused and maintained phenomenon, and estimated to affect between 10\% and 15\% of the adult population \autocite{Baguley:Tinnitus2013}.
The associated annual economic burden amounts to US\$19.4 billion in the
United States \autocite{Bhatt:TinnitusAnxietyDepression2017} and €6.8 billion in the Netherlands alone \autocite{Maes:TinnitusCosts2013}.
Clinical assessment of tinnitus is challenging due patient heterogeneities with respect to perception of tinnitus (laterality, pitch, sound characteristics, frequency, permanence, chronicity), risk factors (including hearing loss, temporomandibular joint disorder, aging), comorbidities (including hyperacusis, depression, sleep disorders), perceived distress, and treatment response \autocite{Cederroth2019}.
These differences make the identification of a suitable and effective form of treatment difficult.
Currently, there does not exist a therapy gold standard: sound therapy (masking), informational counseling (minimal contact education), cognitive behavioral therapy and tinnitus retraining were found to be effective for some patients, but there is also evidence that not all patients benefit equally from these treatment forms \autocite{Hobson:SoundTherapy2012,Kroner:Counseling2003,Henry:Counseling1996,Martinez:CBTReview2007,Phillips:TRT2010}.

Due to the heterogeneous nature of the tinnitus symptom as well as the unclear
evidence-base as to its treatment and management, the identification of patient subgroups is vital to stratify individual pathophysiology and treatment pathways \autocite{Langguth:LCA2017,Tyler:TinnitusClustering2008,Landgrebe:TRI2010}.

The ``tinnitus patients observational therapy study dataset'' (\emph{CHA}) comprises self-report data from 4,103 tinnitus patients who had been treated at the Tinnitus Center of Charité University Medicine Berlin between January 2011 and October 2015.
All patients were 18 years of age or older and had been suffering from tinnitus for at least 3 months.
Exclusion criteria were presence of acute psychotic illness or addiction disorder, deafness and insufficient knowledge of the German language.
Treatment comprised a multimodal 7-day program that included intensive and daily informational counseling, detailed ear-nose-throat as well as psychological diagnostics, cognitive behavior therapy interventions, hearing exercises, progressive muscle relaxation and physiotherapy.
At baseline (\emph{T0}; before therapy commencement) and after treatment (\emph{T1}), patients were asked to complete multiple self-report questionnaires.
These questionnaires were selected to obtain a comprehensive tinnitus assessment, including tinnitus-related distress and the psychosomatic background of tinnitus with anxiety, depression, general quality of life and experienced physical impairments.
Table~\ref{tab:02-cha-questionnaires} provides an overview of all questionnaires used in the analysis.
Most questionnaires contain multiple-choice items with answers on a Likert scale. For example, the TQ contains 52 statements, such as ``I am unable to enjoy listening to music because of the noises.'', and respondents can give 3 possible answers: ``not true'' (encoded as 0), ``partly true'' (1) and ``true'' (2).
Some questionnaires also comprise aggregated variables, called ``subscales'' and ``total scores''.
For example, the TQ total score (TQ\_distress) is calculated as sum of 40 item values, where 2 items are used twice \autocite{GoebelHiller:TF1998}, yielding a value range from 0 to 84, where higher values represent higher tinnitus-related distress.
The cutoff value 46 \autocite{GoebelHiller:TF1998} is used to distinguish between \emph{compensated} (0-46) and \emph{decompensated} (47-84) tinnitus.
Further, for each questionnaire, the average time to answer an item was recorded. Figure @ref:(fig:02-cha-patient-demographics-plot) provides a graphical illustration of demographics for 3,803 (92.7\%) patients with complete data for the SOZ questionnaire and TQ score.



\begin{longtable}[]{@{}rlllr@{}}
\caption{\label{tab:02-cha-questionnaires}Description of questionnaires that form the basis for CHA.}\tabularnewline
\toprule
\begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
~~\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
\textbf{Name}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
\textbf{Scope}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
\textbf{Scales}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
\textbf{\textbar F\textbar{}}\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
~~\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
\textbf{Name}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
\textbf{Scope}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
\textbf{Scales}\strut
\end{minipage} & \begin{minipage}[b]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
\textbf{\textbar F\textbar{}}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
1\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
ACSA: Anamnestic Comparative Self-Assessment \autocite{Bernheim:ACSA1993}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Quality of life\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
1\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
2\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
ADSL: General Depression Scale \autocite{Radloff:adsl1977,Hautzinger:ADSL2003}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Depressive symptoms\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
22\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
3\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
BI: Berlin Complaint Inventory \autocite{Horhold:BI1997}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
General well-being, autonomic nervous system, pain and emotionality\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Exhaustion, abdominal symptoms, limb pain, heart symptoms\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
29\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
4\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
BSF: Berlin Mood Questionnaire \autocite{Horhold:BSF1993}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Mood\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Anger, anxious depression, apathy, elevated mood, fatigue and mindset\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
36\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
5\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
ISR: ICD-10 Symptom Rating \autocite{Tritt:ICD2008}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Mental disorders\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Anxiety, depression, obsessive-compulsive syndrome, somatoform syndrome, eating disorder, additional items\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
36\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
6\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
PHQK: (Short-form) Patient Health Questionnaire \autocite{Spitzer:PHQK1999}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Symptoms of depression and anxiety\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
17\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
7\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
PSQ: Perceived Stress Questionnaire \autocite{Fliege:PSQ2005}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Stress\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Demand, tension, joy, worries\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
35\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
8\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
SES: Pain Perception Scale \autocite{Geissner:SES1996}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Pain\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Affective pain, sensoric pain\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
26\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
9\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
SSKAL: Visual Analogue Scales Pain\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Pain impairment, frequency and intensity\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
3\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
10\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
SF8: Short Form 8 Health Survey \autocite{Bullinger:SF2008}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Health-related quality of life\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Bodily health, overall health, mental health, physical functioning, role emotional, role physical, vitality, mental component, physical component\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
18\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
11\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
SOZK: A socio-demographics questionnaire \autocite{brueggemann:sozk-reference}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Gender, partnership status, education, employment status, among others\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
27\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
12\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
SWOP: Self-Efficacy- Optimism-Pessimism Scale questionnaire \autocite{Scholler:SWOP1999}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Self efficacy, optimism, pessimism\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
12\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
13\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
TINSKAL: Visual analogue scales\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Tinnitus loudness, frequency and distress\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
3\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
14\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
TLQ: Tinnitus Localization and Quality questionnaire \autocite{Goebel:TLQ1992}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Location (left, right, bilateral, entire head) and sound of tinnitus\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
--\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
8\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.04}}\raggedleft
15\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.25}}\raggedright
TQ: Tinnitus Questionnaire (German version) \autocite{GoebelHiller:TF1998}\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.22}}\raggedright
Tinnitus-related distress and tinnitus severity\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.44}}\raggedright
Emotional and cognitive burden, persistence of sound, hearing difficulties, sleep difficulties and somatic complaints\strut
\end{minipage} & \begin{minipage}[t]{(\columnwidth - 4\tabcolsep) * \real{0.05}}\raggedleft
60\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}



\begin{figure}
\includegraphics[width=1\linewidth]{figures/02-cha-patients-summary} \caption[fig.scap]{\textbf{Patient demographics (CHA).} Overview of patient demographics by degree of tinnitus distress measured before therapy commencement.}\label{fig:02-cha-patient-demographics-plot}
\end{figure}

--\textgreater{}
--\textgreater{}
--\textgreater{}
--\textgreater{}

\hypertarget{part-subpopulation-discovery-in-high-dimensional-data}{%
\part{SUBPOPULATION DISCOVERY IN HIGH-DIMENSIONAL DATA}\label{part-subpopulation-discovery-in-high-dimensional-data}}

\hypertarget{imm}{%
\chapter{Interactive Discovery and Inspection of Subpopulations}\label{imm}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We study the separation between two positive and a negative outcome for a disease in epidemiological data.
Our goal is to identify compact subpopulations that are as pure as possible with respect to the target variable.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

Uli Niemann, Henry Völzke, Jens-Peter Kühn, and Myra Spiliopoulou. ``Learning and inspecting classification rules from longitudinal epidemiological data to identify predictive features on hepatic steatosis''. In: \emph{Expert Systems with Applications} 41.11 (2014), pp.~5405-5415. DOI: \href{https://doi.org/10.1016\%2Fj.eswa.2014.02.040}{10.1016/j.eswa.2014.02.040}.

\end{infobox}

\hypertarget{motivation-and-comparison-to-related-work}{%
\section{Motivation and Comparison to Related Work}\label{motivation-and-comparison-to-related-work}}

Medical decisions on diagnosis and treatment of multifactorial conditions such as diseases and disorders are based on clinical and epidemiological studies; the latter accommodate information on participants with and without a condition and allow for learning discriminative models and, in the longitudinal design, for understanding the progress of the condition.
For example, several studies identified risk factors (like obesity or alcohol consumption) and co-morbidities (like cardiovascular diseases) associated with hepatic steatosis (``fatty liver'') \autocite{IttermannEtAl:Thyroid2012,LauEtAl:2010,StickelEtAl:2011,Targher:2010,Markus:2013}.
However, these studies identified risk factors and associated outcomes that pertain to the whole population.
Our work originated from the necessity to identify such factors and outcomes for subpopulations and thus to promote personalized diagnosis and treatment, as expected in personalized medicine \autocite{Hingorani:2013,Voelzke:Cardiol2013}.

Classification on subpopulations was studied by Zhanga and Kodell \autocite{AIM13} who pointed out that classifier performance on the whole dataset can be low if the complete population is very heterogeneous.
Therefore, they first trained an ensemble of classifiers, and then used the predictions of each ensemble member to create a new feature space.
They performed hierarchical clustering to partitioned the instances into three subpopulations: one where the prediction accuracy is high, one where it is intermediate and one where it is low.
With this approach, Zhanga and Kodell split the original dataset into subpopulations that are easy or difficult to classify.
While the method seems appealing in general, it appeared unsuitable for the three-class problem of the SHIP data which exhibits a very skewed distribution, hence it is clear that low classification accuracy is (partially) caused by the skew.
Hence, we studied the dataset exploratively \emph{before} classification, to identify less skewed subpopulations, and exploratively \emph{after} classification, to identify - inside each subpopulation - variables which are highly associated to the outcome.

Pinheiro et al.~performed association rule discovery on patients with liver carcinoma \autocite{PinheiroEtAl:ICCABS13}.
The authors pointed out that early detection of liver cancer may help reducing the five-year mortality rate, but early detection is difficult, because in the onset of a liver carcinoma, the patient often observes no symptoms \autocite{PinheiroEtAl:ICCABS13}.
Pinheiro et al.~leveraged the association rule algorithm FP-growth \autocite{Han:FPGrowth00} to discover high-confidence association rules and high-confidence classification rules regarding mortality in liver cancer patients.
We also considered association rules promising for the analysis of medical data, because they are easy to compute and deliver results that are understandable by humans.
Therefore, we also used association rules as baseline method, though for epidemiological data and for classification rather than mortality prediction.
To use association rules for classification, we specified that the rule consequent is the target variable.

Zhang et al. \autocite{Zhang:CAVA2015} addressed the increasing technical challenges of medical expert-driven subpopulation discovery due to increasingly large and complex medical data which often comprise information of hundrets of variables for thousands of patients in form of tables, images or text.
Whereas in the past it was sufficient for a physician to have working knowledge of basic statistics and a spreadsheet software like Microsoft Excel to analyze a small table with patient data, nowadays more effective and efficient approaches are required for management, analysis and summarization of very large medical data.\\
As a result, domain experts usually rely on technical experts to help them perform these tasks.
However, this back and forth process is often slow, tedious and expensive.
Hence, it would be better to equip the domain expert with a technical tool that allows them to quickly perform exploratory analyses on their own.
Zhang et al. \autocite{Zhang:CAVA2015} presented CAVA, a system that incorporates miscellaneous subgroup visualizations (called ``Views'') and analytic components (called ``Analytics'') for subgroup comparison.
The main panel in Figure \ref{fig:03-cava} shows one of the Views: a flow diagram \autocite{wongsuphasawat2012exploring} of patient subgroups with the same sequence of symptoms.
Users can obtain additional summaries by interacting with the visualization, e.g.~by placing one of the boxes in the flow diagram onto one of the entries in the Analytics panel via drag-and-drop.
Further, user can expand the selected cohort by letting the tool search patients who do not strictly match the current inclusion criteria but are somewhat \emph{similar} to the current patient subpopulation \autocite{ebadollahi2010predicting}.



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/03-cava} 

}

\caption{CAVA's graphical user interface. The flow chart visualizes cardiac patient subgroups organized by shared symptom occurrence. Color represents hospitalization risk. The user can switch between graphical representations and data processing methods via drag-and-drop operations. The top-right panel provides detailed information for the currently selected patients. The bottom-right panel contains a provenance graph that allows the user to undo operations and to revisit previous interaction steps. (from: \autocite{Zhang:CAVA2015})}\label{fig:03-cava}
\end{figure}

Krause et al. \autocite{Krause:Prospector2016} argued that model selection should not be based solely on global performance metrics like accuracy, because these statistics do not contribute to a better understanding of the model's reasoning.
Furthermore, a complex yet very accurate model does not automatically warrant actionable insights.
Krause et al.~propose Prospector \autocite{Krause:Prospector2016}, a system that provides diagnostic components for complex classification models based on the concepts of partial dependence (PD) plots \autocite{Friedman:PDP2001}.
PD plots are a popular tool to visualize the marginal effect of a feature on the predicted outcome probability.
Briefly, each point of a PD curve represents the average prediction of the model over all observations, given that these observations had a fixed value for a feature of interest.
A feature whose PD curve has a high range or variability is considered to be more impactful to the model prediction than a feature with a flat PD curve.
Closely related to PD plots are individual conditional expectation (ICE) plots \autocite{Goldstein:ICE2015}, which display one curve for every observation, and thus help to uncover contrasting subpopulations who might ``average out'' in a PD plot.
Prospector combines PD and ICE curves to depict the relationship between a feature and the model prediction on a (\emph{global}) model level and a (\emph{local}) patient-individual level.
Further, a color bar is provided as more compact alternative to ICE curves (Figure \ref{fig:03-prospector}) (a).
A stacked bar chart shows the distribution of predicted risk scores for each study group (Figure \ref{fig:03-prospector} (b)), and the user can click on a specific decile to get a list of individual patients with their exact prediciton score and label.
In this way, patients whose prediction score is close to the decision boundary can be further investigated.
The authors calculate for each feature the ``most impactful feature change'': given the actual feature value of a patient, they identify a near counterfactual value that led to a large change in the predicted risk score, by minimizing difference to the original feature value and maximizing predicted risk score.
The top-5 of these so called ``suggested changes'' are displayed -- separately for inreasing and decreasing disease risk -- in a table (cf.~Figure \ref{fig:03-prospector} (c)), and incorporated as interactive elements into the IC color bars (cf.~Figure \ref{fig:03-prospector} (d)).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/03-prospector} 

}

\caption{A selection of Prospector's model diagnostics. (a) The upper chart depicts two curves for the feature ``age'': the gray partial dependence (PD) curve represents the marginal prediction of the model over all patients whereas the black individual conditional expectation (ICE) curve illustrates the effect of counterfactual age values on the predicted diabetes risk for an example patient. The histogram shows the age distribution. The color bar placed beneath is a compact depiction of the ICE curve above; the encircled value represents the feature value of the selected patient. (b) Stacked bars show the distribution of predicted risk scores for each study group. Clicking on one of the bars opens a table that shows ID, predicted risk and true label for all patients belonging to the selected prediction risk decile. (c) Summary table of the most ``impactful feature changes'' for decreasing (upper group) and increasing (lower group) predicted risk: each row shows the actual feature value and the ``suggested change'', i.e., a similar but counterfactual value that led to a considerable change in predicted risk. (d) Multiple PD color bars augmented with suggested changes (white encircled labels). (adapted from: \autocite{Krause:Prospector2016})}\label{fig:03-prospector}
\end{figure}

Pahins et al. \autocite{Pahins:COVIZ2019} presented COVIZ, a system for cohort construction in large spatiotemporal datasets.
COVIZ comprises mechanisms for explorative data analysis of treatment pathways and event trajectories, visual cohort comparison and visual querying.
One of COVIZ' design goals is being fast, e.g.~by using efficient data structures like Quantile Data Structure \autocite{de2019real} to ensure low latency for all computational operations and hence suitablity for large datasets.

Bernard et al. \autocite{bernard2015visual} proposed a system for cohort construction in temporal prostate cancer cohort data which comprised visualizations for both subgroups and individual patients.
To guide users during exploration, visual markers indicate interesting relationships between attributes derived from statistical tests.

\hypertarget{other-related-work}{%
\subsection{Other related work}\label{other-related-work}}

\begin{itemize}
\tightlist
\item
  IIComPath: \autocite{Corvo2020}
\end{itemize}

\hypertarget{previous-work-on-ship}{%
\subsection{Previous Work on SHIP}\label{previous-work-on-ship}}

Klemm et al. \autocite{Klemm:RegressionHeatmap2015} presented a ``3D regression cube'' system which enables interactive exploration of feature correlations in epidemiological datasets.
The system generates a large number of multiple regression models from various combinations of one dependent and two independent variables and displays their goodness of fit in a three-dimensional heatmap.
The system allows the user to modify the regression equation, for example, by changing the number of independent variables, by specifying interaction terms or by fixing one of the variables to reduce computational complexity or to specifically focus on a variable of interest.
Our approach is also capable of identifying variables that are highly associated with the outcome, but we search for subpopulation-specific relationships instead of generating a global model for the whole dataset, and we further provide predictive value ranges.

Klemm et al. \autocite{Klemm14} presented a system which combines visual representations of non-image and image data.
They identify clusters of backpain patients on the SHIP data.
As we fixed hepatic steatosis as outcome, we rather opted to build supervised models and classification rules that directly captured the relationships between predictive variables and the outcome.

Alemzadeh et al. \autocite{eurova.20171118} presented S-ADVIsED, a system for interactive exploration of subspace clusters, incorporating various visualization types, such as donut charts, correlation heatmaps, scatterplot matrices, mosaic charts and error bar graphs.
While S-ADVIsED requires the user to input the mining results obtained in advance outside of the system, our tool allows for an expert-driven interactive subpopulation discovery.\\

Hielscher et al. \autocite{Hielscher16} developed a semi-supervised constrained-based subspace clustering algorithm to find diverse sets of \emph{interesting} feature subsets using the SHIP data.
To guide the search for interesting feature subsets, the expert can provide their domain knowledge in form of instance-level constraints, thus forcing pairs of instances to be assigned either to the same or a different cluster.
Hielscher et al. \autocite{Hielscher2018} extended their work and introduced a mechanism to compare subpopulations between independent cohorts.

As a consequence, thesame group developed a constrained-based technique, where theclustering is guided by a small set of constraints, given by an expert{[}HNP*18{]}. As an example, for a few pairs of participants diagnosedwith fatty liver, the expert specifies that these participants must be inthe same clusters, whereas some other pairs of participants are forcedto be in different clusters since one in each pair is diagnosed withthe disorder and the other is not. This semi-supervised subspaceclustering turned out to yield relevant results for epidemiologists{[}HNP*18{]}.

\textbf{TODO:} Preim: Visual analytics of image-centric cohort studies in epidemiology

\hypertarget{subpopulation-discovery-workflow-and-interactive-mining-assistant}{%
\section{Subpopulation Discovery Workflow and Interactive Mining Assistant}\label{subpopulation-discovery-workflow-and-interactive-mining-assistant}}

Our subpopulation discovery workflow is presented hereafter.
The dataset used for population partitioning and class sepration on the target variable hepatic steatosis come from the Study of Health in Pomerania (SHIP) which is described in Subsection \ref{ship}.

In subsection \ref{imm-target}, explains origin and availability of the target variable.
In subsection \ref{imm-partitioning}, the motivation for partitioning of the data and the partitioning steps are presented.
Then, the used methods for class separation on the whole dataset and on the partitions are discussed in Subsection \ref{imm-classification}.

\hypertarget{imm-target}{%
\subsection{Target variable}\label{imm-target}}

The target variable is derived from the participant's liver fat concentration computed with magnetic resonance imaging (MRI).
At the time of writing the original manuscript, MRI results were available only for 578 SHIP-2 participants.
We use the data of these participants for classifier learning, while our interactive ruleInspector (cf.~section \textbf{???}) also juxtaposes these data to the data of the remaining 1755 participants, for whom the MRI recordings were not made available.

Participants with a liver fat concentration of 10\% or less are mapped to class A (``negative'' class, i.e., absence of the disorder), values greater than 10\% and lower than 25\% are mapped to class B (increased liver fat / fatty liver tendency), and values greater than 25\% are mapped to class C (high liver fat).
We consider classes B and C as ``positive''.
Although the cut-off value of 10\% is higher than the value of 5\% suggested by \autocite{KuehnEtAl:2011} for separation between subjects with and without hepatic steatosis.
However, the primary interest from the medical perspective was the identification of important variables for individuals that are likely to be ill.~
The selection of a high cut-off value exacerbated class imbalance and made the data analysis more challenging. Figure \ref{fig:03-fatty-liver-mosaic} depicts the class distribution by gender.
Out of the 578 participants, 438 belong to class A (ca. 76\%), 108 to B (ca. 19\%) and 32 to C (ca. 6\%).
Men are more likely to exhibit increased or high liver fat than women (30.7\% vs.~18.8\% in classes B or C).



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/03-fatty-liver-mosaic} 

}

\caption{Gender-specific distribution of the target variable.}\label{fig:03-fatty-liver-mosaic}
\end{figure}

Next to the target variable, the dataset contains 66 variables extracted from participants' questionnaire answers and medical tests (cf. \autocite{Voelzke:SHIP2011}).
They are variables on sociodemographics (gender, age, etc.),
variables on consumption behaviour (e.g.~alcohol and cigarettes), SNPs (genetic information), variables extracted from laboratory data (e.g.~sera concentrations), and two variables on the results of the liver ultrasound -- \texttt{stea\_s2} and \texttt{stea\_alt75\_s2}.
Both variables take symbolic values that reflect the likelihood that the participant has fatty liver; the latter is a combination of the former and the ALAT recording for the participant; details are in (cf. \autocite{Voelzke:SHIP2011}).
Almost all variables mentioned hereafter have the suffix \texttt{\_s2} which indicates measurements of the SHIP-2 followup, as opposed to SHIP-0 (\texttt{\_s0}) and SHIP-1 (\texttt{\_s1}).
Exceptions are gender, highest school degree and the 10 SNP variables.

\hypertarget{imm-partitioning}{%
\subsection{Partitioning the Dataset into Subpopulations}\label{imm-partitioning}}

Since the dataset is imbalanced with respect to gender (314 women, 264 men), we decided to partition the dataset before classification.
First, we investigated the class distributions in the two partitions on gender.
We observed that the distributions are very different, most notably with respect to class B (cf.~Figure \ref{fig:03-fatty-liver-mosaic}.
As second step, we studied the class distribution by gender and age, whereupon we detected that age is associated with PartitionF but not with PartitionM.
Third, we identified a cut-off point for age by introducing a heuristic that identifies the age value which minimizes the standard deviation with respect to the target variable.
Afterwards, we performed supervised learning separately on the partitions of male and female participants.
Furthermore, we built an additional learner for the subpopulation of older female participants aged above the cut-off point 52.

To understand how age affects the class distribution, we introduced a heuristic that determines the cutoff age value at which \texttt{partitionF} splits into two bins, so that the standard deviations of the liver fat concentration in each bin are minimized.
Let \(\mathsf{splitAge}\) denote the cutoff value and \(X_y=\{x\in\mathsf{PartitionF}|\text{age of } x \leq \mathsf{splitAge}\}\), \(X_z=\{x\in\mathsf{PartitionF}|\text{age of } x > \mathsf{splitAge}\}\) denote the bins.
Further, let \(n\) be the cardinality of \(X_y\cup{}X_z\) i.e.~of PartitionF.
Then, we define the Sum of weighted Standard Deviations (\(SwSD\)) as

\begin{equation}
SwSD\left(X_y,X_z\right) = \frac{|X_y|}{n}\sigma({X_y})+\frac{|X_z|}{n}\sigma({X_z})
\label{eq:03-swsd}
\end{equation}

where \(|X_i|\) is the cardinality of \(X_i\) and \(\sigma(X_i)\) the standard deviation of the original liver fat values.
Our heuristic selects the \(\mathsf{splitAge}\) such that \(SwSD\) is minimal.
For \(\mathsf{PartitionF}\), the minimum value was 7.44 at the age of 52,
i.e.~close to the onset of menopause.



\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/03-histogram-partitions-age-liverfat} 

}

\caption{Distribution of liver fat concentration in male participants, and in female younger and older than 52 years. The horizontal axis shows the liver fat concentration in bins of 5\%, while the vertical axis shows the number of participants in each bin.}\label{fig:03-histogram-partitions-age-liverfat}
\end{figure}

The histograms in Figure \ref{fig:03-histogram-partitions-age-liverfat} depict the differences in the liver fat concentration distributions at the age cutoff value of 52.
Next to \(\mathsf{PartitionM}\) (n=264), we show the subpartitions \(\mathsf{F:age\leq{}52}\) (n=131) and \(\mathsf{F:age>52}\) (n=183) of \(\mathsf{PartitionF}\).
Most of female participants in \(\mathsf{F:age\leq{}52}\) have no more than 5\% liver fat concentration and ca. 95\% have no more than 10\%, i.e., they belong to the negative class A.
In contrast, ca. 28\% of the participants in \(\mathsf{F:age>52}\) have a liver fat concentration of more than 10\%; they belong to the positive classes B and C.

For the classification of the cohort participants we concentrated on algorithms that deliver interpretable models, since we wanted to identify predictive \emph{conditions}, i.e., variables and values/ranges in the models.
Hence, we considered decision trees, classification rules and regression trees.

We employed the J4.8 decision tree classification algorithm (equivalent to the C4.5 algorithm \autocite{Q92}) of the Waikato Environment for Knowledge Analysis (Weka) workbench \autocite{FrankEtAl:Weka2016}.
This algorithm builds a tree successively, by splitting each node (subset of the dataset) on the variable that maximizes information gain within that node.
The original algorithm operates only on variables that take categorical values and creates one child node per value.
However, the implementation in the Weka library also provides an option that forces the algorithm to always create exactly two child nodes: one for the best separating value and one for all other values.
We used this option in our experiments, because it delivers trees of better quality.
Moreover, the Weka algorithm also supports variables that take numeric values:
a node is split into two child nodes by partitioning the value range of the variable into two intervals.

To deal with the skewed distribution, we considered the following classification variants:

\begin{itemize}
\tightlist
\item
  \emph{Naive}: the problem of imbalanced data is ignored.
\item
  \emph{InfoGain}: we keep only the top-30 of the 66 variables, by sorting the variables on information gain towards the target variable.
\item
  \emph{Oversampling}: We use SMOTE \autocite{CBHea02} to resample the dataset with minority-oversampling: for class B, 100\% new instances are generated, for class C 300\% new instances are generated, resulting in following distribution A:438, B:216, C:128.
\item
  \emph{CostMatrix}: We prefered to misclassify a negative case rather than not detecting a positive case, so we penalized false negatives (FN) more than false positives (FP).
  We used the cost matrix depicted in Table \ref{tab:03-costmatrix}.
\end{itemize}



\begin{table}

\caption{\label{tab:03-costmatrix}\textbf{Cost matrix.} Cost matrix penalizing misclassification under class skew.}
\centering
\begin{tabular}[t]{>{}ccccc}
\toprule
\multicolumn{2}{c}{\textbf{ }} & \multicolumn{3}{c}{\textbf{Predicted class}} \\
\cmidrule(l{3pt}r{3pt}){3-5}
   &   & A & B & C\\
\midrule
 & A & 0 & 1 & 2\\

 & B & 2 & 0 & 1\\

\multirow{-3}{*}{\centering\arraybackslash \textbf{True class}} & C & 3 & 2 & 0\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{imm-classification}{%
\subsection{Classification Rule Discovery}\label{imm-classification}}

Classification rules can uncover interesting relationships between one or more features and the outcome \autocite{Fuernkranz:12,Herrera11}.
Compared to model families such as deep neural networks, support vector machines and random forests, classification rules achieve inferior accuracy most of the time.
However, they are easier to interpret and reason about, and hence lend themselves better for interactive subpopulation discovery.
In epidemiological research, interesting subpopulations could be subsequently used to formulate and validate a small set of hypotheses or just to explore associations between risk factors for a specific outcome.
An interesting subpopulation could be phrased as ``In the sample of this study, the prevalence of goiter is 32\%, while the likelihood in the subpopulation described by \emph{thyroid-stimulating hormone} smaller equal 1.63 mU/l and \emph{body mass index} greater than 32.5 kg/m\textsuperscript{2} is 49\%''.

Classification rule algorithms induce descriptions of ``\emph{interesting}'\,' subpopulations of the data where interestingness is quantified by a quality function.
A classification rule is an association rule whose consequent is fixed to a specific class value.
Consider the exemplary classification rule \(r_1\):

\begin{equation}
r_1: \underbrace{som\_waist\_s2 < 80 \wedge age\_ship\_s2 > 59 \left(\wedge \ldots \right)}_{\text{Antecedent}} \longrightarrow \underbrace{\vphantom{som\_waist\_s2 < 80 \wedge age\_ship\_s2 > 59 \left(\wedge \ldots \right)}hepatic\_steatosis = pos}_{\text{Consequent}}
\label{eq:03-rule}
\end{equation}

\hypertarget{underpinnings}{%
\subsubsection*{Underpinnings}\label{underpinnings}}
\addcontentsline{toc}{subsubsection}{Underpinnings}

Classification rules are expressed in the form of \(r: \text{antecedent} \longrightarrow T=v\).
The conjunction of \emph{conditions} (i.e.~feature - feature value pairs) left to the arrow constitutes the rule's \(\text{antecedent}\) (or left hand site).
In the \(\text{consequent}\) (or right hand side), \(v\) is the requested value for the target variable \(T\).

We define \(s(r)\) as the subpopulation or \emph{cover set} of \(r\), i.e.~the set of instances that satisfy the antecedent of \(r\).
The \emph{coverage} of \(r\), which is the fraction of instances covered by \(r\), is then defined as \(Cov(r)=|s(r)|/N\), where \(N\) is the total number of instances.
The \emph{support} of \(r\) quantifies the percentage of instances covered by \(r\) that additionally have \(T=v\), calculated as \(Sup(r)=|s(r)_{T=v}|/N\).
The \emph{confidence} of \(r\) (also referred to as precision or accuracy) is defined as \(Conf(r)= |s(r)_{T=v}|/|s(r)|\) and expresses the relative frequency of instances satisfying the complete rule (i.e., both the antecedent and the consequent) among those satisfying only the antecedent.
The \emph{recall} or \emph{sensitivity} of \(r\) with respect to \(T=v\) is defined as \(Recall(r)=Sensitivity(r)=\frac{|s(r)_{T=v}|}{n_{T=v}}\).
The \emph{Weighted Relative Accuracy} of a rule is an interestingness measure which balances coverage and confidence gain and is often used as internal quality criterion for candidate generation \autocite{Herrera11}.
It is defined as \(WRAcc(r) = Cov(r)\cdot \left(Conf(r)-\frac{n_{T=v}}{N} \right)\).

As an example, Figure \ref{fig:03-rule-intro} illustrates an exemplary rule \(r_2\) in a dataset with 10 instances and a binary target, where circles in cyan color represent instances from the negative class and red circles are positive instances.
The cover set of \(r_2\) contains instances 7, 8, 9 and 10, hence \(Cov(r_2)=0.40\).
Further, \(Sup(r_2)=0.30\), \(Conf(r_2)=0.75\) and \(WRAcc(r_2)=0.4\cdot\left(0.75-0.4\right)=0.14\).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/03-rule-intro} 

}

\caption{Exemplary classification rule.}\label{fig:03-rule-intro}
\end{figure}

\hypertarget{hotspot}{%
\subsection{HotSpot}\label{hotspot}}

For classification rule discovery we use the algorithm HotSpot\footnote{\url{https://weka.sourceforge.io/packageMetaData/hotSpot/1.0.4.html}} provided in the Waikato Environment for Knowledge Analyis (WEKA) workbench \autocite{FrankEtAl:Weka2016}.
HotSpot is a beam search algorithm that implements a general-to-specific approach for extracting rules.
A single rule is constructed by successively adding the condition to the
antecedent that locally maximizes confidence.
Contrary to general hill-climbing which considers only the best rule candidate at each iteration, HotSpot's beam search keeps the \(b\) highest ranked candidates and refines them in later steps.
Consequently, HotSpot reduces the \emph{``myopia''} \autocite{Fuernkranz:12} hill-climbing search typically suffers from.
Briefly, hill-climbing approaches consider only the locally optimal candidate at each iteration.
As a consequence a globally optimal rule is not found if it is not locally optimal in every iteration.
From an application point of view, it is also desirable to generate more than one rule, as alternative descriptions of subpopulations can facilitate hypothesis generation.
The beam width can be specified as \texttt{maximum\ branching\ factor}, the maximum number of conditions that may be added to a rule candidate.
In every iteration, rule candidates must satisfy the \texttt{minimum\ value\ count} which is the sensitivity threshold.
To avoid adding a condition that leads only to a marginal improvement in confidence, the parameter \texttt{minimum\ improvement}, i.e.~the minimum relative improvement in confidence by adding a further condition can be specified.
Computational complexity of the rule search can be reduced by specifying a \texttt{maximum\ rule\ length}, which is the number of conditions in the antecedent.

\textbf{describe parametrization in experiments}

\hypertarget{interactive-medical-miner}{%
\subsection{Interactive Medical Miner}\label{interactive-medical-miner}}

Classification rules can provide valuable insights on potentially prevalent conditions for different subpopulations of the cohort under study.
However, if the number of rules produced is large, as it is usually the case in large epidemiological data, the conditions of the rules overlap and some conditions are present under each of the target feature's classes.
Hence, the medical expert needs inspection aids to decide which rules are informative and which features should be studied further.
Our Interactive Medical Miner (IMM) allows the expert to (a) discover classification rules subject to frequency constraints, inspect the frequency of those rules (b) towards each class and (c) against the unlabeled part of the cohort, and (d) study the statistics of each rule for the values of selected variables.
We describe these functionalities below, referring to the screenshot in Figure \ref{fig:03-imm-modified}.



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/03-imm-modified} 

}

\caption{The Interactive Medical Miner: classification rules are discovered for class B and shown in the bottom left panel. For the selected rule som\_huef\_s2 \textgreater{} 109 \& crea\_u\_s2 \textgreater{} 5.38 \(\longrightarrow\) mrt\_liverfat\_s2 = B, the distribution of the participants covered by the rule among all three classes is shown in absolute values (top middle panel) and as histogram (bottom right panel) with respect to age (top right panel).}\label{fig:03-imm-modified}
\end{figure}

The user interface consists of six panels.
The ``Settings'' panel (upper left) allows the medical expert to set the parameters for rule induction before pressing the button ``Build Rules''.
Below this panel, the discovered rules are displayed.
The panel ``Sorting preference'' allows the expert to specify whether rules should be sorted by confidence, by coverage, or rather alphabetically for better overview of overlapping rules.

The mining criteria include the dataset (choosing between the whole dataset versus one of the partitions), the class for which rules should be generated (drop-down list ``Class'') and the constraints with respect to this class, i.e., \texttt{min\ value\ count} (which can also be given as relative number), \texttt{maximum\ rule\ length}, \texttt{maximum\ branching\ factor} and \texttt{minimum\ improvement}.
As an example how these parameters affect the rule search, consider the selected rule in Figure xxx, som\_huef\_s2 \textgreater{} 109 \& crea\_u\_s2 \textgreater{} 5.38 \(\longrightarrow\) mrt\_liverfat\_s2 = B, which has a coverage of 0.12 and a confidence of 0.56.
The sensitivity of 38/108 = 0.352 satisfies the minimum value count threshold of 0.33.
From the Apriori property, it is apparent that each of the two conditions in the rule's antecedent, namely som\_huef\_s2 \textgreater{} 109 and crea\_u\_s2 \textgreater{} 5.38 must also exceed this threshold.
The position of a condition within the antecedent indicates at which refinement step it was added to the rule candidate.
For example, the first condition som\_huef\_s2 \textgreater{} 109 with a confidence of 44/107 = 0.41 was expanded by the second condition crea\_u\_s2 \textgreater{} 5.38, because the gain in confidence exceeds the minimum improvement threshold, i.e., 38/68 - 44/107 = 0.15 \textgreater{} 0.05.
This rule, however, cannot be further expanded because the maximum rule length is set to 2.
The maximum branching factor was set conservatively to 1000, to prevent potentially interesting rules from not being generated due to small beam width.
The parameter can be lowered interactively, if the number of discovered rules is to high or rule induction takes too long.

The output list of an execution run (area below the ``Settings'') is scrollable and interactive.
When the expert clicks on a rule, the top middle area ``Summary Statistics'' is updated.
The first row shows the distribution of the cohort participants among the classes for the whole dataset, while the second row shows how the participants covered by the rule (column ``Total'' in the second row) are distributed among the classes.
Hence, the expert can specify the discovery of classification rules for one of the classes and then study how often the antecedent of each rule appears among the participants in the other classes.
A rule that covers most of the participants of the selected class (class B in Figure \ref{fig:03-imm-modified}) is not necessarly interesting, for example, if it covers also a high number of participants of the other classes.
The rule som\_huef\_s2 \textgreater{} 109 \& crea\_u\_s2 \textgreater{} 5.38 \(\longrightarrow\) mrt\_liverfat\_s2 = B covers 68 participants in total, 38 of them having class B.
To lower the number of covered participants from other classes , i.e., to increase confidence, the user can lower the minimum value count, to allow for the generation of rules with lower sensitivity, but more homogeneity with respect to the selected class.

Some of the data might be incomplete. For example, not all participants in the cohort have been subjected to liver MRI.
Hence, it is also of interest to know the distribution of the unlabeled participants who support the antecedent of a given rule.
The ``Histogram'' panel can be used to this purpose: the expert chooses a further feature from the interactive area ``Variable selection'' in the upper right panel and can then see how the values of these variable are distributed among the study participants - both the labeled ones and the unlabeled ones; the latter are marked as ``Missing'' in the color legend.
For plotting the histograms we make use of the free Java chart library JFreeChart \cite{GilbertJFree}.
Numeric variables are discretized using ``Scott's rule'' \autocite{scott1979optimal} as follows:
let \(X_{s(r)}\) be the set of values for a numeric variable \(X\) with respect to the cover set \(s(r)\).
The bin width \(h\) is then calculated as \(h(X_{s(r)})=\frac{\max{X_{s(r)}}-\min{X_{s(r)}}}{3.49\text{sd}_{s(r)}}\cdot |s(r)|^{\frac{1}{3}}\).

If the expert does not choose any variable, the target variable is used by default, and only the distribution of the labeled participants is visible.
The histogram in Figure \ref{fig:03-imm-modified} shows the age distribution of both labeled and unlabeled participants covered by our example rule som\_huef\_s2 \textgreater{} 109 \& crea\_u\_s2 \textgreater{} 5.38 \(\longrightarrow\) mrt\_liverfat\_s2 = B.
The value distribution among the labeled participants reveals that ageing might be a risk factor for the displayed subpopulation as the likelihood of class B increases with higher age.
This visual finding suggests adding the condition age\_ship\_s2 \textgreater{} 56.8 to the rule's antecedent.
Indeed, the confidence of this more specific rule increases from 38/68 = 0.56 to 27/40 = 0.675.
However, since the sensitivity reduces from 38/108 = 0.352 to 27/108 = 0.250, the minimum value count threshold is no longer satisfied.
Hence, the visualization of the participants' statistics for selected rules can deliver indications on subpopulations that should be monitored closer, and hints on how to alter the algorithm parameters for subsequent runs, in our example, to lower the minimum value count to 0.25 and to increase the maximum rule length to 3.

\hypertarget{experiments-and-findings}{%
\section{Experiments and Findings}\label{experiments-and-findings}}

\hypertarget{results-of-decision-tree-classifiers}{%
\subsection{Results of Decision Tree Classifiers}\label{results-of-decision-tree-classifiers}}

For the evaluation of decision tree classifiers, we consider accuracy, i.e., the ratio of correctly classified participants, sensitivity and specificity, and the \(F_1\) score, i.e., the harmonic mean between precision and recall.
For \textbf{specificity}, precision and recall, we consider the two classes B and C together as positive class.

\emph{Oversampling} achieved best performance with an accuracy of ca. 80\% and an \(F_1\)score of 62\%.
The best decision trees were found for partition \(\mathsf{F:age>52}\), followed by those for \(\mathsf{PartitionF}\), then \(\mathsf{PartitionM}\).
\textbf{The large discrepancy between accuracy and \(F_1\)score appears also in the models of the partitions, underlying that accuracy scores are unreliable in such a skewed distribution. Therefore, we do not report on accuracy hereafter.}

On partition \(\mathsf{F:age>52}\), the overall best decision tree is achieved by the oversampling variant.
On the larger \(\mathsf{PartitionF}\), best performance was achieved by the decision tree produced with the InfoGain variant, while the best decision tree on \(\mathsf{PartitionM}\) was built with the CostMatrix variant.
Sensitivity and specificity values for these trees are shown in Table \ref{tab:03-tree-performance-sens-spec}, while the trees themselves are depicted in Figures \ref{fig:03-tree-menopause} - \ref{fig:03-tree-men} respectively and discussed in Section \ref{imm-important-features}.



\begin{table}

\caption{\label{tab:03-tree-performance-sens-spec}Best decision trees for the three partitions: best separation is achieved in \(\mathsf{F:age>52}\); \(\mathsf{PartitionM}\) is the most heterogeneous one, the performance values are lowest.}
\centering
\begin{tabular}[t]{llrrr}
\toprule
\textbf{Partition} & \textbf{Variant} & \textbf{Sensitivity (\%)} & \textbf{Specificity (\%)} & \textbf{F1 score (\%)}\\
\midrule
$\mathsf{F:age}>\mathsf{52}$ & Oversampling & 63.5 & 93.9 & 81.5\\
$\mathsf{PartitionF}$ & InfoGain & 52.4 & 94.9 & 69.7\\
$\mathsf{PartitionM}$ & CostMatrix & 38.3 & 86.3 & 53.0\\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:03-tree-performance-sens-spec} indicates that the decision tree variants perform differently on different partitions.
Oversampling is beneficial for \(\mathsf{F:age>52}\), because it partially compensates the skew problem.
As \(\mathsf{PartitionM}\) has the most heterogeneous class distribution out of all paritions, all variants perform relatively poor on it.
Hence, we expected most insights from the decision trees on \(\mathsf{F:age>52}\) and \(\mathsf{PartitionF}\), where better separation is achieved.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/03-tree-menopause} 

}

\caption{\textbf{Best decision tree for \(\mathsf{F:age>52}\)}, achieved by the variant \emph{Oversampling}.}\label{fig:03-tree-menopause}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/03-tree-women} 

}

\caption{\textbf{Best decision tree for \(\mathsf{PartitionF}\)}, achieved by the variant \emph{InfoGain}.}\label{fig:03-tree-women}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/03-tree-men} 

}

\caption{\textbf{Best decision tree for \(\mathsf{PartitionF}\)}, achieved by the variant \emph{InfoGain}.}\label{fig:03-tree-men}
\end{figure}

\hypertarget{discovered-classification-rules}{%
\subsection{Discovered Classification Rules}\label{discovered-classification-rules}}

While the classification rules found by HotSpot on the whole dataset were conclusive for class A but not for the positive classes B and C, we omit reporting these rules as they are not useful for diagnostic purposes.
The classification rules found on the partitions were more informative.
However, classification rules with only one feature in the antecedent had low confidence.
To ensure high confidence, we restricted the output on rules with at least two features in the antecedent.
To ensure a still high coverage, we allowed for at most three features.
A selection of high confidence and high coverage rules for each partition and class are shown in Tables \ref{tab:03-rule-list-women} -- \ref{tab:03-rule-list-men}, respectively.
We describe the most important features in the antecedent of these rules in the next subsection, together with the most important features of the best decision trees.



\begin{table}

\caption{\label{tab:03-rule-list-women}Best HotSpot classification rules (\emph{maxLength} = 3) for \(\mathsf{PartitionF}\) (excerpt).}
\centering
\begin{tabular}[t]{lllrrrr}
\toprule
\multicolumn{3}{c}{\textbf{Rule antecedent}} & \multicolumn{1}{c}{\textbf{Cov}} & \multicolumn{2}{c}{\textbf{Sup}} & \multicolumn{1}{c}{\textbf{Conf}} \\
\cmidrule(l{3pt}r{3pt}){1-3} \cmidrule(l{3pt}r{3pt}){4-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-7}
\textbf{Variable 1} & \textbf{Variable 2} & \textbf{Variable 3} & \textbf{Abs} & \textbf{Abs} & \textbf{Rel [\%]} & \textbf{Rel [\%]}\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: A}}\\
\hspace{1em}som\_waist\_s2 $\leq$ 80 & -- & -- & 132 & 132 & 52 & 100\\
\hspace{1em}som\_bmi\_s2 $\leq$ 24.82 & -- & -- & 109 & 109 & 43 & 100\\
\hspace{1em}som\_huef\_s2 $\leq$ 97.8 & -- & -- & 118 & 117 & 46 & 99\\
\hspace{1em}stea\_s2 = 0 & -- & -- & 218 & 214 & 84 & 98\\
\hspace{1em}stea\_alt75\_s2 = 0 & -- & -- & 202 & 198 & 78 & 98\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: B}}\\
\hspace{1em}stea\_s2 = 1 & gx\_rs11597390 = 1 & age\_ship\_s2 > 59 & 20 & 17 & 40 & 85\\
\hspace{1em}stea\_alt75\_s2 = 1 & hrs\_s\_s2 > 263 & age\_ship\_s2 > 59 & 20 & 17 & 40 & 85\\
\hspace{1em}stea\_alt75\_s2 = 1 & hrs\_s\_s2 > 263 & ldl\_s\_s2 > 3.22 & 20 & 17 & 40 & 85\\
\hspace{1em}stea\_s2 = 1 & age\_ship\_s2 > 66 & tg\_s\_s2 > 1.58 & 17 & 14 & 33 & 82\\
\hspace{1em}stea\_s2 = 1 & age\_ship\_s2 > 64 & hrs\_s\_s2 > 263 & 17 & 14 & 33 & 82\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: C}}\\
\hspace{1em}gluc\_s\_s2 > 7 & tsh\_s2 > 0.996 & -- & 6 & 6 & 35 & 100\\
\hspace{1em}som\_bmi\_s2 > 38.42 & age\_ship\_s2 $\leq$ 66 & asat\_s\_s2 > 0.22 & 6 & 6 & 35 & 100\\
\hspace{1em}som\_bmi\_s2 > 38.42 & sleeph\_s2 > 6 & blt\_beg\_s2 $\leq$ 38340 & 6 & 6 & 35 & 100\\
\hspace{1em}som\_bmi\_s2 > 38.42 & sleeph\_s2 > 6 & stea\_s2 = 1 & 6 & 6 & 35 & 100\\
\hspace{1em}hrs\_s\_s2 > 371 & sleepp\_s2 = 0 & ggt\_s\_s2 > 0.55 & 6 & 6 & 35 & 100\\
\bottomrule
\end{tabular}
\end{table}



\begin{table}

\caption{\label{tab:03-rule-list-menopause}Best HotSpot classification rules (\emph{maxLength} = 3) for \(\mathsf{F:age>52}\) (excerpt).}
\centering
\begin{tabular}[t]{lllrrrr}
\toprule
\multicolumn{3}{c}{\textbf{Rule antecedent}} & \multicolumn{1}{c}{\textbf{Cov}} & \multicolumn{2}{c}{\textbf{Sup}} & \multicolumn{1}{c}{\textbf{Conf}} \\
\cmidrule(l{3pt}r{3pt}){1-3} \cmidrule(l{3pt}r{3pt}){4-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-7}
\textbf{Variable 1} & \textbf{Variable 2} & \textbf{Variable 3} & \textbf{Abs} & \textbf{Abs} & \textbf{Rel [\%]} & \textbf{Rel [\%]}\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: A}}\\
\hspace{1em}crea\_u\_s2 $\leq$ 5.39 & stea\_s2 = 0 & -- & 75 & 75 & 57 & 100\\
\hspace{1em}crea\_u\_s2 $\leq$ 5.39 & stea\_alt75\_s2 = 0 & -- & 72 & 72 & 55 & 100\\
\hspace{1em}som\_waist\_s2 $\leq$ 80 & -- & -- & 54 & 54 & 41 & 100\\
\hspace{1em}som\_bmi\_s2 $\leq$ 24.82 & -- & -- & 50 & 50 & 38 & 100\\
\hspace{1em}crea\_u\_s2 $\leq$ 5.39 & ggt\_s\_s2 $\leq$ 0.43 & -- & 50 & 50 & 38 & 100\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: B}}\\
\hspace{1em}stea\_s2 = 1 & ggt\_s\_s2 > 0.48 & ggt\_s\_s2 $\leq$ 0.63 & 15 & 15 & 38 & 100\\
\hspace{1em}stea\_s2 = 1 & gx\_rs11597390 = 1 & hdl\_s\_s2 $\leq$ 1.53 & 20 & 19 & 48 & 95\\
\hspace{1em}stea\_s2 = 1 & gx\_rs11597390 = 1 & fib\_cl\_s2 > 3.4 & 15 & 14 & 35 & 93\\
\hspace{1em}crea\_s\_s2 $\leq$ 61 & som\_waist\_s2 > 86 & stea\_s2 = 1 & 15 & 14 & 35 & 93\\
\hspace{1em}stea\_s2 = 1 & gx\_rs11597390 = 1 & hrs\_s\_s2 > 261 & 20 & 18 & 45 & 90\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: C}}\\
\hspace{1em}som\_bmi\_s2 > 38.42 & age\_ship\_s2 $\leq$ 66 & -- & 4 & 4 & 33 & 100\\
\hspace{1em}som\_bmi\_s2 > 38.42 & stea\_alt75\_s2 = 3 & -- & 4 & 4 & 33 & 100\\
\hspace{1em}som\_huef\_s2 > 124 & stea\_alt75\_s2 = 3 & -- & 4 & 4 & 33 & 100\\
\hspace{1em}som\_waist\_s2 > 108 & gluc\_s\_s2 > 6.2 & -- & 4 & 4 & 33 & 100\\
\hspace{1em}stea\_alt75\_s2 = 3 & som\_bmi\_s2 > 37.32 & -- & 4 & 4 & 33 & 100\\
\bottomrule
\end{tabular}
\end{table}



\begin{table}

\caption{\label{tab:03-rule-list-men}Best HotSpot classification rules (\emph{maxLength} = 3) for \(\mathsf{PartitionM}\) (excerpt).}
\centering
\begin{tabular}[t]{lllrrrr}
\toprule
\multicolumn{3}{c}{\textbf{Rule antecedent}} & \multicolumn{1}{c}{\textbf{Cov}} & \multicolumn{2}{c}{\textbf{Sup}} & \multicolumn{1}{c}{\textbf{Conf}} \\
\cmidrule(l{3pt}r{3pt}){1-3} \cmidrule(l{3pt}r{3pt}){4-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-7}
\textbf{Variable 1} & \textbf{Variable 2} & \textbf{Variable 3} & \textbf{Abs} & \textbf{Abs} & \textbf{Rel [\%]} & \textbf{Rel [\%]}\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: A}}\\
\hspace{1em}stea\_alt75\_s2 = 0 & -- & -- & 106 & 101 & 55 & 95\\
\hspace{1em}stea\_s2 = 0 & -- & -- & 138 & 131 & 72 & 95\\
\hspace{1em}ggt\_s\_s2 $\leq$ 0.52 & -- & -- & 79 & 73 & 40 & 92\\
\hspace{1em}hrs\_s\_s2 $\leq$ 310 & ggt\_s\_s2 $\leq$ 0.77 & -- & 81 & 74 & 40 & 91\\
\hspace{1em}som\_waist\_s2 $\leq$ 90.8 & -- & -- & 79 & 72 & 39 & 91\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: B}}\\
\hspace{1em}som\_huef\_s2 > 108.1 & age\_ship\_s2 > 39 & crea\_u\_s2 > 7.59 & 28 & 22 & 33 & 79\\
\hspace{1em}som\_bmi\_s2 > 32.29 & hdl\_s\_s2 > 0.94 & ATC\_C09AA02\_s2 = 0 & 29 & 22 & 33 & 76\\
\hspace{1em}som\_bmi\_s2 > 32.29 & hgb\_s2 > 8.1 & gout\_s2 = 0 & 29 & 22 & 33 & 76\\
\hspace{1em}som\_waist\_s2 > 109 & sleeph\_s2 $\leq$ 8 & jodid\_u\_s2 > 9.44 & 29 & 22 & 33 & 76\\
\hspace{1em}som\_huef\_s2 > 108.1 & hdl\_s\_s2 > 0.97 & crea\_u\_s2 > 5.38 & 29 & 22 & 33 & 76\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{Target class: C}}\\
\hspace{1em}ggt\_s\_s2 > 1.9 & crea\_s\_s2 $\leq$ 90 & quick\_s2 > 59 & 6 & 6 & 40 & 100\\
\hspace{1em}ggt\_s\_s2 > 1.9 & crea\_s\_s2 $\leq$ 90 & chol\_s\_s2 > 4.3 & 6 & 6 & 40 & 100\\
\hspace{1em}ggt\_s\_s2 > 1.9 & crea\_s\_s2 $\leq$ 90 & fib\_cl\_s2 > 1.9 & 6 & 6 & 40 & 100\\
\hspace{1em}ggt\_s\_s2 > 1.9 & crea\_s\_s2 $\leq$ 90 & crea\_u\_s2 > 4.74 & 6 & 6 & 40 & 100\\
\hspace{1em}ggt\_s\_s2 > 1.9 & tg\_s\_s2 > 2.01 & som\_waist\_s2 > 93.5 & 6 & 6 & 40 & 100\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{imm-important-features}{%
\subsection{Important Features for Each Subpopulation}\label{imm-important-features}}

The most important features in the decision trees of Figures \ref{fig:03-tree-menopause} - \ref{fig:03-tree-men} are those closer to the root.
For better readability, the tree nodes in the figures contain short descriptions instead of the original variable names.
In all three decision trees, the root node is the ultrasound diagnosis variable stea\_s2.
A negative ultrasound diagnosis points to the negative class A, but
a positive ultrasound diagnosis does not directly lead to one of the positive classes B and C.
The decision trees of the three partitions differ in the nodes placed near the root.

In the best decision tree of \(\mathsf{PartitionF}\) (cf.~Figure \ref{fig:03-tree-women}) we observe that
if the ultrasound report is positive \emph{and} the HbA1C concentration is more than 6.8\%, the class is C.
The classification rules with high coverage and confidence in Table \ref{tab:03-rule-list-women}) point to further interesting features:
a waist circumference of at most 80 cm, a BMI of no more than 24.82 kg/m\textsuperscript{2}, a hip circumference of 97.8 cm or less characterize participants of the negative class.
All 6 participants with a serum glucose concentration greater than 7 mmol/l \emph{and} a TSH concentration greater than 0.996 mu/l belong to class C.
Further, severe obesity (a BMI value of more than 38.42 kg/m\textsuperscript{2} points to class C with high confidence -- but only in combination with other variables.

In contrast to the best tree for \(\mathsf{PartitionF}\), the best decision tree for the subpartition \(\mathsf{F:age>52}\) (cf.~Figure \textasciitilde\ref{fig:03-tree-menopause}) also contains nodes with SNPs, indicating potentially genetic associations to fatty liver for these participants.
Classification rules with high coverage and confidence for class B also contain SNPs, as can be seen in Table \textasciitilde\ref{tab:03-rule-list-menopause}.
Similarly to \(\mathsf{PartitionF}\), high BMI values point to a positive class when combined with other features: in Table \ref{tab:03-rule-list-menopause}, we see that all four participants with stea\_alt75\_s2 = 3 (i.e.~a positive ultrasound diagnosis combined with a critical ALAT value) and a BMI larger than 38.42 kg/m\textsuperscript{2} belong to class C.
A similar association holds for stea\_alt75\_s2 = 3 combined with a high waist circumference (\textgreater{} 124 cm).
19 out of 20 participants in class B having a positive ultrasound diagnosis, a genetic marker gx\_rs11597390 = 1 and a serum HDL concentration of at most 1.53 mmol/l.

The role of the ultrasound report in predicting the negative class is the same for \(\mathsf{PartitionM}\) (cf.~Figure \ref{fig:03-tree-men} as for \(\mathsf{PartitionF}\).
As with the best tree for \(\mathsf{F:age>52}\), the best tree for \(\mathsf{PartitionM}\) contains nodes with SNPs and serum GGT value ranges.
Such features are also in the antecedent of top Hotspot rules (cf.~Table \textasciitilde\ref{tab:03-rule-list-men}):
a Serum GGT concentration of more than 1.9,\(\mu\)mol/sl in combination with creatinine concentration of at most 90 mmol/l or a thromboplastin time ratio (quick\_s2) of more than 59\% point to class C.
Similarly, positive ultrasound diagnosis and a serum HDL concentration not exceeding 0.84 mmol/l point to class C.

The decision trees and classification rules give insights into features that seem diagnostically important.
However, the medical expert needs additional information to decide whether a feature is worth further investigation.
In particular, decision trees highlight the importance of a feature only in the context of the subtree it is located; a subtree describes a subpopulation that is usually very small.
In contrast, classification rules return information on larger subpopulations.
However, these subpopulations may overlap; for example, the first four rules on class C for \(\mathsf{PartitionM}\) (cf.~Table \ref{tab:03-rule-list-men}) may refer to the same 6 participants.
Moreover, unless a classification rule has a confidence close to 100 \%, there may be participants in the other classes that also support it.
Hence, to decide whether the features in the rule's antecedent deserve further investigation, the expert also needs insights on the rule's statistics for the other classes as well.
To assist the expert in this task, we propose the Interactive Medical Miner, a tool that discovers classification rules for each class \emph{and} delivers information on the statistics of these rules for all classes.

\hypertarget{enhancements}{%
\subsubsection{Enhancements}\label{enhancements}}

In \autocite{Niemann:IMM2014}, we added functionalities to specify a (sub-)cohort of the dataset, for example, to focus on the subset of female participants.
In an additional window, the expert can specify filtering queries in the form of \texttt{Variable} \texttt{Operator} \texttt{Value}.
The defined restrictions are shown in a table where the user can undo previous constraints.
Further, the user can (de-)select variables for model generation.
For example, the expert might exclude a variable that is known to be highly correlated with another variable that is already considered for model learning.

\autocite{Schleicher:CBMS17} added panels that contain tables displaying additional rule statistics such as lift and p-value.
Further, a mosaic chart juxtaposes the class distributions of a subpopulation and its ``complements'' which are subgroups of participants who do not match one or both of the conditions of the length-2 rule which describes that subpopulation.

In this paper, we study SD on a binary target variable problem (positive or negative outcome). A subpopulation of \(r\), \(s(r)\) is the set of instances that satisfy the antecedent of a rule \(r\), also known as cover set of \(r\). \% \%
\%A subpopulation comprises of instances that fulfill the logical condition of a rule's antecedent; ae a rule is described as:\\
\%
\%Further, \(|s(r)_{T=v}|\) is the number of instances that additionally belong to the value for the target variable in \(s(r)\), the support set; \(n\) is the total number of instances in the dataset; \(n_{T=v}\) is the total number of instances which exhibit the target variable value of interest. \%
\%

The conjunction of feature - feature value pairs of left to the arrow constitute the rule's \emph{antecedent} (or left hand site), while the pair of target feature - target feature value right to the arrow is the rule's \emph{consequent} (or right hand side).
The rule in Equation \eqref{eq:03-rule} is a \emph{classification rule} referring to class B.

Features with continuous values (e.g.~age, waist) are restricted within a specific range.
This range is chosen by the algorithm in such a way as to maximize the support of the rule within the cohort and the confidence of the rule's consequent given the antecedent. The first rule in the example belongs to the results on \(\mathsf{PartitionF}\) (see Table \ref{tab:03-rule-list-women}, described in the next section): out of the 20 participants supporting the antecedent, 17 belong to class B.
Note that we use the expressions \emph{"[number] participants support rule [ruledescription] "} and \emph{"[number] participants support the rule's \emph{antecedent}"}.

For classification rule discovery we use the Weka algorithm HotSpot. For each class, this algorithm determines the rules with the best confidence \emph{and} the optimal boundary values for the features in the antecedent. We have wrapped the algorithm into a mechanism that selects for each class only rules supported by at least \(\tau\) participants. In our experiments, we use \(\tau=\frac{1}{3}\), but our interactive tool (section \ref{sec:ruleinspector}) allows the expert to set this threshold freely.

\hypertarget{conclusions-on}{%
\section{Conclusions on \ldots{}}\label{conclusions-on}}

\autocite{qing2020improved}

\hypertarget{sdclu}{%
\chapter{Identifying Distinct Subpopulations}\label{sdclu}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-1}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-1}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We study redundancy in large rule sets describing subpopulations.
We present a workflow that extracts a smaller number of representative rules.
These rules are selected to avoid instance overlap as much as possible, thus covering different concepts in the data space.
We evaluate our workflow on samples of SHIP with hepatic steatosis and goiter as target variables.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

Uli Niemann, Myra Spiliopoulou, Bernhard Preim, Till Ittermann, and
Henry Völzke. ``Combining Subgroup Discovery and Clustering to Identify
Diverse Subpopulations in Cohort Study Data''. In: \emph{Proc. of IEEE Int.
Symposium on Computer-Based Medical Systems (CBMS)}. 2017, pp.~582-587.
DOI: \href{https://doi.org/10.1109\%2FCBMS.2017.15}{10.1109/CBMS.2017.15}.

\end{infobox}

Epidemiologists search for significant relationships between risk factors and outcome in large and heterogeneous datasets that encompass participant health information gathered from questionnaires and medical examinations.
In the previous chapter, we described a expert-driven workflow that can help epidemiologists to automatically detect such relationships in form of classification rules, which are descriptions of risk factors and predictive value ranges in for a specific subpopulation and an outcome of interest.
However, rule induction algorithms often produce large and overlapping rule sets requiring the expert to manually pick the most interesting rules and to remove less interesting or redundant rules.
This post-filtering step is time-consuming and tedious.
This chapter presents a clustering-based algorithm that hierarchically reorganizes large rule sets and summarizes all important concepts while maintaining \emph{distinctiveness} between the clusters.
For each cluster, a representative rule is shown to the expert who in turn can drill-down to other cluster members.
We evaluate our algorithm on two subsets of SHIP where the outcomes hepatic steatosis and goiter serve as target variable, respectively.
Further, we report on effectiveness of our algorithm and we present selected subpopulations.

This chapter presents SD-Clu, an approach that combines subgroup discovery with clustering to return a set of \(k\) representative classification rules.
Building up on a set of potentially highly overlapping rules generated by a SD algorithm, we leverage hierarchical agglomerative clustering to find groups of rules that cover different sets of instances.
For each cluster, we nominate one rule as the group's representative that exhibits best trade-off between rule confidence and coverage towards the target variable.
We define similarity between a pair of subgroups based on the fraction of mutually covered instances and individually covered instances.
Rules covering (almost) the same instances are likely to be condensed into the same cluster and thus more likely to be represented by a proxy rule.
We evaluate our algorithm on two samples from \emph{SHIP} investigating the diseases hepatic steatosis and goiter.

Section~\ref{sdclu-intro} serves as motivation for the related field of subgroup discovery and redundancy of classification rules.
Section~\ref{sdclu-method} presents our SD-Clu algorithm that generates distinct rules.
Section~\ref{sdclu-experiments} contains the experimental setup.
In Section~\ref{sdclu-results}, we describe our evaluation results.
In Section~\ref{sdclu-discussion}, we discuss our findings regarding hepatic steatosis and goiter on the SHIP data.
In Section~\ref{sdclu-conclusions}, we summarize our contributions.

\hypertarget{sdclu-intro}{%
\section{Motivation and Comparison to Related Work}\label{sdclu-intro}}

Subgroup discovery (SD) algorithms aim to uncover \emph{interesting} relationships between one or more conditions (variables and value ranges) and a target variable in the form of classification rules~\autocite{Herrera11,Atzmueller15}.
Compared to more accurate but predominantly opaque black-box models such as neural networks, support vector machines, or random forests, SD algorithms provide higher confidence and interpretability of results, making them highly suitable for domain expert-guided subpopulation discovery.
SD algorithms have been used in several medical studies where descriptive knowledge needed to be inferred, e.g., to extract potential drug targets from multi-relational data sources for the treatment of dementia~\autocite{Nguyen:Nature15}, to identify predictive auditory-perceptual, speech-acoustic, and articulatory-kinematic features of preschool children with speech sound disorders~\autocite{Vick14}, and to discover discriminative features in patient subpopulations with different admission times to psychiatric emergency departments~\autocite{Carmona11}.

However, SD methods often yield large sets of rules that domain experts are not willing to tediously go through all of them to manually separate interesting from irrelevant and redundant rules.
A common observation is that there are groups of rules that cover almost the same set of instances, as shown in Figure~\ref{fig:04-rule-redundancy}.
Instead of presenting \emph{all} rules found by an SD algorithm at once, we propose to organize rule sets hierarchically so that the domain expert is able to explore a compact set of different concepts, equipped with mechanisms to drill down to specific rules of interest.



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/04-rule-redundancy} 

}

\caption{\textbf{Example of two \emph{redundant} rules.} Both \(r_1\) and \(r_2\) cover the instances 7,8,9,10; \(r_2\) additionally covers instance 2. The cover set overlap is due to the high correlation between \emph{\#Teeth} and \emph{Age}, and between \emph{BMI} and \emph{Waist circumference}. Both rules describe the same concept, i.e., \emph{elderly overweight people have a higher risk of the outcome}.}\label{fig:04-rule-redundancy}
\end{figure}

Similar to the HotSpot algorithm described in Section~\ref{hotspot}, popular SD algorithms such as SubgroupMiner~\autocite{Kloesgen:SubgroupMiner2002},
SD~\autocite{Gamberger:SD02} and CN2-SD~\autocite{Lavravc:CN2-SD04} use a fixed beam width to limit the number of further expanded subgroup candidates at each iteration.
A post-pruning step can be applied to reduce the cardinality of the rule set -- e.g., to return the \emph{top-k} rules -- using a quality criterion such as the weighted relative accuracy or the p-value of a statistical test.
Even when both beam-width search and top-\(k\) pruning are applied, the result often still contains redundant rules.
This is due to the correlation between the (non-target) variables, which leads to a large number of variations of a given finding, cf.~Figure~\ref{fig:04-rule-overlap-heatmap} for an illustrative example.
In particular, top-\(k\) pruning leads to different variations of the same concepts (i.e.~large instance overlap)~\autocite{VanLeeuwen:DSSD12}.



\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figures/04-rule-overlap-heatmap} 

}

\caption{Graphical representation of 1115 HotSpot rules found on SHIP data on 886 labeled instances. A gray cell indicates that the instance at that x-position is covered by the rule at the associated y-position. Instances and rules are sorted by agglomerative hierarchical clustering. When partitioning into 10 clusters based on the covered instances, the rules in each cluster describe similar subpopulations.}\label{fig:04-rule-overlap-heatmap}
\end{figure}

Unlike SD, which generates multiple rules that overlap in terms of their coverage sets, predictive rule learning algorithms such as CN2~\autocite{Clark:CN289} or RIPPER~\autocite{Cohen:RIPPER95} are designed to generate rules that capture different spaces in the data.
They work iteratively according to a divide-and-conquer strategy~\autocite{Fuernkranz:12}:
First, a rule that maximizes the quality function of the algorithm is generated on the set of instances not yet covered.
Second, all covered instances are removed from the training set.
This process of rule induction and removal of instances from the training set is repeated until all instances are covered by at least one rule.
The output of this algorithm is often a decision list.
To classify an instance, the prediction of the first rule that covers the instance is used.
While these algorithms avoid the problem of rule redundancy, important concepts may remain uncovered.
For example, the algorithm might induce a rule that includes instances with a high BMI, but it might not be able to find a slightly weaker association with income because those instances are immediately removed from the instance candidate space.
In addition, the coverage of rules generally decreases with each iteration.
Rules with low coverage are negligible in epidemiological studies because they may represent spurious correlations of the study sample.

Instead of simply eliminating covered instances from rule induction of subsequent iterations, \emph{weighted coverage approaches} take into account how many times instances have been covered so far in the rule candidate expansion step~\autocite{Lavravc:CN2-SD04}.
While this leniency over removing instances allows for a larger number of rules, a new hyperparameter is introduced to control the tradeoff between reusing already covered instances and minimum rule confidence.
This parameter is unintuitive and difficult to set, especially for domain experts.
Moreover, both traditional predictive rule learning algorithms and their weighted covering extensions introduce order dependencies between rules: a rule depends on all previous rules in the rule list and the instances of the target variable it covers, and it may not make sense to interpret a single rule.

\hypertarget{sdclu-method}{%
\section{Finding Distinct Classification Rules}\label{sdclu-method}}

In this section, we present our algorithm SD-Clu, which combines subgroup discovery with clustering to return a set of \(k\) distinct classification rules.
The algorithm consists of three main steps.
First, the SD algorithm generates a set of potentially highly overlapping rules.
Using hierarchical agglomerative clustering, this set of rules is then grouped into a set of distinct rule clusters covering different sets of instances.
For each cluster, the rule that has the best tradeoff between confidence and coverage of the target variable is appointed as the representative of the group.
Rules covering the same instances are grouped in the same cluster and are therefore represented by the same proxy rule.

\hypertarget{rule-clustering}{%
\subsection{Rule clustering}\label{rule-clustering}}

We use the same notation for classification rule discovery as in Section~\ref{imm-classification}.
Agglomerative hierarchical clustering iteratively merges similar instances to clusters, in a bottom-up way.
The order of merging two clusters depends on the linkage strategy: in \emph{complete linkage} the distance between two clusters is defined as the maximum distance between any two of their instances.
The pair of clusters minimizing this maximum distance are selected for merging.
A dendrogram is a tree representation of this stepwise process.
Optionally, a parameter \(k\) can be specified to obtain a specific partitioning.
We define rule similarity for clustering on the basis of the mutually covered instances as an adaption of the Sørensen--DICE coefficient~\autocite{Dice45}.
The distance between two rules \(r_1\), \(r_2\) with corresponding subpopulations \(s(r_1)\), \(s(r_2)\) is given as
\begin{equation}
\text{dist}(r_1,r_2) = 1 - \frac{2\cdot\left|s(r_1)\cap s(r_2)\right|}{\left|s(r_1)\right| + \left|s(r_2)\right|}.
\label{eq:dice}
\end{equation}
The number of clusters can be specified as parameter \(k\).
Alternatively, we describe an approach that derives an appropriate \(k\) from the rule set using a cluster quality function such as the Silhouette coefficient.
For a clustering \(\xi\) and a set of rules \(R\), the Silhouette coefficient is calculated as
\begin{equation}
\text{Silh}(R,\xi) = \frac{1}{|R|}\sum_{r\in R}{\frac{b(r)-a(r)}{\max\left\{a(r), b(r)\right\}}}
\label{eq:silh}
\end{equation}
where
\begin{equation}
a(r)=\frac{\sum_{y\in{}Y}\text{dist}(r,y)}{|Y|-1}
\label{eq:silh-a}
\end{equation}
is the average dissimilarity between \(r\) and the other rules in the cluster \(Y\in\xi\) which contains \(r\), while
\begin{equation}
b(r)=\frac{\sum_{y\in{}Z}\text{dist}(r,y)}{|Z|}
\label{eq:silh-b}
\end{equation}
is the average dissimilarity between \(r\) and the rules in the cluster \(Z\in\xi\) which is the closest to the cluster \(Y\) containing \(r\).
Then, we traverse the dendrogram bottom-up, compute the Silhouette for each set of clusters \(\xi\) and select as \(\xi_{opt}\) the set of clusters with the best Silhouette value.
The optimal number of clusters is then the cardinality \(|\xi_{opt}|\).
Finally, we map each cluster \(Y\in\xi_{opt}\) to a representative rule.
To do so, we invoke the rule interestingness measure \emph{Weighted Relative Accuracy} (WRA hereafter) which balances coverage and confidence gain and is defined as
\begin{equation}
WRA(r)=Cov(r)\cdot\left(Conf(r)-\frac{n_{T=v}}{N}\right)
\label{eq:wra}
\end{equation}
where \(N\) is the total number of instances in the dataset and \(n_{T=v}\) is the number of instances with the target variable value of interest.
We compute WRA for each rule \(r\in{Y}\) and select as cluster proxy \(cp(Y)\) the rule for which WRA is maximum.

\hypertarget{representativeness-of-a-set-of-cluster-proxies}{%
\subsection{Representativeness of a set of cluster proxies}\label{representativeness-of-a-set-of-cluster-proxies}}

The rule proxies should be a good representation of the total rule set.
Thus, each instance should be covered equally often by the cluster proxies compared to the total rule set.
Hence, we define representativeness as difference between the average fraction of proxy rules the instances are covered by and the total average fraction of rules the instances are covered by.
If the difference between the two ratios is small, then the representativeness of the cluster proxy rules is high.

Typically, the set of rule clusters \(\zeta\) for a set of rules \(R\) will be the optimal set of clusters, as described in the previous subsection, but it can be any set of clusters chosen by the user, as long as it contains all rules in \(R\).
For \(\zeta\), let \(R_{\zeta}=\{cp(Y)|Y\in\zeta\}\) denote the set of cluster proxy rules.
To quantify how representative such a set of rules is, we proceed as follows.
First, let \(U\subseteq{}R\) be an arbitrary subset of the complete set of rules, and let \(x\) be an instance.
The \emph{coverage rate} of \(x\) towards \(U\) is calculated as
\begin{equation}
covRate(x,U) = \frac{\sum_{r\in{}U}isCovered(x,r)}{|U|}
\label{eq:covRate}
\end{equation}
where \(isCovered(x,r)\) is equal to 1, if \(r\) covers \(x\), i.e., \(x\in s(r)\), and 0 otherwise.
We observe that for a set of rules \(U\), an instance \(x\) cannot be covered by more than \(|U|\) rules.
Let \(R_x\) be the set of rules that cover instance \(x\), i.e., for \(R_x=\{r\in{}U | isCovered(x,r)=1 \}\).
For the whole set of instances \(X\) we create bins:
\begin{equation}
bin_i(U)= \{x \in{}X | |R_x|=i \}.
\label{eq:bini}
\end{equation}
Further, let \(bin_0(U)= \{x \in{}X | \forall r\in U : isCovered(x,r)=0 \}\).
Evidently, an instance \(x\) can be covered by \(0, 1, \ldots, |U|\) rules, i.e.~\(covRate(x,U)\) can take one of \(|U|+1\) values.
In contrast, \(covRate(x,R)\) can take one of \(|R|+1\) values, a usually much larger number. We therefore map the possible values of \(covRate(x,R)\) into the much smaller set of possible values by rounding, computing:
\begin{equation}
adjCovRate(x,U, R)=\frac{\lfloor covRate(x,R)\cdot|U| \rceil}{|U|}
\label{eq:adjCovRate}
\end{equation}
where \(\lfloor\rceil\) is the rounding operator.
Then, for the complete set of instances \(X\), a set of induced rules \(R\), the clustering \(\zeta\) over \(R\) and the set of cluster proxy rules \(R_{\zeta}\), the \emph{representativeness} of \(R_{\zeta}\) is defined as
\begin{equation}
representativeness(R_{\zeta},R)=1-
\frac{1}{|X|}\sum_{x\in{}X} |adjCovRate(x,U,R) - covRate(x,R_{\zeta})|.
\label{eq:representativeness}
\end{equation}

\hypertarget{sdclu-experiments}{%
\section{Experimental Setup}\label{sdclu-experiments}}

\textbf{Datasets.}
To evaluate our method, we used data from the SHIP study.
For a description of SHIP, see Section~\ref{ship}.
We considered hepatic steatosis (see section~\ref{imm-target}) and goiter as target variables.
For the sample \texttt{HepStea}, we derived a binary outcome variable by discretizing the liver fat concentration obtained from the MRI report so that study participants with a concentration no greater than 10\% were assigned to the negative class and values greater than 10\% were assigned to the positive class indicating the presence of the disease.
Of 886 participants for whom the MRI report from SHIP-2 was available at that time, 694 (78.3\%) were negative and 192 (21.7\%) were positive.
We considered 99 variables selected exclusively from SHIP-0 to assess their long-term effects as expressed 10 years later in SHIP-2.
For the \texttt{Goiter} sample, the outcome variable was derived by thyroid ultrasound.
The presence of goiter was defined for a thyroid volume greater than 18ml in women and 25ml in men~\autocite{Gutekunst:88}.
Of the 4400 participants for whom the outcome variable is available in TREND-0, 3010 belong to the negative class (68.4\%) and 1390 (31.6\%) to the positive class.
Apart from the target variable, we use a total of 182 variables that were pre-selected by a medical expert as potential risk factors.

\textbf{SD algorithms.}
For subgroup discovery, the two algorithms HotSpot~\autocite{Hall:Weka09} (cf.~section~\ref{hotspot} and SD-Map~\autocite{Atzmueller:SD-Map:06} are used.
SD-Map is an exhaustive algorithm that adapts the popular FP-Growth association rule learning method~\autocite{Han:FPGrowth00}.
Rules that fall below a minimum coverage threshold are pruned.
A deep-first search is performed for candidate generation.
Rules are ranked according to a user-defined quality function.
We used the implementation from the VIKAMINE framework~\autocite{Atzmueller:VIKAMINE12}.
The implementation of SD-Map only supports categorical variables.
Therefore, each numeric variable was discretized using the minimum description length based approach of Fayyad and Irani~\autocite{Fayyad:MDL93}.
For SD-Map, we set the minimum coverage threshold to 0.05 to avoid overfitting rules that are too small.
We use WRA as a quality function and define a minimum threshold of 0.025.
For HotSpot, we set the support threshold of a rule to be above 0.05.
The beam width is set to 500.
In addition, to avoid rather meaningless literals, we restrict the extension of a rule body with another literal to a relative confidence gain of at least 0.3.
To avoid having many overly specific rules that cover only a small number of study participants, we limit the length of a rule body, i.e., the number of literals to 3.

\hypertarget{sdclu-results}{%
\section{Results}\label{sdclu-results}}

In Figure~\ref{fig:04-silh-comparison}, we show the optimal number of clusters for each combination of study sample and SD algorithm.
Table~\ref{tab:04-silh} shows the optimal \(k\) and the ratio of proxy rules vs.~total number of rules.
For example, clustering with optimal silhouette coefficient for the algorithm HotSpot on \texttt{Goiter} has 76 clusters and thus 76 rule cluster proxies (cf.~Table~\ref{tab:04-silh}), which is only 21.3\% of the total number of rules.
Thus, if only the cluster proxies are displayed to the expert at the beginning, the time needed to check the rules is reduced.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/04-silh-comparison} 

}

\caption{\textbf{Silhouette coefficients (\(Silh\)) of SD-Clu using complete linkage for each combination of dataset and algorithm.} The cardinality of the clustering with the highest \(Silh\) score (\(|\zeta_{opt}|\)) is indicated by a dashed vertical line.}\label{fig:04-silh-comparison}
\end{figure}



\begin{table}

\caption{\label{tab:04-silh}\textbf{Statistics of best runs per dataset and algorithm.} Number of rules \(|R|\), optimal Silhouette coefficient \(Silh(\zeta_{opt})\), the corresponding cardinality of the optimal clustering \(|\zeta_{opt}|\) and percentage of cluster proxies relative to the total number of rules for every combination of data sample and SD algorithm.}
\centering
\begin{tabular}[t]{llrrrr}
\toprule
Dataset & Algorithm & $|R|$ & $Silh(\zeta_{opt})$ & $|\zeta_{opt}|$ & $\frac{|\zeta_{opt}|}{|R|} [\%]$\\
\midrule
HepStea & HotSpot & 208 & 0.41 & 100 & 48.1\\
HepStea & SD-Map & 68 & 0.40 & 30 & 44.1\\
Goiter & HotSpot & 356 & 0.37 & 76 & 21.0\\
Goiter & SD-Map & 106 & 0.66 & 54 & 51.6\\
\bottomrule
\end{tabular}
\end{table}

The optimal cardinality of \(|\zeta_{opt}\) shown in Figure~\ref{fig:04-silh-comparison} could be used as a suggestion, but the expert is free to specify the number of rules they wishes to obtain.
For example, if the expert considers \(|\zeta_{opt}|\) = 100 too large for HotSpot on \texttt{HepStea}, the diagram would show them that a reduction to \(|\zeta_{opt}|\) = 58 is possible, reducing \(Silh\) only slightly from 0.48 to 0.37.
Also in the other direction: if \(|\zeta_{opt}|\) is rather low, the diagram shows that a small increase does not change \(Silh\) much; therefore, the added rules may also be important.
The expert could even analyze the diagram to derive a range instead of a single value, e.g., the range where \(Silh\) is above 90\% of its maximum.

To assess the representativeness of the cluster proxies, we compare them with three baseline criteria that return the top \(k\) rules according to odds ratio (baseline 1), coverage (baseline 2), and WRA (baseline 3).
Figure~\ref{fig:04-representativeness-hepStea-hotSpot} juxtaposes the representativeness of SD-Clu and the three baselines for different numbers of rules \(k\) returned to the expert for the HotSpot algorithm on the sample \texttt{HepStea}.
The plots are arranged by rule selection method (rows) and number of representative rules \(k\) returned to the expert (columns).
Each graph shows the \(adjCovRate\) (y-axis) of instances (x-axis) with respect to \(R\) (solid black curve).
The instances are sorted by the number of rules in \(R\) they are covered by, with their respective \(covRate\) shown as dots.
The dotted curve represents a locally weighted scatterplot smoothing (LOWESS) of the points.
Ideally, both curves are close to each other, meaning that the instances are covered by approximately the same proportion of rules in the cluster proy as the proportion of rules in general.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/04-representativeness-hepStea-hotSpot} 

}

\caption{\textbf{Evaluation of \(representativeness\) on \texttt{HepStea} using the HotSpot algorithm.} \(representativeness\) of SD-Clu and three baseline approaches for different numbers of clusters \(k\) on the \texttt{HepStea} sample using HotSpot. Points depict an instance's \(adjCovRate\) with respect to the set of representative rules of the approach (row) and \(k\) (column). Instances are sorted by \(covRate\) with respect to \(R\) , i.e., the set of all rules) in descending order, shown by a solid black curve. A dotted curve depicts a LOWESS regression fit on the points. The similarity between solid and dotted curve illustrates \(representativeness\) of the top-\(k\) rules of the respective approach. This is illustrated by the dark gray area in-between solid curve and dotted curve where smaller areas are better and reflect higher \(representativeness\) values.}\label{fig:04-representativeness-hepStea-hotSpot}
\end{figure}

For all approaches, \(representativeness\) improves with increasing number of representative rules \(k\).
For example, \(representativeness\) increases from 0.87 to 0.96 for SD-Clu and \(k=10\) to \(k=50\) which means that the absolute difference between \(adjCovRate\) of \(\zeta\) and \(covRate\) of \(R\) over all instances successively decreases.
Further, for a given \(k\), the representative rules of the baselines are less representative than SD-Clu's cluster proxy rules, e.g.~0.91, 0.92, 0.91 vs.~0.96 for \(k=50\), respectively (cf.~5th column of plot matrix in Figure~\ref{fig:04-representativeness-hepStea-hotSpot}).

\hypertarget{sdclu-discussion}{%
\subsection{Discussion of Findings}\label{sdclu-discussion}}

Tables~\ref{tab:04-findings-hep-stea} and~\ref{tab:04-findings-goiter} show the antecedent, support, and confidence of cluster proxy rules found by two algorithms for \texttt{HepStea} and \texttt{Goiter} with \(k\)=5.
The prevalence of hepatic steatosis or goiter is significantly higher in each of the subpopulations described by these rules than in the corresponding overall population.
These subpopulations are characterized by known risk factors for hepatic steatosis, such as large waist circumference and BMI, blood pressure and hypertension, advanced age, and high values in some of the medical tests (ALAT and LDL).
Furthermore, apolipoprotein A1 (ApoA1), a major protein component of high-density lipoprotein (HDL) particles in plasma, was found to be associated with outcome in elderly patients (see fourth hotspot rule in Tables~\ref{tab:04-findings-hep-stea}).
Lipoprotein metabolism is considered the main process contributing to the development of fatty liver~\autocite{jiang2013lipoprotein}.
In addition, Poynard et al~\autocite{poynard1986apolipoprotein} found that patients with hepatic steatosis had higher levels of ApoA1 than patients with hepatic fibrosis, who in turn had higher levels than patients with cirrhosis.
The fifth hotspot rule describes a subpopulation with elevated levels of liver high-sensitivity C-reactive protein (CRP) (approximately 0.80-quantile) and elevated levels of uric acid (approximately 0.36-quantile).
Lizardi-Cervera et al.~\autocite{lizardi2007association} reported increased levels of ultra-sensitive CRP in subjects with hepatic steatosis independent of other metabolic states.
Similarly, Keenan et al.~\autocite{keenan2012relation} found elevated uric acid levels in patients with hepatic steatosis independent of metabolic syndrome.



\begin{table}

\caption{\label{tab:04-findings-hep-stea}\textbf{Representative rules (\texttt{HepStea}).} Proxy rules for \(k\)=5 on the \texttt{HepStea} sample learned and the positive outcome as target.}
\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10cm}rrr}
\toprule
\textbf{\#} & \textbf{Antecedent of cluster proxy} & \textbf{Sup} & \textbf{Conf} & \textbf{Lift}\\
\midrule
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{HotSpot}}\\
\hspace{1em}1 & increased waist circumference = TRUE & 0.72 & 0.37 & 1.69\\
\hspace{1em}2 & hypertension = TRUE & 0.60 & 0.33 & 1.54\\
\hspace{1em}3 & age > 44 $\wedge$ apolipoprotein A1 $\leq$ 1.56 g/l & 0.37 & 0.41 & 1.90\\
\hspace{1em}4 & physical health score $\leq$ 47.3 $\wedge$ increased waist circumference = TRUE & 0.29 & 0.48 & 2.12\\
\hspace{1em}5 & high-sensitivity C-reactive protein > 2.8 mg/l $\wedge$ uric acid > 246 µmol/l & 0.29 & 0.46 & 2.21\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{SD-Map}}\\
\hspace{1em}1 & diastolic blood pressure > 79.75 mmHg $\wedge$ hip circumference > 98.05 cm $\wedge$ cholesterol-HDL-quotient > 3.015 & 0.67 & 0.40 & 1.85\\
\hspace{1em}2 & increased waist circumference = TRUE $\wedge$ diastolic blood pressure > 79.75 mmHg $\wedge$ body mass index > 26.3 kg/m² & 0.58 & 0.45 & 2.07\\
\hspace{1em}3 & waist circumference > 88.15 cm $\wedge$ diastolic blood pressure > 79.75 mmHg $\wedge$ body mass index > 26.3 kg/m² & 0.59 & 0.46 & 2.11\\
\hspace{1em}4 & body mass index > 26.3 kg/m² $\wedge$ alanin-aminotransferase > 0.385 µkatal/l $\wedge$ diastolic blood pressure > 79.75 mmHg & 0.55 & 0.48 & 2.20\\
\hspace{1em}5 & body mass index > 26.3 kg/m² $\wedge$ uric acid > 278.5 µmol/l $\wedge$ alanin-aminotransferase > 0.385 µkatal/l $\wedge$ treated urinary tract diseases = TRUE & 0.54 & 0.46 & 2.34\\
\bottomrule
\end{tabular}
\end{table}

Similarly, the identified subpopulations for goiter (see Table~\ref{tab:04-findings-goiter}) are characterized by common risk factors such as increased weight and body mass index and participants prescribed angiotensin II receptor blockers (see second HotSpot rule).
Furthermore, the first HotSpot rule describes participants with intima-media thickness greater than 0.73 mm (approximately 0.80-quantile).
Previous studies found associations between intima-media thickness and thyroid-stimulating hormone~\autocite{takamura2009thyroid} and subclinical hypothyroidism
\autocite{gao2013carotid,unal2017association}.
The condition of the third HotSpot rule describes the duration of an ECG phase.
Jabbar et al~\autocite{jabbar2017thyroid} summarize in their review that pathological thyroid hormone levels increase the risk of cardiovascular disease.
This association appears to be especially true in the elderly~\autocite{fazio2004effects}.
The fourth rule suggests that certain thrombocyte levels indicate increased thyroid volume, which confirms Erikci et al~\autocite{erikci2009effect} who found that hypothyroid patients had higher platelet volume and platelet distribution width than a control group.



\begin{table}

\caption{\label{tab:04-findings-goiter}\textbf{Representative rules (\texttt{Goiter}).} Proxy rules for \(k\)=5 on the \texttt{Goiter} sample learned and the positive outcome as target.}
\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10cm}rrr}
\toprule
\textbf{\#} & \textbf{Antecedent of cluster proxy} & \textbf{Sup} & \textbf{Conf} & \textbf{Lift}\\
\midrule
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{HotSpot}}\\
\hspace{1em}1 & intima-media thickness > 0.73 mm & 0.27 & 0.43 & 1.34\\
\hspace{1em}2 & intake of angiotensin II receptor blocker = TRUE & 0.10 & 0.62 & 1.90\\
\hspace{1em}3 & duration of QRS complex on ECQ > 114 ms $\wedge$ discouraged and sad mood = "never" $\wedge$ body mass index > 26.65 kg/m² & 0.06 & 0.87 & 2.68\\
\hspace{1em}4 & education level = 8 $\wedge$ thrombocytes < 209 Gpt/l & 0.11 & 0.62 & 1.92\\
\hspace{1em}5 & aorta descendens thickness > 2.79 mm $\wedge$ thyroid-stimulating hormone $\leq$ 1.05 mU/l $\wedge$ hemoglobin > 8.8 mmol/l & 0.06 & 0.88 & 2.73\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{SD-Map}}\\
\hspace{1em}1 & intima media thickness > 0.55 mm $\wedge$ proton pump inhibitors intake = FALSE $\wedge$ age > 38.5 & 0.68 & 0.44 & 1.34\\
\hspace{1em}2 & duration of ECG P wave > 111 ms $\wedge$ age > 38.5 $\wedge$ proton pump inhibitors intake = FALSE & 0.60 & 0.45 & 1.38\\
\hspace{1em}3 & hip circumference > 98.75 cm $\wedge$ age > 38.5 $\wedge$ proton pump inhibitors intake = FALSE & 0.60 & 0.44 & 1.34\\
\hspace{1em}4 & increased waist circumference = TRUE $\wedge$ age > 38.5 $\wedge$ proton pump inhibitors intake = FALSE & 0.59 & 0.44 & 1.34\\
\hspace{1em}5 & increased waist circumference = TRUE $\wedge$ intima media thickness > 0.55 mm $\wedge$ proton pump inhibitors intake = FALSE & 0.51 & 0.46 & 1.41\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sdclu-conclusions}{%
\section{Conclusions on identifying distinct subpopulations}\label{sdclu-conclusions}}

SD-Clu tackles the problem of high instance overlap in sets of rules generated by subgroup discovery algorithms.
By limiting the number of rules time spent for rule inspection is reduced.
SD-Clu nominates a representative rule from a hierarchical clustering from a large set of rules, and thus returns rules that express distinct concepts, i.e., rules that cover different sets of instances.
The introduced \emph{representativeness} measure assesses whether instances are similarly often covered by representatives as by the total rule set.
SD-Clu was evaluated on two samples from an epidemiological study where an optimal set of \emph{proxy} rules was selected that (i) contains considerably less rules than the total rule set and (ii) is more representative compared to the baseline approaches, respectively.

\hypertarget{phenotypes}{%
\chapter{Visual Identification of Informative Features}\label{phenotypes}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-2}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-2}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We identify distinct tinnitus phenotypes by clustering screening data using a parameter-free algorithm that leverages the Bayesian information criterion to determine a suitable number of subgroups.
We present novel radial bar and radial line visualizations to juxtapose phenotypes in a high-dimensional feature space and to explore phenotype-specific idiosyncrasies.
We provide a web application with enhanced versions of these visualizations that also depict treatment effects.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

Uli Niemann, Petra Brueggemann, Benjamin Boecking, Matthias Rose, Myra Spiliopoulou, and Birgit Mazurek. ``Phenotyping chronic tinnitus patients using self-report questionnaire data: cluster analysis and visual comparison''. In: \emph{Scientific Reports} 10.1 (2020), p.~16411. DOI: \href{https://doi.org/10.1038\%2Fs41598-020-73402-8}{10.1038/s41598-020-73402-8}.

\end{infobox}

The supervised methods of classification and subgroup discovery described in the previous chapters show great potential for applications where there is one or a small number of well-defined target variables.
However, some medical conditions have heterogeneous appearances which are not well understood yet.
For example, chronic tinnitus is a complex, multi-factorial and heterogeneous symptom.
Clinical assessment and selection of a suitable therapy strategies are difficult as not all patients benefit equally from treatment.
Due to the large number and heterogeneity of available assessment tools, unsupervised methods like cluster analysis appear to be promising approaches to extract clinically relevant tinnitus phenotypes.
Clinical acceptance of these empirical results can be further strengthened by a comprehensive visualization which intuitively illustrate major characteristics of a phenotype and differences between multiple phenotypes.
In this chapter, we describe a workflow (1) to identify distinct tinnitus phenotypes by a parameter-free clustering algorithm and (2) to visualize these subgroups to explore phenotype idiosyncrasies.
In Section~\ref{phenotypes-motivation}, we describe the clinical value of patient subgroup stratification, briefly review previous approaches and list requirements for a clustering solution.
We specify the features used for clustering in Section~\ref{phenotypes-features}, give an overview of the clustering algorithm in Section~\ref{phenotypes-xmeans}, introduce our cluster visualization in Section~\ref{phenotypes-visualization} and our interactive web application in Section~\ref{phenotypes-app}.
In Section~\ref{phenotypes-results}, we present the phenotypes and describe their major characteristics.\\
The medical interpretation of our findings was developed together with tinnitus experts and is provided in Section~\ref{phenotypes-clinical-interpretation}.
Further, we discuss the strength and weaknesses of our workflow in Section~\ref{phenotypes-discussion}.
Finally, we conclude the chapter with a summary in Section~\ref{phenotypes-conclusions}.

\hypertarget{phenotypes-motivation}{%
\section{Motivation and Comparison to Related Work}\label{phenotypes-motivation}}

Challenges for management and treatment of tinnitus are caused to a large extend by its clinical heterogeneity, which includes individual perception, risk factors, comorbidities, degrees of perceived stress and treatment response (cf.~Section~\ref{cha}).
These factors make it difficult for clinicians (a) to choose \emph{the} treatment which is most effective for an individual patient, and (b) to design a unified treatment strategy all patients equally benefit from.
The awareness of the existence of distinct patient subgroups may stimulate the development of more effective therapy modules.
Since clinically relevant subgroups have not been established yet, clustering emerges as a promising approach to identify characteristic tinnitus \emph{phenotypes} in a data-driven and hypothesis-free way.

Previous studies found subgroups of tinnitus patients with cluster analysis based on a small number of audiometric features~\autocite{Langguth:LCA2017}, a combination of features extracted from self-reports, audiometry and psychoacoustics~\autocite{Tyler:TinnitusClustering2008}, or neuroimaging data and
socio-demographics~\autocite{Schecklmann:BrainResearch2012}.
Although each of these studies provided insights in tinnitus subgroup patterns, acceptance among medical scholars may be increased by presenting the clustering results with intuitive visualizations that show individual subgroup patterns and enable the visual juxtaposition of multiple subgroups with respect to high-dimensional data.

Addressing this requirement, Schlee et al.~\autocite{Schlee:RadarVis2017} proposed a compact radar chart visualization that allows to compare the degree of health burden between individuals or subgroups based on measurements from self-report questionnaires.
While their visualization could be applied to any disease domain, Schlee et al.~demonstrated its efficacy showing subgroup differences with respect to measurements of tinnitus distress and associated comorbidities.
However, they did not aim to visualize clustering results, but restricted themselves to pre-defined cohorts by graphically comparing female against male patients, and patients with low tinnitus frequency against patients with high tinnitus frequency.

\hypertarget{phenotypes-features}{%
\section{Selection of Measurement Instruments}\label{phenotypes-features}}

Discussions with tinnitus experts about the selection of measurement instruments (hereafter denoted as \emph{features}) for clustering resulted in two main requirements:
(1) features should cover the clinical heterogeneity of tinnitus to a great degree, and
(2) if available, more robust compound scores should be preferred over single items from a questionnaire.
From the routine questionnaire assessment battery (cf.~Section~\ref{cha}), we selected a total of 64 features\footnote{The complete list of features is provided in Appendix~\ref{appx-pheno}.} from 14 questionnaires.
These include all questionnaire total scores, all questionnaire subscale scores and all items of questionnaires that have neither subscales nor total scores.
The features measure
(general) tinnitus characteristics,
physical quality of life,
experiences of pain,
somatic expressions,
affective symptoms,
tinnitus-related distress,
internal resources,
perceived stress, and
mental quality of life.

From a total of 4,103 patients, data from 2,875 (70.1\%) was incomplete and therefore excluded.
The N=1,228 patients included in the final sample were only slightly, yet significantly younger than the excluded ones (\(\mu\)\textsubscript{included} = 50.0, \(\sigma\)\textsubscript{included} = 11.9; \(\mu\)\textsubscript{excluded} = 51.7, \(\sigma\)\textsubscript{excluded} = 13.6; \(t\)(2630.8) = 4.0, p \(<\) 0.01).
Additionally, for 989 of the included patients patients (80.5\%) post-treatment data were also available and used to visually explore treatment effect differences between clusters.
Since most of the features have greater scores for higher health burden, we reversed the remaining features with greater scores for higher quality of life.
A feature \(X\) is reversed as \(X_{reversed} = \max{(X)} - X\).
The asterisk suffix in a feature name (e.g.~ACSA\_qualityoflife*) denotes a reversed feature.
Due to widely differing value ranges, each feature was standardized via z-score normalization.
A feature \(X\) with expected value \(E(X)=\mu\) and variance \(Var(X) = \sigma^2\) is standardized into \(Z = \frac{X - \mu}{\sigma}\).
For \(Z\), it holds true that \(\mu\) = 0 and \(\sigma^2\) = 1.

\hypertarget{phenotypes-xmeans}{%
\section{Identification of Tinnitus Phenotypes using Clustering}\label{phenotypes-xmeans}}

Practical considerations of data clustering include how to set the number of clusters \(K\).
Since a ground truth is often not available, several heuristics to automatically determine \(K\) have been proposed.
A popular approach is the so called ``elbow'' method which involves running the clustering algorithm with different values for \(K\) (cf.~Section~\ref{evo-concept-clustering} for the application of the elbow method for density-based clustering).
The number of clusters is plotted against cluster \emph{compactness}.
In the popular \(K\)-means algorithm, cluster compactness is quantified by total within-sum of squares (WSS), the sum of squared distances between each observation and its centroid over all clusters.
As WSS or similar goodness of fit measures increase monotonically with increasing \(K\), the idea of the elbow method is to identify the curve's characteristic ``knee point'' which is the first point from which adding another cluster leads only to a \emph{minor} improvement in compactness.
Because the plot is not guaranteed to exhibit such a distinctive knee point and universal compactness thresholds do not exist, this approach is sometimes impracticable.
Another popular clustering evaluation measure is the Silhouette coefficient which favor clusterings where similar objects are assigned to the same cluster and dissimilar objects are assigned to different cluster.

Instead of a evaluating clustering quality post-hoc, we decided to chose an algorithm which internally identifies an appropriate number of clusters.
X-means~\autocite{Pelleg:xmeans2000} is a parameter-free adaption of the popular K-means algorithm which incorporates the Bayesian information criterion~\autocite{Schwarz:BIC1978} (BIC) to find a good trade-off between low total sum of squares and a small number of clusters.

Let \(\mathcal{D}\) be the dataset with \(d\) dimensions and let \(D\) be a subset of \(\mathcal{D}\), i.e., \(D\subseteq \mathcal{D}\).
A K-means clustering on \(D\) creates the set of clusters
\(\mathcal{C}=\left\{C_1,\ldots,C_k,\ldots,C_K\right\}\), where \(c_k\) is the centroid of cluster \(k\), \(r_k\) is the number of objects in \(D\) assigned to \(C_k\) and \(p\) is the number of free parameters, i.e., \(p = (d+1) \cdot K\).
The BIC of a cluster \(C_k\) using the Schwarz criterion is calculated as

\begin{equation}
\text{BIC}(C_k) = \hat{l}_k(\mathcal{D}) - \frac{p_k}{2} \cdot \log |\mathcal{D}| 
\label{eq:06-bic}
\end{equation}

where \(\hat{l}_k(\mathcal{D})\) is the log-likelihood of \(\mathcal{D}\) according to \(C_k\).
The point probabilities are computed as
\begin{equation}
\hat{P}(x_i)=\frac{r_{(i)}}{|\mathcal{D}|}\cdot \frac{1}{\sqrt{2\pi}\hat{\sigma}}\text{exp}\left(\frac{1}{2\hat{\sigma}^2}||x_i-c_{(i)}||\right)
\label{eq:06-point-probs}
\end{equation}
where the maximum likelihood estimate for the variance (under the identical spherical Gaussian assumption) is
\begin{equation}
\hat{\sigma}^2=\frac{1}{|\mathcal{D}|-K}\sum_{i=1}^{|\mathcal{D}|}\left(x_i - \mu_{(i)}\right)^2.
\label{eq:06-mle}
\end{equation}
The log-likelihood of \(\mathcal{D}\) according to \(\mathcal{C}\) is
\begin{equation}
l(\mathcal{D})=\log\prod_{i=1}^{|\mathcal{D}|} P(x_i)=\sum_{i=1}^{|\mathcal{D}|}\left(\log \frac{1}{\sqrt{2\pi}\hat{\sigma}} - \frac{1}{2 \sigma^2} ||x_i-c_{(i)}||^2 + \log \frac{r_{(i)}}{|\mathcal{D}|}  \right).
\label{eq:06-log-likelihood}
\end{equation}

The main steps of the X-means algorithm are summarized in Figure~\ref{fig:06-xmeans}.
At the start, an initial partitioning is generated by ordinary K-means with \(K\) = \(K\)\textsubscript{lower}, where \(K\)\textsubscript{lower} is a lower bound for the number of clusters.
Then, each cluster is bisected; the resulting two child centroids are placed in opposite direction along a randomly chosen vector by a distance proportional to the cluster radius.
For each pair of child clusters, a local K-means clustering with \(K=2\) is run.
If the BIC score of the new partitioning exceeds the BIC score of the parental one, the child centroids are kept, otherwise the parent centroid is retained.
The iterative steps are repeated until there is no cluster whose bisection leads to a better BIC score, or until the number of clusters exceeds an optional upper bound \(K\)\textsubscript{upper}.
We used the R implementation of Ishioka~\autocite{Ishioka:xmeansimplementation2005}.
Since we did not aim to restrict the solution space with respect to the number of clusters, we set \(K\)\textsubscript{lower}, i.e., the lowest possible value, and we did not set \(K\)\textsubscript{upper}.
For internal validation, we recorded the number of clusters produced by \(X\)-means on 500 bootstrap samples.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-xmeans} 

}

\caption{\textbf{Principal steps of X-means (simplified).} Adapted from~\autocite{Pelleg:xmeans2000}.}\label{fig:06-xmeans}
\end{figure}

\hypertarget{phenotypes-visualization}{%
\section{Visualisation of Phenotypes}\label{phenotypes-visualization}}

Visualizing clusters in high-dimensional data is challenging.
Scatterplot matrices (SPLOMs) can intuitively represent the
relationship between all pairs of features as a matrix of two-dimensional scatterplots~\autocite{Im:GPLOM2013,Klemm:RegressionHeatmap2015}.
However, as the number of features increases, the number of scatterplots grows quadratically, leading to scalability problems such as overplotting.
Several advanced visualization techniques have been proposed as a remedy, from simply adding transparency or colors to points to more sophisticated density contours, hexagon binning, layers with aggregated geometric features (Minimal Spanning Trees, Alpha Shape, Convex Hull), animation, or combinations of several techniques such as
splatterplots~\autocite{Mayorga:Splatterplots2013}.
However, SPLOMs and other traditional visualization techniques such as parallel coordinate graphs~\autocite{Hartigan:PCC1975} are still more suitable for low-dimensional data.

Dimensionality reduction (DR) techniques are often used to project the original data onto a low-dimensional projection that allows the above visualization types to be used.
Ideally, this projection preserves the most important structures of the original data, such as relative pairwise distance, clusters, outliers, and correlations.
Principal Component Analysis~\autocite{Hotelling:PCA1933} (PCA) is a seminal DR algorithm that generates linear, orthogonal combinations of the original dimensions.
Each new dimension, called a principal component, contains a loading indicating how much variability in the data it covers.
Typically, the first two or three dimensions that carry the highest charges are selected for visualization.
PCA is not robust to outliers and cannot capture nonlinear relationships.
Multidimensional scaling~\autocite{Gower:MDS1966} (MDS) is another early DR technique that emphasizes the preservation of pairwise distances, ie,
Objects that are close to each other in high-dimensional space should also be close to each other in low-dimensional projected space.
For complex, arbitrarily shaped structures, pairwise distances may be subject to the curse of dimensionality, leading to poor results.
\(t\)-stochastic neighborhood embedding~\autocite{VanDerMaaten:tSNE2008} (\(t\)-SNE) and Uniform Manifold Approximation and Projection~\autocite{mcinnes2018umap} (UMAP) are nonlinear dimensionality reduction methods that represent a matrix of pairwise similarities.
The idea is to obtain both global structures such as clusters and local structures such as distances and neighbors.
Both \(t\)-SNE and UMAP can produce superior projections compared to traditional linear techniques, provided their hyperparameters are appropriately tuned.
However, a shortcoming of these techniques is that the mapping to the original features cannot be quantified.
Moreover, a projection cannot be applied to new observations; instead, a new projection must be recomputed.
Because of their stochasticity, different runs with the same hyperparameters may yield different results.
Since the semantics of the original dimensions are lost, we decided that DR methods are not suitable for this application.

Discussions with tinnitus experts led to the following cluster visualization requirements:

\begin{itemize}
\tightlist
\item
  keep original (interpretable) features,
\item
  represent high-dimensional data with dozens of features,
\item
  compactly compare multiple clusters at a glance,
\item
  Contrast cluster characteristics with the overall patient mean.
\end{itemize}

Following these requirements, we implemented (a) a radial bar chart as a visualization of a single cluster (Figure~\ref{fig:06-all-radial-barcharts-export}) and (b) a radial line chart visualization for comparing multiple clusters at once~\ref{fig:06-radar}.
The radial bar chart is used to compare observations assigned to a single cluster with the overall population.
The mean values of the features within a cluster are represented by bars arranged in a radial layout.
Each bar begins at the black ``zero line,'' which represents the feature means of the entire population, i.e., all subjects used for clustering.
Because features are standardized (i.e., z-scored) prior to clustering, bars inclined to the outside represent feature averages above the population average and bars inclined to the inside represent feature averages below the population average.
For example, a bar whose top is positioned at -1 characterizes a feature average within a cluster that is 1 standard deviation smaller than the population average.
In addition to the combination of bar height and bar direction, within-cluster averages are also mapped to the bar color by a sequential color gradient from dark blue (lower boundary) to yellow (population average) to light red (upper boundary).
Gray error lines at the top of a bar represent the within-cluster standard error.
To allow quick feature localization, features were grouped into expert-defined categories, which are displayed in the inner circle along with the cluster name and the number of subjects assigned.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-all-radial-barcharts-export} 

}

\caption{\textbf{Radial bar graphs for each of the 4 phenotypes (PT).} (a) PT 1 characterizes the subgroup with the lowest health burden. (b) PT 2 includes the most suffering subjects, in whom all mean values of psychosomatic and somatic characteristics exceed the population mean by more than 0.5 standard deviations (SD). (c) PT 3 indicates somatic indicator scores above the population mean. (d) PT 4 indicates subjects with elevated distress scores, including subjective stress and perceived quality of life. Bars are arranged in a circular layout, with the height and direction of a bar representing the within-cluster feature average and the gray line centered at the top of the bar illustrating the 95\% confidence interval. The characteristics were grouped into 9 categories defined by tinnitus experts. The categories are shown inside the inner circle. See Appendix~\ref{appx-pheno} for a feature description. Adapted from~\autocite{Niemann:SREP_Pheno2020}.}\label{fig:06-all-radial-barcharts-export}
\end{figure}

The radial line chart (\ref{fig:06-radar}) allows a comparison of all clusters in a single display.
Instead of bars, the feature mean values are represented as points.
Points of the same cluster and feature category are connected by line segments.
Points and line segments are colored according to their respective cluster.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-radar} 

}

\caption{\textbf{Radial line chart for phenotypes comparison.} Points show within-phenotype feature averages. Points depicting features of the same category are connected with line segments. Points and lines are colored by cluster. See Figure~\ref{fig:06-all-radial-barcharts-export} for a description of the phenotypes and Appendix~\ref{appx-pheno} for a description of the features. Adapted from~\autocite{Niemann:SREP_Pheno2020}.}\label{fig:06-radar}
\end{figure}

\hypertarget{phenotypes-app}{%
\section{Interactive Components and Exploration of Treatment Effects}\label{phenotypes-app}}

We provide an interactive demo of the cluster solutions and visualizations as a web application\footnote{The demo is available at \url{https://unmnn.de/phs/app/}.} (Figure~\ref{fig:06-gui-pheno}).
The following interactive components have been added:
hovering over bars, lines, or feature labels in the visualizations opens tooltips with additional cluster summaries and compact feature descriptions.
Clicking on a feature triggers an additional plot showing the original (unscaled) distribution of the selected feature stratified by clusters and, if selected, also after treatment.
For continuous features, semi-transparent boxplots are displayed on violin plot~\autocite{Hintze:Violin1998} layers.
For categorical traits, the points and error lines represent the category proportions and 95\% confidence intervals, respectively.
While clustering was performed on baseline data only, we included treatment effect indicators to allow visual detection of potential differences between clusters in treatment efficacy.
To this end, we extended the radial bar chart by plotting cluster averages before treatment (T0) and after treatment (T1) as adjacent bars.
The ends of a pair of bars are connected by a line with an arrowhead pointing from the T0 score to the T1 score.
To highlight large treatment effects, the connecting lines and feature labels are colored according to their relative decrease from T0 to T1.
Given the user-defined parameter \(\Delta\), i.e., the minimum relative magnitude of the difference between T0 and T1 considered as a change, elements were colored (a) red if the T1 score was greater than the T0 score by \(\Delta\) or more, (b) green if the T0 score was less than the T0 score by \(\Delta\) or less, and (c) black, respectively.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-gui-pheno} 

}

\caption{\textbf{User interface of interactive demo.} Radial bar charts were enhanced by interactive components: hovering over a bar or feature label opens a tooltip with additional cluster summaries and a compact feature description. Clicking on a feature updates the right plot showing the distribution of the selected feature stratified by cluster, and if selected, also after treatment. Continuous features are shown using semi-transparent boxplots placed on violin plot~\autocite{Hintze:Violin1998} layers whereas for nominal features, category proportions alongside their 95\% confidence intervals are displayed as points and error lines, respectively.}\label{fig:06-gui-pheno}
\end{figure}

\hypertarget{phenotypes-results}{%
\section{Results}\label{phenotypes-results}}

Four clusters (also referred to as phenotypes hereafter) represent an optimal solution for the present data according to \(X\)-means (cf.~Figure~\ref{fig:06-bootstrap-eval}).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-bootstrap-eval} 

}

\caption{\textbf{Results of internal validation.} Bars show the distribution of the number of clusters generated by \(X\)-means for 500 bootstrap samples. The most common cluster number was 4, which occurred in 82 samples (16.4\%).}\label{fig:06-bootstrap-eval}
\end{figure}

Phenotype 1 (PT 1) represents the largest subgroup (697 of 1,228 patients; 56.8\%), characterized by substantially below-average symptom expression on tinnitus-related and more general psychosomatic symptom indices, including affective symptoms, perceived stress, tinnitus-related distress, and somatic symptoms, as well as (above-average) quality of life and internal resources (Figure~\ref{fig:06-all-radial-barcharts-export} (a)).
Because this group of patients is potentially more help-seeking, presents to the clinic more frequently, and wishes to participate in multimodal treatment, it can be assumed that they experience psychological distress but strive to present as unburdened as possible.
Therefore, this phenotype is referred to as the \emph{``avoidance group''}.
Patients in this subgroup have comparatively high levels of education, employment, and duration of illness and psychotherapeutic treatment (Figure~\ref{fig:06-inter-group-socio}).

PT 2 included 173 patients (14.1\%) who reported the highest emotional and somatic burden among all PTs (Figure~\ref{fig:06-all-radial-barcharts-export} (b)).
More specifically, PT 2 represents a patient subgroup with high
psychosomatic comorbidity and is therefore referred to as the \emph{``psychosomatic group''}.
This patient subgroup shows high tinnitus burden in addition to clinically relevant impairment in all affective indices, including depression, anxiety, and perceived stress.
These affective symptoms appear to be consistent with somatoform expressions of distress, including somatic symptoms.
Patients in this subgroup report greatly reduced quality of life and coping behaviors, with more pessimism, less experienced self-efficacy, and optimism.
Relative to the overall population, this subgroup has a higher proportion of women, patients who live alone, are unemployed, or have an overall lower educational status.
Patients in this cluster also report consulting more physicians, taking more sick days, and using more psychotherapy.
PT 2 patients reported that the tinnitus noise was audible throughout the head (i.e., not unilateral) with a higher percentage than the other groups.

PT 3 contains 187 patients (15.2\%) characterized by above-average scores of traits measuring somatic complaints and near-average scores for affective symptoms (Figure~\ref{fig:06-all-radial-barcharts-export} (c)).
Because the pain scores SF8\_bodilyhealth* and SSKAL\_painfrequency were similar in magnitude to PT2, this patient subgroup is referred to as the \emph{``somatic group''}.
PT 3 includes the oldest subgroup, with the largest proportion of female patients and the largest reported time since tinnitus onset.

In contrast to PT3, PT 4 (n=171; 13.9\%) has above-average values for affective scores, quality-of-life components, and perceived stress (Figure~\ref{fig:06-all-radial-barcharts-export} (d)), e.g., mental component summary score (SF8\_mentalcomp*; 0.85) and anxious depression score (BSF\_anxdepression; 0.79).
Therefore, PT 4 is referred to as the \emph{``distress group''}.
PT 4 represents the youngest of the 4 subgroups (mean 47.3 years), with the largest proportion of male patients (Figure~\ref{fig:06-inter-group-socio}).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/06-inter-group-socio} 

}

\caption{\textbf{Inter-phenotype comparison of demographic characteristics.} Summaries are given as means {[}95\% confidence interval{]} for the entire population and for each of the 4 phenotypes. Confidence intervals were estimated using nonparametric Basic Bootstrap Sampling~\autocite{Davidson:Bootstrap1997} with 2,000 samples each. The Kruskal-Wallis test was used for statistical comparison of differences between phenotypes for continuous features (such as age), and Pearson's chi-square test was used for categorical features (such as sex). An asterisk indicates statistical significance (\(\alpha\) = 0.05). Correction for multiple comparisons was not performed due to the exploratory nature of the study. Adapted from~\autocite{Niemann:SREP_Pheno2020}.}\label{fig:06-inter-group-socio}
\end{figure}

Figure~\ref{fig:06-treatment-effects} depicts the top-10 features with highest average change between T1 and T0 per cluster.
For PT 1 and PT 3, BSF\_elevatedmood* decreased the most, namely by 0.48 \(\pm\) 0.75 and 0.65 \(\pm\) 0.85 (\(Z\) units), respectively.
For PT 2 and PT 4, the top-ranked features is ADSL\_depression with an average difference between T\textsubscript{1} and T\textsubscript{0} of 0.73 \(\pm\) 0.88 and 0.74 \(\pm\) 0.83, respectively.
Six out of these ten features were among the top-10 features for all clusters, including BSF\_elevatedmood*, TQ\_cogintivedistress, TQ\_psychodistress, TQ\_emodistress, TQ\_distress and BSF\_fatigue.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/06-treatment-effects} 

}

\caption{\textbf{Cluster-specific top-10 features with highest average treatment effect magnitude.} Bars depict the intra-cluster average differences between the measurements at T1 and T0 based on the standardized values. Lower values represent better treatment effectiveness. The symbols right to the feature names indicate whether the feature is among the top-10 features for the cluster at position i. For example, the character string \ding{55} \ding{51} \ding{55} \ding{51} for TQ\_intrusiveness (ranked 2nd for PT 1) means that the feature is among the top-10 for PT 1 and PT 3, but not for PT 2 and PT 4.}\label{fig:06-treatment-effects}
\end{figure}

\hypertarget{phenotypes-clinical-interpretation}{%
\section{Clinical Value}\label{phenotypes-clinical-interpretation}}

We discussed the clinical relevance of the phenotypes with three of the five tinnitus experts who co-authored the original publication~\autocite{Niemann:SREP_Pheno2020}.

PT 1 (\emph{avoidant group}) represents more than half of the patient sample.
Besides the actual tinnitus symptom, patients in this subgroup reported few other affective or psychosomatic symptoms.
Because of the biased presentation of these patients (\emph{``everything is fine if it weren't for the tinnitus''}), clinicians might easily be led to believe that assessment of possible other factors contributing to individual distress is unnecessary.
However, clinical experience suggests a thorough assessment of other psychosocial stressors.
The psychosocial resourcefulness of this subgroup enables patients to seek help quickly and in a solution-oriented manner.
Adequate tinnitus-specific counseling and individualized (online) therapy modules that include audiological, psychological, or relaxation techniques may represent an adequate treatment strategy for this patient subgroup.

PT 2 (\emph{psychosomatic group}) represents 15\% of patients with high tinnitus distress and clinically relevant impairment across all affective indices, including depression, anxiety, and perceived stress.
These affective symptoms appear to interact strongly with somatoform expressions of distress, including physical complaints and somatic symptoms.
Patients in this subgroup reported greatly reduced quality of life and coping behaviors, higher pessimism, lower experienced self-efficacy, and optimism.
The frequently asked question is whether increased tinnitus-related distress contributes to an increase in depression or vice versa.
In this group, depressive or anxious symptoms may be considered a crucial underlying factor in overall symptom distress, and treatment must initially focus on improving mood and alleviating depression.
Here, tinnitus-related distress may need to be viewed in a broader context of medical and psychological contributing factors that require patient-specific conceptualization.

PT 3 (\emph{somatic group}) appears to represent a patient subgroup that is
characterized by somatopsychic symptom expression, i.e., physical symptoms that may reflect stress and/or underlying medical conditions.
To meet the needs of this patient subgroup, multimodal interventions may include a proportion of body-oriented procedures such as relaxation exercises or physiotherapy, but their effects should be interpreted in terms of both direct and indirect psychological effects (e.g., through increased well-being or affection from others).

Patients in PT 4 (\emph{stress group}) reported higher than average perceived stress, accompanied by physical exhaustion and anxious-depressed mood.
This group is more likely to include younger, employed, and male patients who reported chronic distress and may be susceptible to a burnout syndrome with subjectively reduced mental performance (``hamster wheel''), which is used to describe life situations even in the absence of tinnitus distress.
Multimodal therapy should initially focus on stress regulation techniques, including relaxation or individually tailored behavior modification approaches.
Similar to PT 2, which has a high psychosomatic burden, patients in PT 4 could also benefit from longer psychotherapeutic or multimodal treatment procedures (inpatient or rehabilitative).

\hypertarget{phenotypes-discussion}{%
\section{Discussion}\label{phenotypes-discussion}}

Although for our dataset the optimal number of clusters was four, we expect that this number may be different for a different sample of tinnitus patients even with the same clustering algorithm.
Figure~\ref{fig:06-bootstrap-eval} shows a high variance in terms of the number of clusters returned by \(X\)-means on different bootstrap samples.
Considering the only slightly lower occurrence of 5 and 6 clusters, our clustering result is certainly not set in stone.

Nonetheless, comparison of all clusters showed that some questionnaires and characteristics differed considerably between patient phenotypes.
In particular, patient subgroups differed substantially in terms of coping behaviors, stress, tinnitus burden, perceived pain, discomfort, and perception of quality of life.
In contrast, patients did not appear to differ with respect to localization and noise.
Regarding the separability between phenotypes, the mostly high correlations between feature within the same category suggest that phenotyping is also possible with fewer questionnaires, especially since some of the questionnaires overlap semantically, e.g.~PHQK\_depression, ISR\_depression, ADSL\_depression, etc.

Without a ``ground truth'' and because of the different sets of available measurements, it is difficult to compare our results with those of similar studies.
The greatest strength of our workflow was the inclusion of a wide range of self-report questionnaire assessments.
Other studies also used audiometry~\autocite{Tyler:TinnitusClustering2008,Langguth:LCA2017} and cardiac imaging data~\autocite{Schecklmann:BrainResearch2012}.
Nevertheless, PT 2 (psychosomatic distress group) seems to be associated with the ``constant distressing tinnitus'' subgroup reported by Tyler et al.~\autocite{Tyler:TinnitusClustering2008}, as the mean scores of tinnitus-related health distress were much larger than in the other subgroups.
Clearly, the selection of a meaningful set of characteristics is central to the effectiveness of a cluster analysis.

The closest to our radial bar chart visualization is the radar chart proposed by Schlee et al.~\autocite{Schlee:RadarVis2017}, whose solution tends to overplot when more than two subsets need to be displayed simultaneously.
Therefore, we did not fill areas spanned by connected points with color to
avoid that one polygon completely overlaps another.
Furthermore, inferences about the radar map~\autocite{Schlee:RadarVis2017} depend heavily on the arrangement of features, since the main criterion for comparison is the shapes of the polygons.
Their solution to compute an arrangement that yields areas that achieve a maximum mean area difference between subgroups and a minimum area variance within subgroups only partially solves the problem, since still only a moderate number of up to about 20 features can be represented.
We chose to organize the 64 features according to expert-determined categories, e.g., quality of life, which makes it easier to find features and compare similar features.

In addition, our visualization is tinnitus-specific but can be used to display a compact summary for any condition or subset of index symptoms.
Whether the visualizations will be adopted by clinicians to guide appropriate tinnitus management strategies is adopted remains to be tested.
In a preliminary user study, clinicians suggested that graphical summaries of possible patient subtypes could ease the challenge of assigning an appropriate treatment strategy for specific combinations of symptom presentations.

By excluding patients who did not complete all questionnaires at T0, there may be a selection bias.
Possible reasons for noncompletion include unfamiliarity with the technical equipment used to record item responses, loss of motivation due to the relatively large number of questionnaires, and collisions with other baseline studies in the laboratory.
Nevertheless, the analysis of all 15 questionnaires led to insights into the
contributions of these questionnaires to phenotyping, possibly allowing a reduction in the number of questionnaires.
Because our results reflect only a snapshot of the patients' situation at baseline, a patient may transition from one phenotype to another at later stages of life, depending on tinnitus management.
Therefore, a next step would be to investigate the effects of treatment on these phenotypes in more detail and to find out whether some patient phenotypes benefit more than others.

\hypertarget{phenotypes-conclusions}{%
\section{Conclusions}\label{phenotypes-conclusions}}

We presented a workflow to find distinct tinnitus phenotypes by clustering using an algorithm that internally determines the optimal number of clusters.
We visualized the characteristics of each phenotype and the differences between multiple phenotypes using radial bar and line graphs.
We also provided a web application where phenotypes can be further explored with their treatment effects.
To our knowledge, this is the first approach that combines clustering of tinnitus patients with comprehensive visualization of subgroups in high-dimensional data and interactive components for exploration of patterns and treatment effects.

\hypertarget{part-exploiting-dynamics}{%
\part{EXPLOITING DYNAMICS}\label{part-exploiting-dynamics}}

\hypertarget{evo}{%
\chapter{Constructing Evolution Features to Capture Study Participant Change over Time}\label{evo}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-3}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-3}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We present a framework for cohort analysis in longitudinal cohort studies which constructs ``evolution features'' from latent temporal information describing the change of cohort participants over time.
We show that exploiting these novel features improves the generalization performance of classification models.
We report on results for SHIP and the outcome ``fatty liver''.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

Uli Niemann, Tommy Hielscher, Myra Spiliopoulou, Henry Völzke, and Jens-Peter Kühn. ``Can we classify the participants of a longitudinal epidemiological study from their previous evolution?''. In: \emph{Proc. of IEEE Int. Symposium on Computer-Based Medical Systems (CBMS)}. 2015, pp.~121-126. DOI: \href{https://doi.org/10.1109\%2FCBMS.2015.12}{10.1109/CBMS.2015.12}.

\end{infobox}

We begin the chapter with work related to the construction of temporal representations in medical data (Section~\ref{evo-intro}).
In Section~\ref{evo-concept} we present our evolution feature framework including a full workflow that encompasses steps for the extraction of evolution features, dealing with class imbalance and feature selection. Subsequently, we describe our evaluation setup (Section~\ref{evo-evaluation}) and present our results (Section~\ref{evo-results}).
Finally, in Section~\ref{evo-conclusions} we conclude this chapter and give answers to the aforementioned research questions.

\hypertarget{evo-intro}{%
\section{Motivation and Comparison to Related Work}\label{evo-intro}}

Epidemiological studies serve as basis for the identification of risk factors associated with a medical condition.
Machine learning is still rather little used in epidemiology, mainly due to the hypothesis-driven nature of their research.
However, examples of machine learning applications are the identification of health failure subtypes~\autocite{austin2013using} and the discovery of factors (including biomarkers) that modulate a medical outcome~\autocite{Raju2014,valavanis2013derivation}.
In longitudinal cohort studies measurements are performed in multiple study waves, hence researchers obtain access to sequences of recordings.
In the context of machine learning, extracting and leveraging the inherent temporal information from these sequences may increase model performance and thus, the understanding of the medical condition of interest.

In clinical applications temporal information is often exploited, predominantly for the analysis of patient records.
For example, Pechenizkiy et al.~\autocite{PechenizkiyEtAl:CBMS10} analyzed streams of recordings to predict patient rehospitalization after a health failure treatment.
Sun et al.~\autocite{SunEtAl:ICDM10} computed the similarity between streams of patients from patient monitoring data.
Combi et al.~\autocite{CombiEtAl:2010} reported on streams of life signals, in particular on the temporal analysis of the timestamped medical records of hospital patients.
However, participants of an epidemiological, population-based study are not hospital patients -- they are a random sample of the studied population, often with skewed class distribution.
In a \emph{longitudinal} study of this kind, recordings for the same cohort member are made at each moment.
Hielscher et al.~\autocite{HielscherEtAl:IDA14} presented a feature engineering approach to extract temporal information from multiple, but few patient recordings in a longitudinal epidemiological study.
First, for each assessment clusters of feature-value sequences associated with the target variable are found.
Afterwards, original and sequence features are used in conjunction for classification.
Hielscher et al.~\autocite{HielscherEtAl:IDA14} showed that classification performance increases when features with temporal information are incorporated into the feature space.
Instead of modeling the individual change of measurement values, our approach involves deriving multivariate change descriptors.
Patient evolution with clustering was studied by Siddiqui et al. \autocite{SiddiquiEtAl:BIH14} who proposed a method that predicts the evolution of a patient from timestamped data by clustering them on similarity and predicting cluster movement in the multi-dimensional space. However, the patient data considered in \autocite{SiddiquiEtAl:BIH14} are labeled at each moment.

Our workflow combines labeled and unlabeled timestamped data from a longitudinal study to improve classification performance on skewed data.
As the target variable, we study the multi-factorial disorder hepatic steatosis (fatty liver) on a sample of participants from the longitudinal population-based ``Study of Health in Pomerania'' (SHIP)~\autocite{Voelzke:SHIP11}, see Section~\ref{ship}.
For the SHIP cohort, the assessments (interviews, medical tests, etc.) were recorded in three \emph{moments} (SHIP-0, SHIP-1 and SHIP-2), that are ca. 5 years apart.
Temporal information is often used when analyzing patient data in a hospital, but there the time granularity is different.
For example, in an intensive care unit, timestamped data are collected at a fast pace, i.e., every minute or even every second.
In contrast, the participants of a longitudinal epidemiological study are monitored for a period of months or even years.
This implies that measurements of the same assessment in an epidemiological dataset are few and possibly far apart.
The large time span between two consecutive recordings complicates the application of methods designed for data that arrive with a higher frequency.
For example, a participant may exhibit alcohol abuse or become pregnant, stop smoking and start again, take antibiotics that affect the liver, or experience other lifestyle changes that turn the medical recordings taken 5 years ago irrelevant for the learning of the participant's current health state.
Another patient may have no changes in lifestyle and no illnesses, so their past data reflect only aging.

A further challenge is that only the recordings in SHIP-2 are labeled.
A reliable estimate of the fat accumulation in the liver was computed from magnetic resonance tomography images.
In SHIP-0, MRT was unavailable.
Instead, liver fat accumulation was derived from ultrasound -- a procedure with lower clinical accuracy.
In SHIP-1, the calculation was omitted altogether.
As a consequence, for a given SHIP participant a class label is available in SHIP-2, no label in SHIP-1 and a partially reliable indicator in SHIP-0.
Since hepatic steatosis is a reversible disorder, label imputation -- by means of a growth model~\autocite{SingerWillelt03} -- is not possible; participant evolution must be learned with only one moment with labeled data.

We address these challenges as follows.
First, we group study participants at each moment on similarity, thus building clusters of cohort members that have similar recordings at one of the three moments.
Then, we connect the clusters across time, thus capturing the transition of each cluster from one study wave to the next.
These transitions reflect the \emph{evolution of subgroups}, not individuals.
Hence, next to the single labeled recording per cohort participant, we also exploit the earlier, unlabeled recordings, the description of the cluster they are assigned to and information on how the clusters evolve over time.
We show that this new, augmented dataset, combining labeled and unlabeled data on individuals and on subgroups, improves classification and delivers additional insights on some factors associated to hepatic steatosis.

\hypertarget{evo-concept}{%
\section{Evolution Features}\label{evo-concept}}

We leverage latent temporal information of a longitudinal cohort study dataset by extracting informative features based on the individual change of participants and the transition of their respective clusters over time.
For this purpose, we exploit the similarity among participants at each moment, as surrogate to the labels which are not available in the first two moments, assuming that similar participants evolve similarly.
We call these new features \emph{``evolution features''}.
Our approach is illustrated in Figure \ref{fig:07-concept-workflow}~(a).
We monitor the individual change of participants across the study waves, trace the change of the clusters separately, extract new features (from labeled \emph{and} unlabeled data) and augment the original data space with our new descriptors of change.
The complete classification workflow is depicted in Figure \ref{fig:07-concept-workflow}~(b).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/07-concept-workflow} 

}

\caption{\textbf{Concept of evolution feature extraction for classification performance improvement}. (a) Clustering of longitudinal cohort data and subsequent generation of evolution features from the change of individuals (red) and whole clusters (green). (b) Overview of the classification workflow. \textbf{TODO: explain parentheses}}\label{fig:07-concept-workflow}
\end{figure}

In the following, we describe the clustering of study participants (Section~\ref{evo-concept-clustering}), the generation of evolution features (Section~\ref{evo-concept-evo-features}), our strategy of undersampling the majority class to balance class distribution (Section~\ref{evo-concept-undersampling}) and our feature selection strategy to extract a subset of \emph{informative} features as input for classification (Section~\ref{evo-concept-feature-selection}).

\hypertarget{evo-concept-clustering}{%
\subsection{Clustering}\label{evo-concept-clustering}}

For clustering, we prefer density-based clustering over partitional algorithms (like K-means), because our data contain extreme cases, the clusters may be arbitrarily shaped and of different sizes, and we cannot determine their number in advance.
At each moment \(t\), we run the DBSCAN~\autocite{EsterEtAl:DBSCAN96} algorithm to cluster the set \(Z(t)\) of recordings of all cohort members observed at \(t\).
For participant \(x\), \(v(x,f,t)\) denotes the value of \(x\) for feature \(f\in\) feature-set \(F\) at \(t\), and \(obs(x,F,t)\) the set of all feature recordings for \(x\) at \(t\) (cf.~notation in Table~\ref{tab:07-nomenclature}).



\begin{table}

\caption{\label{tab:07-nomenclature}\textbf{Symbols and basic functions.}}
\centering
\begin{tabular}[t]{ll}
\toprule
\textbf{Term} & \textbf{Description}\\
\midrule
$x$ & a study participant from cohort $X$\\
$t$ & a study moment, one of \vphantom{1} $\{t,\ldots,t_T\}$\\
$t$ & a study moment, one of $\{t,\ldots,t_T\}$\\
$f$ & a feature from the set of all features $F$\\
$v(x,f,t)$ & the value of $x$ for feature $f$ at moment $t$\\
$obs(x,F,t)$ & all measurements for $x$ at $t$, i.e., $\{v(x,f,t)|f\in{}F\}$\\
$Z(t)$ & all observations at $t$, i.e., $\{obs(x,F,t)| x \in X\}$\\
$c(x,t)$ & cluster membership of $x$ at $t$; if $x$ is an outlier at $t$, then $c(x,t)$ is empty\\
$d(x,z,t)$ & distance between $x$ and $z$ at $t$ (cf. Eq. \ref{eq:heom-adjusted})\\
$kNN(x,k,t)$ & the set of $k$ nearest neighbors of $x$ at  $t$\\
$centr(x,t)$ & centroid of $c(x,t)$\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Distance function.}
For the distance between participants \(x,z\) at \(t\), we use the \emph{adjusted heterogeneous Euclidean overlap metric}~\autocite{HielscherEtAl:IDA14,Wilson97}, which weights the difference between two values \(x,z\) for feature \(f\) by the feature's information gain \(G(f)\), scaled to the largest observed value \(G^*\), defined as:
\begin{equation}
d(x,z,t)=\sqrt{\sum_{f \in F} \left(\frac{G(f)}{G^*}\cdot \delta\left(v(x,f,t),v(z,f,t)\right)\right)^2}.
\label{eq:heom-adjusted}
\end{equation}
For continuous features, \(\delta(a,b)\) is the min-max-scaled difference between the values \(a,b\), i.e., \((a-b)/(\max(f)-\min(f))\).
For nominal features, \(\delta(a,b)\) is 0 if \(a=b\), and 1 otherwise.

\textbf{DBSCAN Parameter setting.}
DBSCAN relies on two parameters: the radius \(eps\) of the neighborhood around a data point, and the minimum number \(minPts\) of neighbors for a point to be a core point.
We use the ``elbow'' heuristic of ~\autocite{EsterEtAl:DBSCAN96} which determines a suitable \(eps\) value for a given \(minPts\) value, and is illustrated in Figure~\ref{fig:07-k-dist-graph}.
More specifically, we define a parameter \(k\), and compute for each \(x\in{}Z(t)\) the distance k-dist\((x,k)\) to its k-th nearest neighbor.
We sort these distances, draw the k-dist\((x,k)\) graph \emph{g} and span the line \emph{l} connecting the smallest k-dist() value to the largest one.
Then, we set \(eps\) to the k-dist value with maximum distance between \emph{g} and \emph{l}.



\begin{figure}

{\centering \includegraphics[width=0.34\linewidth]{figures/07-k-dist-graph} 

}

\caption{\textbf{Setting \(eps\) based on the k-dist graph for a given \(minPts\).} The k-dist graph \emph{g} depicts the sorted distances to the points' k-th next neighbors. A suitable \(eps_{opt}\) can be identified at the position with maximum distance between the k-dist and the line \emph{l} that connects the first and the last point of \emph{g}. For DBSCAN clustering with \(minPts\) = k, \(eps_{opt}\), points with k-dist \(\leq\) \(eps_{opt}\) will become core points, else border or noise points.}\label{fig:07-k-dist-graph}
\end{figure}

\hypertarget{evo-concept-evo-features}{%
\subsection{Constructing Evolution Features}\label{evo-concept-evo-features}}

Table~\ref{tab:07-tab-evo-features} provides a description of all evolution features.
For each cohort member \(x\) and moment \(t\), we record the cluster containing \(x\) (feature 1 in Table~\ref{tab:07-tab-evo-features}),
(2) the distance of \(x\) to this cluster's centroid,
(3) the fraction of positively labeled participants among the \(k\) nearest neighbors of \(x\),
(4) the (graph-based) cohesion~\autocite{TanDMbook} and (5) Silhouette coefficient~\autocite{TanDMbook} of \(x\), and (6) the (graph-based) separation~\autocite{TanDMbook} of \(x\) to cohort participants outside this cluster.
We compute the difference of the cohesion, silhouette and separation values from \(t\) to all later moments \(\{t' \in T|t'>t\}\) (7-9), and also check how much the values of these metrics change as \(x\) moves from \(c(x,t)\) to \(c(x,t')\) (10-12).
We record whether \(x\) is an outlier, i.e., a DBSCAN noise point at some moment (13).
For \(t\) and \(\{t' \in T|t'>t\}\), we compute the fraction of cohort members who are in the same cluster as \(x\) in \(t\) and \(t'\) (14), and the fraction of common \(k\) nearest neighbors (15), and the change of the distance between \(x\) and its centroid at \(t\), from \(t\) to \(t'\) (16).
We further record changes in the sequence of values for a feature, including real (17), absolute (18) and relative (19) differences between the values at two moments.
We measure how a cluster shrinks/grows from \(t\) to \(t'\) (20), and how much its members move (on average) closer or far apart from their previous positions (21-23).
In that way, we extract information about the evolution of the participants, distinguishing among those that evolve smoothly and those that switch among clusters.
We transfer this information to evolution features, thus enriching the feature space with information from the unlabeled moments.



\begin{table}

\caption{\label{tab:07-tab-evo-features}\textbf{Overview of extracted features.} The first group of features (I) comprises the cluster membership and aggregated distance information for each participant and for each moment; feature group II is on changes in the participant's position (in the hyperspace) relative to the cluster and to its closest neighbors; feature group III captures changes in the values of the participant's recordings; feature group IV refers to changes in the clusters.}
\centering
\begin{tabular}[t]{llp{8cm}}
\toprule
\textbf{\#} & \textbf{Name} & \textbf{Description}\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{I: Features for participant $x$ at each moment}}\\
\hspace{1em}1 & \texttt{Cluster\_t} & Cluster ID of $x$ at $t$\\
\hspace{1em}2 & \texttt{dist\_To\_Centroid\_t} & distance of $x$ to the centroid of cluster $c(x,t)$, denoted as $\widehat{c(x,t)}$\\
\hspace{1em}3 & \texttt{fraction\_Of\_POS\_kNN\_}$k$\texttt{\_t} & fraction of the $k$ nearest neighbors of $x$ at $t$ from the positive class\\
\hspace{1em}4-6 & \texttt{a\_t := a(x,t,c(x,t))} & where $a$ is one of {cohesion, silhouette, separation} \cite{TanDMbook}; cohesion$_t$ is the cohesion of $x$ at $t$ w.r.t. the members of $c(x,t)$ -- and similarly for silhouette and for separation\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{II: Evolution features linked to each participant $x$}}\\
\hspace{1em}7-9 & \texttt{a\_Delta\_t\_t'} & difference $a_{t} - a_{t'}$ for $a$ as above\\
\hspace{1em}10-12 & \texttt{a\_Movement\_t\_t'} & difference $a(x,t,c(x,t)) - a(x,t',c(x,t))$ for $a$ as above, referring to the same cluster $c(x,t)$ at two moments $t,t'$; at $t'$, $x\in{}c(x,t')$, which does not need to be the same as $c(x,t)$\\
\hspace{1em}13 & \texttt{was\_becomes\_Outlier\_t\_t'} & 4-valued flag on whether $x$ was outlier in both $t, t'$, only in $t$, only in $t'$ or in neither$t, t'$\\
\hspace{1em}14 & \texttt{same\_Cluster\_t\_t'} & $c(x,t)\cap{}c(x,t')\setminus\{x\}$: set of cohort members that are in the same cluster as $x$ in $t$ and in $t'$\\
\hspace{1em}15 & \texttt{same\_kNN\_}$k$\texttt{\_t\_t'} & $kNN(x,k,t)\cap{}kNN(x,k,t')$, i.e., the set of cohort members who are among the $k$ nearest neighbors of $x$ in both $t$ and in $t'$; they do not need to be in the same cluster as $x$\\
\hspace{1em}16 & \texttt{shift\_To\_Old\_Centroid\_t\_t'} & difference $d(x,\widehat{c(x,t)},t')-(x,\widehat{c(x,t)},t)$\\
\addlinespace[0.3em]
\multicolumn{3}{p{\linewidth}}{\textbf{III: Evolution features associated w. the value of each original feature $f$ for participant $x$}}\\
\hspace{1em}17-19 & \texttt{A\_Diff\_f\_t\_t'} & difference between $v(x,f,t)$ and $v(x,f,t')$ for feature $f$, where $A$ is either real difference, absolute difference or relative difference to $v(x,f,t)$\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{IV: Evolution features linked to a whole cluster $c$}}\\
\hspace{1em}20 & \texttt{smaller\_Cluster\_Fraction\_t\_t'} & difference of the size of cluster $c$ at $t'$ with respect to the size it had at $t$; $c$ is matched to the clusters of $t$ on the basis of member overlap\\
\hspace{1em}21-23 & \texttt{movement\_d\_t\_t'} & distance $d$ between the locations of the members of cluster $c$ at $t$ and their locations at $t'$, where $d$ is one of Euclidean distance, HEOM distance (cf. Eq. \ref{eq:heom-adjusted}, Cosine similarity\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{evo-concept-undersampling}{%
\subsection{Undersampling}\label{evo-concept-undersampling}}

For imbalanced data, feature selection and classification are often biased in favor of the majority class~\autocite{leevy2018survey}.
To avoid this problem, prior to the application of CFS we undersample the majority class and generate a balanced data set to select features informative with respect to all classes.

\hypertarget{evo-concept-feature-selection}{%
\subsection{Feature Selection}\label{evo-concept-feature-selection}}

We use the feature selection method of Hielscher et al.~\autocite{HielscherEtAl:CBMS14} as follows.
First, we invoke correlation-based feature selection~\autocite{hall2000correlation} (CFS), which builds up a feature set by iteratively inserting to it the feature that adds most ``merit'' to it.
The merit \(M_F\) of a feature set \(F\) is computed by calculating the information gain for each pair of features in \(F\) (lower gain corresponds to low correlation and is thus preferred) and for each feature in \(F\) towards the target variable (higher gain is better).
Continuous features are first discretized with the entropy-based method of Hielscher et al.~\autocite{Fayyad:MDL93}.
We discretize only for feature selection; for clustering and classification we use the original values.

As shown in Figure~\ref{fig:07-concept-workflow}, we perform feature selection twice.
The first time, we consider only features recorded in all moments.
This is essential for evolution tracing: we can only compute distances between objects in clusters located in the same topological space.
After generating the evolution features, we build up the complete set of features, also considering those not recorded in each moment.
On this set we perform feature selection again, to discard unpredictive (original or evolution) features.
This final feature set is then used for classification.

\hypertarget{evo-evaluation}{%
\section{Evaluation Setup}\label{evo-evaluation}}

We evaluate our workflow with 10-fold cross-validation on four off-the-shelf classification algorithms: random forest~\autocite{Breiman:RandomForests2001} (RF), C4.5 decision tree~\autocite{Quinlan:C4.5.1993}, Naïve Bayes~(NB) and k-nearest neighbor (kNN).
Next, we compare the generalization performance for each algorithm when it is used alone (\emph{baseline} variant) vs.~when incorporated into our workflow (\emph{worflow-enhanced} variant).
Further, we study the impact of different combinations of the three workflow components \emph{undersampling} (\texttt{U}), \emph{feature selection} (\texttt{F}) and \emph{incorporation of generated evolution features} (\texttt{G}).
As shown in Table~\ref{tab:07-workflow-variants}, \texttt{Baseline} simply invokes the classification algorithm; we use the classification algorithms which achieves the highest F-measure scores.
The variant \texttt{U-G} performs undersampling and uses the generated evolution features for classification.
Since we undersample only for feature selection and then build the classification models on the original dataset, so \texttt{U-G} is identical to \texttt{-\/-G} and \texttt{U-\/-} is identical to the \texttt{Baseline} variant, so we omit to explicityl list \texttt{U-G} and \texttt{U-\/-}.

The main parameter is \(k\) which is the number of neighbors of a data point: we set \(minPts\) = \(k\) and use \(k\) to derive the values of the DBSCAN parameter \(eps\) (cf.~Section~\ref{evo-concept-clustering}) and of the parameters for the features \texttt{same\_kNN\_}\(k\)\texttt{\_t\_1\_t\_2} and \texttt{fraction\_Of\_POS\_kNN\_}\(k\)\texttt{\_t} (cf.~Table~\ref{tab:07-tab-evo-features}).
Further, the number of nearest neighbors for the k-NN classification algorithm is also set to \(k\).
We vary \(k\) to measure its impact on classification performance.

Following the findings in Chapters~\ref{03-imm} and~\ref{04-sdclu} on the differences between female and male participants with respect to the outcome, we run the experiments on the whole dataset (\emph{Partition\textsubscript{all}}), and on the partitions of female (\emph{Partition\textsubscript{f}}) and of male (\emph{Partition\textsubscript{m}}) participants.
Finally, we list the most important features found in \emph{Partition\textsubscript{all}} and its two subsets.



\begin{table}

\caption{\label{tab:07-workflow-variants}\textbf{Workflow variants.} \texttt{UFG} is the complete workflow.}
\centering
\begin{tabular}[t]{>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{2.5cm}}
\toprule
\textbf{Workflow components} & \textbf{Under-sampling} & \textbf{Feature selection} & \textbf{Evolution features}\\
\midrule
\ttfamily{UFG} & \ding{51} & \ding{51} & \ding{51}\\
\ttfamily{UF-} & \ding{51} & \ding{51} & \ding{55}\\
\ttfamily{-FG} & \ding{55} & \ding{51} & \ding{51}\\
\ttfamily{-F-} & \ding{55} & \ding{51} & \ding{55}\\
\ttfamily{-{}-G} & \ding{55} & \ding{55} & \ding{51}\\
\ttfamily{Baseline} & \ding{55} & \ding{55} & \ding{55}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{evo-results}{%
\section{Results}\label{evo-results}}

Figure~\ref{fig:07-perf-wf-classifiers} shows sensitivity (left), specificity (center) and F-measure (right) for the simple classifiers (gray curves) and their workflow-enhanced counterparts (same line style, colored), for different \(k\).
Overall, each workflow-enhanced variant outperforms its simple counterpart with respect to sensitivity and F-measure, and outperforms or performs slightly worse with respect to specificity.
The workflow-enhanced Naive Bayes performs best with respect to sensitivity for any \(k\) and best for \(k=31\).
Decision trees exhibit highest F-measure, with improvements on sensitivity and F-Measure compared to its simple variant, albeit specificity being sligthly worse; improvements are less for large \(k\).
Random Forests benefit the most from our workflow, with an absolute improvement in F-measure of over 30\% (green vs gray ``+'' curves in right part of Figure~\ref{fig:07-perf-wf-classifiers}).
One explanation for the rather poor sensitivity of the simple RF variant is the large number of trees (100) learned on data samples containing very few positive examples, and RF could be trapped by the many majority class examples.
This is consistent with the specificity curve (almost straight line around 95\%) of simple RF, while the F-measure is slightly above 40\%.
Our workflow improves RF sensitivity (63\%) and F-measure (65\%), while specificity remains high (90\%).
Overall, the impact of \(k\) on the three measures is limited for all algorithms except of the workflow-enhanced and the baseline k-NN which is naturally affected stronger by the value of \(k\) than any other algorithm.
Therefore, the workflow-enhanced variants outperform their simple counterparts in terms of sensitivity and F-measure. For some algorithms, our workflow prevents overfitting of the negative class.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/07-perf-wf-classifiers} 

}

\caption{\textbf{Comparison of classification performance between workflow and baseline.} Sensitivity (true positive rate), specificity (false positive rate) and F-measure scores of different classifiers when varying the number k of neighbors to a cohort member which impacts the clustering result. For each classifier two performance curves are shown: a colored one for the workflow-enhanced version and a gray one for the baseline counterpart. Higher values are better for all measures. From \autocite{Niemann:CBMS2015}.}\label{fig:07-perf-wf-classifiers}
\end{figure}

The workflow component-specific analysis results in Figure~\ref{fig:07-perf-wf-components} show that our complete workflow \texttt{UFG} and the variants \texttt{UF-} and \texttt{-\/-G} outperform \texttt{-\/-\/-} in sensitivity and F-measure.
The variants \texttt{-F-} and \texttt{-FG} perform well only regarding specificity, which suggests that feature selection may not be fruitful without undersampling for datasets with class imbalance.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/07-perf-wf-components} 

}

\caption{\textbf{Comparison of classification performance for workflow components.} Sensitivity, specificity and F-measure scores for each workflow variant and the baseline using decision tree for learning. From \autocite{Niemann:CBMS2015}.}\label{fig:07-perf-wf-components}
\end{figure}

\textbf{Important features.} The performance of the workflow variants which include feature selection indicates that a small number of features is sufficient for class separation.
Hereafter, for each partition we report on the evolution features selected for classification and among the top-15 features according to information gain.
Figure~\ref{fig:07-imp-features-all} shows that for Partition\textsubscript{All} 3 out of these 15 features are generated evolution features.
The boxplots (a) and (c) in Figure~\ref{fig:07-imp-features-all} refer to differences between values recorded in two moments.
The feature \texttt{separationDelta\_g\_1\_2} measures the difference in cluster separation for each participant based on the cluster assignment in moment 1 and 2, and corresponds to entry \#9 in Table~\ref{tab:07-tab-evo-features}.
Participants of the positive exhibit a higher median in \texttt{separationDelta\_g\_1\_2} than participants of the negative class indicating that clusters harboring mostly positive participants cover larger, more sparse areas.\\
The feature \texttt{relative\_Difference\_som\_huef\_g\_0\_1} (\#19) quantifies the difference in a participant's hip circumference between SHIP-0 and SHIP-1, relative to the value in SHIP-0.
While on average study participants from both classes lose weight when they grow older, negative participants reduce more weight compared to positive participants (cf.~Figure \ref{fig:07-imp-features-all}~(c)), which in general reflects differences in life styles.
The mosaic chart in Figure \ref{fig:07-imp-features-all}~(b) for feature \texttt{fraction\_of\_Positives\_kNN\_1\_g\_2} (\#3) indicates that the \emph{nearest neighbor} of a participant with fatty liver is more likely to also exhibit the disorder than it is for a non- fatty liver participant.



\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/07-imp-features-all} 

}

\caption{\textbf{Visualization of selected evolution features which contribute most to class separation for the whole dataset Partition\textsubscript{All}.}}\label{fig:07-imp-features-all}
\end{figure}

For Partition\textsubscript{F}, 5 out of the top-15 features are evolution features, cf.~Figure~\ref{fig:07-imp-features-women}.
Compared with female participants without the disorder, female subjects with fatty liver exhibit a larger distance to the centroid of their cluster in SHIP-1 (\#2), a lower silhouette coefficient in SHIP-1 (\#5), a higher difference in waist circumference between SHIP-0 and SHIP-2 (\#19), and a lower relative difference in serum triglycerides concentration between SHIP-0 and SHIP-1 (\#19).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/07-imp-features-women} 

}

\caption{\textbf{Visualization of selected evolution features which contribute most to class separation for Partition\textsubscript{F}.}}\label{fig:07-imp-features-women}
\end{figure}

For Partition\textsubscript{M}, 2 out of the top-15 features are evolution features (Figure~\ref{fig:07-imp-features-men}), including the relative difference in waist circumference between SHIP-1 and SHIP-2 (\#19) and difference in separation between SHIP-0 and SHIP-1 (\#6).
For both features, patients exhibiting the disorder have greater values.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/07-imp-features-men} 

}

\caption{\textbf{Visualization of selected evolution features which contribute most to class separation for Partition\textsubscript{M}.}}\label{fig:07-imp-features-men}
\end{figure}

\hypertarget{evo-conclusions}{%
\section{Conclusions from Exploiting Study Participant Evolution}\label{evo-conclusions}}

\textbf{TODO:LINK TO RESEARCH QUESTIONS}
\textbf{TODO: say that in the original submission, we also report on static features}

We proposed a workflow for the the classification of longitudinal cohort study data which exploits inherent temporal information by clustering the cohort participants at each moment, linking the clusters and tracing participant evolution over the moments of the study.
From the clusters and their transitions we extract \emph{evolution features}, which are added to the feature space and subsequently used for classification.
The workflow improves the generalization performance with respect to sensitivity and F-measure scores.
The generated evolution features contribute to this improvement, even when they are used alone and without undersampling of the skewed data.
We show that the \emph{change} of the values of somatographic variables and cluster quality indices over time are predictive.

\hypertarget{diabfoot}{%
\chapter{Feature Extraction From Short Temporal Sequences for Clustering}\label{diabfoot}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-4}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-4}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We present an approach to build representations from short temporal sequences via clustering by the example of pressure- and posture-dependent plantar temperature and pressure in patients with diabetic foot syndrome.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

\begin{itemize}
\tightlist
\item
  Uli Niemann, Myra Spiliopoulou, Fred Samland, Thorsten Szczepanski, Jens Grützner, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, and Peter R. Mertens. ``Learning Pressure Patterns for Patients with Diabetic Foot Syndrome''. In: \emph{Proc. of IEEE Int. Symposium on Computer-Based Medical Systems (CBMS)}. 2016, pp.~54--59. DOI: \href{https://doi.org/10.1109\%2FCBMS.2016.31}{10.1109/CBMS.2016.31}.
\item
  Uli Niemann, Myra Spiliopoulou, Thorsten Szczepanski, Fred Samland, Jens Grützner, Dominik Senk, Antao Ming, Juliane Kellersmann, Jan Malanowski, Silke Klose, and Peter R. Mertens. ``Comparative Clustering of Plantar Pressure Distributions in Diabetics with Polyneuropathy May Be Applied to Reveal Inappropriate Biomechanical Stress''. In: \emph{PLOS ONE} 11.8 (2016), pp.~1--12. DOI: \href{https://doi.org/10.1371\%2Fjournal.pone.0161326}{10.1371/journal.pone.0161326}.
\item
  Uli Niemann, Myra Spiliopoulou, Jan Malanowski, Juliane Kellersmann, Thorsten Szczepanski, Silke Klose, Eirini Dedonaki, Isabell Walter, Antao Ming, and Peter R. Mertens. ``Plantar temperatures in stance position: A comparative study with healthy volunteers and diabetes patients diagnosed with sensoric neuropathy''. In: \emph{EBioMedicine} 54 (2020), p.~102712. DOI: \href{https://doi.org/10.1016\%2Fj.ebiom.2020.102712}{10.1016/j.ebiom.2020.102712}.
\end{itemize}

\end{infobox}

\textbf{to be written}

\hypertarget{part-post-mining-for-interpretation}{%
\part{POST-MINING FOR INTERPRETATION}\label{part-post-mining-for-interpretation}}

\hypertarget{iml}{%
\chapter{Post-Hoc Interpretation of Classification Models}\label{iml}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-5}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-5}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We present a machine learning workflow that combines classification of high-dimensional medical data and model explanation using post-hoc interpretation methods.
To this end, we use Shapely value explanations (SHAP), LASSO coefficients, and partial dependency graphs.
Our approach provides statistics and visualizations representing global feature importance, instance-individual feature importance, and subpopulation-specific feature importance, all of which help illuminate complex black-box machine learning models.
We report our results on three applications: (i) tinnitus-related distress in tinnitus patients, (ii) depressivity in tinnitus patients, and (iii) rupture risk in intracranial aneurysms.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

\begin{itemize}
\tightlist
\item
  Uli Niemann, Philipp Berg, Annika Niemann, Oliver Beuing, Bernhard Preim, Myra Spiliopoulou, and Sylvia Saalfeld. ``Rupture Status Classification of Intracranial Aneurysms Using Morphological Parameters''. In: \emph{Proc. of IEEE Int. Symposium on Computer-Based Medical Systems (CBMS)}. 2018, pp.~48-53.\\
  DOI: \href{https://doi.org/10.1109\%2FCBMS.2018.00016}{10.1109/CBMS.2018.00016}.
\item
  Uli Niemann, Benjamin Boecking, Petra Brueggemann, Wilhelm Mebus, Birgit Mazurek, and Myra Spiliopoulou. ``Tinnitus-related distress after multimodal treatment can be characterized using a key subset of baseline variables''. In: \emph{PLOS ONE} 15.1 (2020), pp.~1-18.\\
  DOI: \href{https://doi.org/10.1371\%2Fjournal.pone.0228037}{10.1371/journal.pone.0228037}.
\item
  Uli Niemann, Petra Brueggemann, Benjamin Boecking, Birgit Mazurek, and Myra Spiliopoulou. ``Development and internal validation of a depression severity prediction model for tinnitus patients based on questionnaire responses and socio-demographics''. In: \emph{Scientific Reports} 10.1 (2020), p.4664.\\
  DOI: \href{https://doi.org/10.1038\%2Fs41598-020-61593-z}{10.1038/s41598-020-61593-z}.
\end{itemize}

\end{infobox}

In medical applications, understanding and clearly communicating the results of a machine learning model is critical to deriving actionable knowledge that can ultimately be used to improve disease prevention, diagnosis, and treatment.
Obtaining results that are easily understood by both data scientists and medical experts helps formulate new hypotheses regarding the relationship between potential risk or protective factors and the target; the significance of these relationships can be tested in follow-up studies.
Current state-of-the-art machine learning algorithms produce models with superior performance compared to simpler but interpretable models, such as decision trees, rule lists, or linear regression fits.
However, because these opaque \emph{black boxes} involve many complex feature interactions or decisions, some of which are nonlinear, it is often difficult to explain them in an understandable way.
Arising from the need to provide understandable insights into otherwise opaque models, the \emph{interpretability} community of machine learning has gained traction with the goal of resolving the dilemma of choosing between moderately accurate but interpretable models and highly accurate but opaque black-box models.

In this chapter, we describe a comprehensive data analysis workflow for high-dimensional medical data that includes classification, feature elimination, and post-learning analysis steps in addition to application-specific preprocessing steps.
A variety of learners are used for classification, from simple interpretable models to complex black boxes.
For the best classifier, we explore different post-hoc interpretation methods to derive model-, observation-, and subpopulation-level insights.
We report our results and evaluate our approach based via experiments on three applications.

This chapter is organized as follows.
In Section~\ref{iml-motivation}, we describe reasons for using interpretable machine learning methods and provide methodological underpinnings of a selection of pioneering methods.
Subsequently, we present the components of our mining workflow in Section~\ref{iml-workflow}, which includes correlational analysis, image preprocessing, feature selection, classification of high-dimensional medical data, hyperparameter tuning, model evaluation and our approach of putting interpretation methods into use.
In Section~\ref{iml-results}, we report our findings on three applications: prediction of (i) tinnitus-related distress and (ii) depression after treatment in tinnitus patients, as well as (iii) rupture status classification in intracranial aneurysms.
We discuss these results in Section~\ref{iml-discussion} and conclude the chapter in Section~\ref{iml-conclusion}.

\hypertarget{iml-motivation}{%
\section{Motivation and Methodological Underpinnings}\label{iml-motivation}}

Current state-of-the-art machine learning algorithms, such as gradient boosting~\autocite{Friedman:PDP2001} for tabular data and deep learning~\autocite{Goodfellow:DL2016} for unstructured data (images, videos, audio recordings), are widely used to support medical decision-making.
These methods produce models which typically achieve better predictive performance than simpler models such as decision trees, rule lists, or linear regression fits.
However, they are also more complex, making it more difficult to understand why a prediction was made.
Thus, practitioners may face the dilemma of choosing either an opaque black-box model with high predictive power or a simple, less accurate model that can at least be explained to the domain expert.
Especially in high-risk domains such as healthcare, where misconceptions can have serious consequences, the ability to explain the reasoning of a model is a highly desirable, if not essential, property of any decision-support system~\autocite{guidotti2018survey,molnar2020interpretable}.

As a result, methods that explain the predictions of complex machine learning models have attracted increasing attention in recent years~\autocite{carvalho2019machine,adadi2018peeking}.
Existing methods are classified according to different criteria~\autocite{molnar2020interpretable}.
For example, a distinction is made between \emph{intrinsically interpretable models} and \emph{post-hoc explanations}.
The former often entails limiting model complexity by choosing algorithms that produce transparent models, such as decision trees or linear regression models.
Decision trees, for example, can be intuitively visualized with node-link diagrams.
Features in split conditions near the tree root generally have a higher impact on predictions than features occurring at lower tree levels or within leaf nodes.
Quantitative measures calculate the overall importance of a feature by the decrease in impurity or variance in nodes where the feature occurs compared to parent nodes~\autocite{kazemitabar2017variable}.
Furthermore, the data partitions created by a decision tree can be described by understandable conditions such as ``body mass index \textgreater{} 30,'' and the decision paths from root to leaf nodes provide insights into feature interactions.
In addition, they can be used for contrasting predictions for individual instances, e.g., by considering alternative feature values and their effects on model prediction (\emph{``If the patient had a body mass index of 25 instead of 30, what difference in terms of prediction would that have.''}).
Disadvantages are that decision trees are not able to capture linear, non-axis-parallel relationships between predictors and response, and they can be unstable with respect to small changes in training data~\autocite{hastie2009elements}.
Therefore, they may be unsuitable for very complex learning tasks.

If a more sophisticated model is trained instead, post-hoc methods can be applied to examine the model after training.
The output of these methods can be \emph{feature summary statistics}, \emph{model internals}, \emph{individual observations}, and \emph{feature summary visualization}~\autocite{molnar2020interpretable}.
In general, feature summary statistics are individual scores that express the overall importance of a feature to model prediction or the strength of the feature's interaction with the other features.
Examples of model internals are the coefficients of a linear model or weight vectors of a neural network.
Individual observations can describe representatives (or prototypes) of observation subgroups for which the model provides consistent predictions for all subgroup members.
Individual observations can also be used to provide counterfactual explanations, e.g., to determine the minimum change that will cause the model to predict a different class for a particular observation of interest.

\textbf{Partial dependence plots.} Visualizations of feature summaries typically depict trends in the relationship between a subset of features and the predicted response, often in the form of curves or surface plots.
The \emph{partial dependence plot}~\autocite{Friedman:PDP2001} (PDP) is a widely used tool for visually depicting the marginal effect of one or more predictors on the predicted response of a model.
As an example, the PDP in Figure \ref{fig:09-pd-intro}~(a) shows a roughly S-shaped relationship between the values of the predictor and the values of the response estimated by the model.



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/09-pd-intro} 

}

\caption{\textbf{Illustrations of a partial dependence plot (PDP) and an individual conditional expectation (ICE) plot on artificial data.} (a) PDP for a predictor on an artificial dataset. Points represent a sample of the predictor distribution. (b) PDP augmented with ICE curves. There are 2 distinct subsets of observations for which the PD is different in the upper half of the predictor distribution.}\label{fig:09-pd-intro}
\end{figure}

Let \(\zeta\) be the classification model (or any general function that returns a single real value) and let \(F=Q\cup R\) be the total set of features, where \(Q\) is the chosen subset of features and \(R\) is the complement subset.
The partial dependence \(PD\) of a model \(\zeta\) on \(Q\) can be represented as

\begin{equation}
PD(Q)=\mathbb{E}_{R}\left[\zeta(X)\right]=\int\zeta(Q,R)p_R(R)\,dR.
\label{eq:pd}
\end{equation}

Here, \(p_R(R)\) is the marginal probability density of \(R\), i.e., \(p_R(R)=\int p(X)\,dQ\), where \(p(X)\) is the joint density of dataset \(X\).
When this complement marginal density \(p_R(R)\) is estimated from the training data, \(PD\) can be approximated as

\begin{equation}
PD(Q)=\frac{1}{N}\sum_{i=1}^N \zeta(Q,R_{i})
\label{eq:pd-approx}
\end{equation}

where \(R_i\) are the actual values of the complementary features for observation \(i\), and \(N\) is the total number of observations in the training data.
The cardinality of \(Q\) is usually chosen to be either equal to 1 or 2.
The results are visualized as a line chart (if \(|Q|=1\)) or a contour chart (if \(|Q|=2\)).
In practice, a random sample is often drawn from \(Q\) to reduce computation time.

Because averaging across all observations removes information about variability, PD curves can obscure the potentially distinct observation subgroups with substantially different effects between predictors and model output.
As a remedy, Goldstein et al.~\autocite{Goldstein:ICE2015} proposed \emph{individual conditional expectation} (ICE) plots showing a curve for each observation.
Figure \ref{fig:09-pd-intro}~(b) illustrates an example where there are a small number of observations (black curves) that differ from the rest because their PD is constant for the second half of the predictor distribution.

\textbf{LIME.} Another criterion for distinguishing model interpretation methods is whether their explanations are \emph{global} or \emph{local}, i.e., whether the explanations apply to all observations or only to one or a small number of selected observations.
\emph{Local Interpretable Model-Agnostic Explanations}~\autocite{RibeiroEtAl:KDD2016} (LIME) is a popular local post-hoc interpretation method.
The main assumption of LIME is that a complex model is linear on a local scale~\autocite{RibeiroEtAl:KDD2016}.
Thus, to explain the predictions of a black-box model for a particular observation of interest \(i\), LIME generates a surrogate model that is intrinsically interpretable and whose predictions are similar to the predictions of the black-box model in the \emph{proximity} of \(i\).
The main ideas of LIME are shown in Figure~\ref{fig:09-lime}.
Figure \ref{fig:09-lime}~(a) shows the decision boundary of a black-box model.
Since the form of the nonlinear decision boundary is quite complex, the model and its predictions cannot be explained in simple terms.
LIME attempts to approximate the behavior of the black-box model by creating a linear surrogate model that performs well, especially near a user-selected instance of interest.
To this end, a \emph{perturbed} training set is created by repeatedly randomly changing the values of the instance of interest.
Figure \ref{fig:09-lime}~(b) shows the instance of interest and the perturbed instances, where the glyph size represents the proximity to the instance of interest.

A linear \emph{surrogate model} is then trained on this dataset, with observation weights proportional to their distance from the instance of interest.
In Figure \ref{fig:09-lime}~(b), the decision boundary of the surrogate model is shown by the dashed line.
Finally, model internals are displayed to the user as an explanation, such as the coefficients of a logistic regression model.
Figure \ref{fig:09-lime}~(c) shows a feature importance ranking, where the bar height represents the model coefficient of a feature.
While LIME provides intuitive interpretations and is applicable to both tabular and non-tabular data, there are several design decisions to make and hyperparameters to tune, including neighborhood kernel and width, surrogate model family, feature selection mechanism, number of features considered for the surrogate model, among others.
The stability of the results of LIME has been questioned~\autocite{alvarez2018robustness,visani2020optilime}.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-lime} 

}

\caption{\textbf{Illustration of LIME's main ideas.} (a) A data set with a two-class problem, represented as a two-dimensional scatterplot for simplicity. The nonlinear decision boundary of a black-box model cannot be easily explained. (b) LIME aims to approximate the predictions of a black-box model for the vicinity of an instance of interest by an intrinsically interpretable model, such as a logistic regression model. The dashed line shows the linear decision boundary of this surrogate model. (c) A feature importance ranking can be derived from the model coefficients.}\label{fig:09-lime}
\end{figure}

\emph{Model-specific} interpretation methods are limited to specific model families, while \emph{model-agnostic} interpretation methods can be applied to any type of model.
Model-specific methods are based on model internals and are widely used for neural networks~\autocite{samek2020toward}, e.g.~layered relevance propagation~\autocite{bach2015pixel}, which explicitly uses the layered structure of a neural network to infer explanations.
In contrast, model-agnostic methods are decoupled from the actual learning process and do not have access to algorithmic internals.
Since they only consider the output of the models, i.e., the predictions, most model-agnostic methods are also post-hoc.
For example, LIME is a representative of a model-agnostic, post-hoc interpretability method.

\textbf{SHAP.} Closely related to LIME is the \emph{Shapley Additive Explanations}~\autocite{Lundberg:SHAP2017} (SHAP) framework, which derives additive feature attributions to the predictions of a model.
SHAP is based on \emph{Shapley values}~\autocite{lipovetsky2001analysis,vstrumbelj2014explaining,shapley1953value}, originally developed for game theory.
The term ``additive'' denotes that for a given observation, the model output should be equal to the sum of the attributions of each feature.
More specifically, for observation \(x\), the model output \(\zeta(x)\) is
\begin{equation}
\zeta(x)=\phi(\zeta,x)_0 + \sum_{j=1}^M \phi(\zeta,x)_j
\label{eq:shap-additivity}
\end{equation}

where \(\phi(\zeta,x)_0=E(\zeta(x))\) is the expected value of the model over the training data, \(\phi(\zeta,x)_j\) is the attribution of feature \(j\) for \(x\), and \(M\) is the total number of features.
Then, for each combination of feature \(j\) and observation \(x\), the Shapely value \(\phi\) represents the impact of each predictor being added, aggregated by a weighted average over all possible feature subsets \(S\subseteq S_{all}\):

\begin{equation}
\phi_{j}(x)=\sum_{S\subseteq S_{all}\setminus\left\{j\right\}}\frac{|S|!(M-|S|-1)!}{M!}\left(\zeta_{S\cup{j}}(x)-\zeta_S(x)\right).
\label{eq:shapley}
\end{equation}

The SHAP feature importance estimates offer several practical properties.
First, the sum of the feature attributions for an observation is equal to the difference between the average prediction of the model and the actual prediction for that observation (\emph{local accuracy}).
Second, if a feature is more important in one model than in another, regardless of which other features are also present, then the importance attributed to that feature should also be higher (\emph{symmetry} / \emph{monotonicity}).
Third, if a feature value is missing, the associated feature importance should be 0 (\emph{missingness}).
Several approaches have been proposed to reduce the complexity of Shapley value estimation from exponential to polynomial time, including KernelSHAP~\autocite{Lundberg:SHAP2017}, which works on any model type, and TreeShap~\autocite{Lundberg:TreeSHAP2019} for tree-based models.

\textbf{Feature Selection.}
In the context of predictive modeling, feature selection (FS) methods generally aim to reduce the number of predictor features to either (a) maximize model performance or (b) affect model performance as little as possible.
Some modeling families are sensitive to predictors that are irrelevant to the target feature, such as support vector machines~\autocite{Boser:SVM1992} and neural networks~\autocite{nnet,Goodfellow:DL2016}.
Others, such as linear and logistic regression models, are susceptible to correlated predictors.
Often domain experts require intrinsically interpretable models, which requires eliminating predictors that do not contribute substantially to model performance.

Traditionally, FS methods are broadly classified into three categories: embedded, filter, and wrapper~\autocite{Guyon:RFE2003}.
Embedded FS refers to internal mechanisms of modeling algorithms that evaluate the usefulness of features.
Examples of such algorithms include tree- and rule-based models~\autocite{Quinlan:C451993,kuhn2013applied}, regularization methods such as Least Absolute Shrinkage and Selection Operator~\autocite{lasso} (LASSO) and Ridge~\autocite{ridge} regression.
Filtering methods rank predictors only once based on some measure of importance, e.g., correlation with target feature.
Popular examples include correlation-based feature selection~\autocite{Hall:CFS2000} and Relief~\autocite{kira1992feature}.
Wrapper methods rank and refine candidate feature subsets through an iterative search driven by model performance.
Examples include sequential forward search, recursive backward elimination, and genetic search~\autocite{chandrashekar2014survey}.

\hypertarget{iml-workflow}{%
\section{Overview of the Mining Workflow}\label{iml-workflow}}

In this section, we describe the components of our mining workflow (Figure~\ref{fig:09-iml-workflow-cropped}).
We apply the workflow on three classification tasks: prediction of (i) tinnitus-related distress and (ii) depression after treatment in tinnitus patients, as well as (iii) rupture status classification in intracranial aneurysms.
We refer to these tasks as CHA-Tinnitus, CHA-Depression and AneurD, hereafter.

\textbf{TODO: COMPLETE}



For the CHA dataset, we selected patients with complete data for each of the two classification tasks. For AneurD, we segmented the aneurysms from the raw image data, performed automated centerline and neck curve extraction, and generated the morphological features. We performed correlation analysis to identify relevant correlations between predictors, correlations between predictors and response, and significant differences in correlation between predictors and response between T0 and T1. We embedded model training in an iterative feature elimination wrapper that retained predictors identified as important to the model. We selected the best overall model based on AUC and used post-hoc interpretation methods to identify predictors with the highest attribution to model prediction on a global, subpopulation and observation level.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-iml-workflow-cropped} 

}

\caption{\textbf{The mining workflow.}}\label{fig:09-iml-workflow-cropped}
\end{figure}

\hypertarget{preprocessing-of-raw-image-data}{%
\subsection{Preprocessing of Raw Image Data}\label{preprocessing-of-raw-image-data}}

For AneurD, it was necessary to extract the morphological features first.

\textbf{Segmentation and neck curve extraction.}
Aneurysms and vessels were segmented using a threshold-based approach~\autocite{Glasser2015} from digital subtraction data reconstructed from 3D rotational angiography images.
Subsequently, the centerline of the vessel was extracted using the Vascular Modeling Toolkit (VMTK, vmtk.org)~\autocite{Antiga2008}.
Subsequently, the plane separating the aneurysm from its parent vessel was determined using the automatic ostium detection of Saalfeld et al.~\autocite{Saalfeld2018}.

\textbf{Morphological feature extraction.}
For each 3D surface mesh, we obtained the neck curve, the dome point \(D\), and the two base points \(B_1\) and \(B_2\).
As described in~\autocite{Saalfeld2018}, \(B_1\) and \(B_2\) were approximated as points on the centerline with largest distance where the rays from \(B_1\) and \(B_2\) to \(D\) do not intersect the surface mesh.
Figure~\ref{fig:09-morph-parameters} illustrates the extracted parameters, where \(H_{max}\), \(W_{max}\), \(H_{ortho}\), \(W_{ortho}\), and \(D_{max}\) (see Figure~(a)) describe the aneurysm shape~\autocite{Dhar2008,LauricEtAl:Neurosurgery2012}.
The angle parameters \(\alpha\), \(\beta\), and \(\gamma\) (Figure \ref{fig:09-morph-parameters}~(b)) were extracted based on \(B_1\), \(B_2\), and \(D\), respectively.
The absolute difference between \(\alpha\) and \(\beta\) is denoted as \(\Delta_{\alpha\beta}\).
By separating the aneurysm from its parent vessel by the neck curve, we were able to derive the surface area \(A_A\) and volume \(V_A\) of the aneurysm (Figure \ref{fig:09-morph-parameters}~(c)).
We provide two variants for the surface area of the ostium, \(A_{O1}\) and \(A_{O2}\) (see Figure \ref{fig:09-morph-parameters}~(d)).
\(A_{O1}\) is the area of the ostium, i.e., the area of the triangulated ostium surface resulting from the connection of the neck curve points with their centroid \(C_{NC}\), and \(A_{O2}\) denotes the area of the neck curve when projected into a plane (cf.~\autocite{Saalfeld2018}).
Therefore, \(A_{O2}\) was extracted as a parameter comparable to other studies that often use a cutting plane to determine the ostium.
For highly lobulated aneurysms, our method achieves a local optimum and considers only one of the many dome points.
Although the estimated positions of \(B_1\) and \(B_2\) may vary slightly, neck curve detection is still performed and morphological parameters are calculated. Table~(tab:09-morphological-features) provides an overview of all extracted morphological features.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-MorphParameters5} 

}

\caption{\textbf{Illustration of the extracted morphological features.} (a) Features that describe aneurysm width, height, and diameter. (b) The angles \(\alpha\), \(\beta\) and \(\gamma\) are extracted from the base points \(B_1\), \(B_2\) and the dome point \(D\). (c) After separating the aneurysm from its parent vessel via the neck curve, the area \(A_A\) and volume \(V_A\) are computed. (d) The area of the ostium \(A_{O1}\) and the area of the projected ostium \(A_{O2}\) are extracted after estimating the center of the neck curve \(C_{NC}\).}\label{fig:09-morph-parameters}
\end{figure}



\begin{table}

\caption{\label{tab:09-morphological-features}\textbf{Overview of morphological features extracted for AneurD}.}
\centering
\begin{tabular}[t]{rl>{\raggedright\arraybackslash}p{12cm}}
\toprule
\textbf{\#} & \textbf{Feature} & \textbf{Description}\\
\midrule
1 & $A_A$ & area of the aneurysm (without the ostium) [mm$^2$]\\
2 & $V_A$ & volume of the aneurysm [mm$^3$]\\
3 & $A_{O1}$ & area of the ostium (variant 1) [mm$^2$]\\
4 & $A_{O2}$ & area of the ostium (variant 2) [mm$^2$]\\
5 & $D_{max}$ & max. diameter of the aneurysm [mm]\\
6 & $H_{max}$ & max. height of the aneurysm [mm]\\
7 & $W_{max}$ & max. width of the aneurysm perpendicular to $H_{max}$ [mm]\\
8 & $H_{ortho}$ & height of the aneurysm approximated as length of the ray perpendicular to the ostium plane starting from $C_{NC}$ [mm]\\
9 & $W_{ortho}$ & max. width parallel to the projected ostium plane [mm]\\
10 & $N_{max}$ & max. $NC$ diameter, i.e., the max. possible distance between two $NC$ points [mm]\\
11 & $N_{avg}$ & average $NC$ diameter, i.e., the mean distance between $C_{NC}$ and the $NC$ points [mm]\\
12 & $AR_1$ & aspect ratio (variant 1): $H_{ortho}/N_{max}$\\
13 & $AR_2$ & aspect ratio (variant 2): $H_{ortho}/N_{avg}$\\
14 & $V_{CH}$ & volume of the convex hull of the aneurysm vertices [mm$^3$]\\
15 & $A_{CH}$ & area of the convex hull of the aneurysm vertices [mm$^2$]\\
16 & $EI$ & ellipticity index $EI=1-\left(18\pi\right)^{\frac{1}{3}}V_{CH}^{\frac{2}{3}}/A_{CH}$\\
17 & $NSI$ & non-sphericity index, i.e.,  $NSI=1-\left(18\pi\right)^{\frac{1}{3}}V^{\frac{2}{3}}/A$\\
18 & $UI$ & undulation index. $UI=1-\frac{V}{V_{CH}}$\\
19 & $\alpha$ & min. of $\measuredangle DB_1B_2$ and $\measuredangle DB_2B_1$ [deg]\\
20 & $\beta$ & max. of $\measuredangle DB_1B_2$ and $\measuredangle DB_2B_1$ [deg]\\
21 & $\gamma$ & angle at $D$, i.e., $\measuredangle B_1DB_2$ $[deg]$\\
22 & $\Delta_{\alpha\beta}$ & abs. difference between $\alpha$ and $\beta$ [deg]\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{correlational-analysis}{%
\subsection{Correlational Analysis}\label{correlational-analysis}}

For CHA tinnitus, we calculate pairwise Spearman correlation between the predictors as part of exploratory data analysis.
Using agglomerative hierarchical clustering with complete linkage, we arrange predictors in a correlation heat map so that potential subgroups of predictors with similar intra-group correlations and similar inter-group correlations can be visually identified.
Furthermore, we calculate the median correlation between the response and the predictors from the same questionnaire at T0 and T1 to obtain potential candidate predictors important in the later modeling step.
In addition, we identify predictors with the highest absolute correlation with respect to the response at T0 and T1, respectively.
Finally, we examined predictors whose correlation with the TQ distress score differed most between T0 and T1.

\hypertarget{classification-algorithms}{%
\subsection{Classification Algorithms}\label{classification-algorithms}}

In order not to be limited to a particular classification algorithm, but to create a model with the highest possible predictive power, we examined a total of eleven classifiers:

\begin{itemize}
\tightlist
\item
  Least absolute shrinkage and selection operator~\autocite{lasso} (\emph{LASSO}) and \emph{Ridge}~\autocite{ridge} are extensions of ordinary least squares (OLS) regression that perform feature selection and regularization to improve both predictive performance and interpretability.
  For a dataset with \(n\) observations, \(p\) predictor features and a target \(y\), the objective of LASSO and Ridge is to solve
  \begin{equation}
  \underset{\beta}{\text{argmin}} \underbrace{\sum_{i=1}^n \left( y_i - \left( \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right) \right)^2}_{\text{Residual Sum of Squares}} + \alpha \lambda \underbrace{\sum_{j=1}^p |\beta_j|}_{\text{L1 Penalty}} + (1-\alpha) \lambda \underbrace{\sum_{j=1}^p \beta_j^2}_{\text{L2 Penalty}}
  \label{eq:regression-with-shrinkage}
  \end{equation}
  where \(\beta\) are the to be determined model coefficients, and \(\lambda\) is a tuning hyperparameter that controls the amount of regularization.
  LASSO uses the L1 norm penalty term, i.e., \(\alpha\) = 1, which shrinks the absolute values of the coefficients, often forcing some of them to be exactly equal to 0.
  Ridge uses the L2 norm penalty term, i.e., \(\alpha = 0\), which shrinks the coefficient magnitudes.
  In general, LASSO performs better than Ridge when there is a relatively small number of predictors with substantial coefficients and the remaining predictors have coefficients that are close or equal to zero.
  Ridge performs better in settings where the response depends on many predictors, each of them with approximately equal importance.
  From the perspective of interpretability, LASSO has the advantage of producing sparser models by reducing the values of some of the predictors' coefficients to exactly zero.
\item
  Partial least squares is another derivative of OLS regression which first performs a projection to extract latent variables which capture as much of the variability among the predictors as possible while modeling the response well.
  A linear regression is then fit on a preferably small number of latent features from this projection.
  We use the generalized partial least squares (\emph{GPLS}) implementation from Ding and Gentleman~\autocite{DingEtAl:GPLS2005}.
\item
  A support vector machine (\emph{SVM})~\autocite{Boser:SVM1992} learns linear or nonlinear decision boundaries in the feature space to separate the classes.
  The decision boundary is represented by the training observations that are most difficult to classify, i.e., the \emph{support vectors}.
  The goal is to find the \emph{maximum margin hyperplane} which is the separating hyperplane with the maximum margin to the support vectors.
  In case a linear decision boundary does not exist, nonlinear SVM approaches can be used which apply the so called \emph{kernel trick} to transform the original feature space into a new, higher-dimensional space in which a linear hyperplane can be found to separate the classes.
\item
  An artifical neural network (\emph{NNET}) consists of a structure of nodes that are connected with each other by directed edges.
  Each node performs a basic unit of computation.
  Nodes are supplied by data values that are passed over via incoming edges from other nodes.
  Each edge holds a weight that controls the impact on the node it forwards values to.
  The main goal of a NNET is to adjust the weights of the edges such that the relationship between predictors and response in the underlying data is represented.
  Neural networks extract new useful features from the original predictors that are relevant for classification.
  By combining interconnected nodes to complex predictive features, NNETs are capable of extracting more classification-relevant feature sets compared to expert-driven feature engineering or by dimension reduction techniques.
  NNETs have undergone widespread adoption in the last decade and led to various success stories in computer vision and natural language processing~\autocite{Goodfellow:DL2016}. We used a feed-forward NNET with one intermediary layer (\emph{hidden unit})~\autocite{VenablesAndRipley:NNET2002}.
\item
  Weighted k-nearest neighbor~\autocite{Hechenbichler:wknn2004} (\emph{WKNN}) is a variant of KNN classification.
  To classify an observation with unknown response value, the k \emph{nearest} training observations are identified and the modus of their response values are taken as prediction.
  Proximity between observations is quantified by a distance measure such as Euclidean distance.
  Whereas in ordinary KNN all neighbors have equal influence on the prediction, weighted KNN takes into account the actual distance magnitudes.
  As a result, WKNN assigns weights to training observations that are inversely proportional to their distance from the observation being classified.
\item
  A Naïve Bayes classifier (\emph{NB}) uses Bayes' theorem to calculate class membership probabilities.
  The naive property refers to the assumption of class-conditional independence among the predictors, which is employed to reduce computational complexity and to obtain more reliable class-conditional probability estimates.
\item
  Classification and regression trees~\autocite{BreimanEtAl:CART1984} (CART),
  C5.0~\autocite{Quinlan:C451993},
  random forests~\autocite{Breiman:RandomForests2001} (RF) and
  gradient boosted trees (GBT)~\autocite{Friedman:PDP2001} are tree-based models.
  Algorithm from this model family partition the predictor space into a set of non-overlapping hyperrectangles based on combinations of predictor-value conditions, such as ``IF age \(>\) 52 \& body-mass index \(<\) 25''.
  A new observation is classified based on the majority class of training data associated with the hyperrectangle to which it belongs.
  Random forests and gradient boosted trees are ensembles of several different decision trees, with each tree casting a vote for the final prediction.
  In a random forest, the base trees are created independently.
  In a a gradient boosted model, the base trees are constructed and added to the composite model in a way that any new tree reduces the error of the current set of trees.
\end{itemize}

\hypertarget{classifier-evaluation-and-hyperparameter-tuning}{%
\subsection{Classifier Evaluation and Hyperparameter Tuning}\label{classifier-evaluation-and-hyperparameter-tuning}}

We use 10-fold stratified cross-validation (CV) for classifier evaluation.
In k-fold CV, the observations are split into k disjunct partitions.
Each partition serves once as test set for a model trained on the remainder of the partitions.
The k performance estimates are aggregated to obtain an overall performance score.
We performed a grid search for hyperparameter selection (cf.~Table~\ref{tab:09-hyper-tab}).
Because the three applications have dichotomous responses with different skew, accuracy might be inappropriate to estimate generalization performance.
Instead, we used the area under the receiver operating characteristic curve (AUC) as performance measure.
A receiver operating characteristic curve (ROC) shows the relationship between sensitivity (true positive rate (TPR)) and false positive rate (FPR) for a binary classifier.
The area under the ROC curve (AUC) takes values from 0 (0\% TPR, 100\% FPR) to 1 (100\% TPR, 0\%FPR).
A higher AUC suggests that the classifier is better at separating the classes.



\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tab:09-hyper-tab}\textbf{Overview of hyperparameter tuning grid.} All classifiers were implemented with the statistical programming language R \autocite{rlanguage} using the package \texttt{mlr} \autocite{Bischl:mlr2016}, which provides a uniform interface to the listed machine learning algorithms from other R packages. A grid search was used to tune the hyperparameters using area under the ROC curve (AUC) as the evaluation measure. The table provides an overview of each classifier, including the R package used, the tuned hyperparameters and their value ranges. All other hyperparameters were set to default values. * = \{linear, polynomial, radial, sigmoid\}}\tabularnewline
\toprule
\textbf{Algorithm (R package)} & \textbf{Parameter} & \textbf{Min.} & \textbf{Max.} & \textbf{No.~of values}\tabularnewline
\midrule
\endfirsthead
\toprule
\textbf{Algorithm (R package)} & \textbf{Parameter} & \textbf{Min.} & \textbf{Max.} & \textbf{No.~of values}\tabularnewline
\midrule
\endhead
LASSO, Ridge (\texttt{glmnet} \autocite{lasso}) & \texttt{lambda} & 10\textsuperscript{-2} & 10\textsuperscript{10} & 100\tabularnewline
GPLS (\texttt{caret} \autocite{caret}) & \texttt{ncomp} & 1 & 5 & 5\tabularnewline
SVM (\texttt{e1071} \autocite{e1071}) & \texttt{cost} & 0.01 & 3 & 6\tabularnewline
& \texttt{gamma} & 0 & 3 & 4\tabularnewline
& \texttt{kernel} & -- & -- & 4*\tabularnewline
NNET (\texttt{nnet} \autocite{nnet}) & \texttt{size} & 1 & 13 & 7\tabularnewline
& \texttt{decay} & 10\textsuperscript{-4} & 1 & 6\tabularnewline
WKNN (\texttt{kknn} \autocite{Hechenbichler:wknn2004}) & \texttt{k} & 1 & 77 & 20\tabularnewline
NB (\texttt{e1071} \autocite{e1071}) & \texttt{laplace} & 1 & 5 & 5\tabularnewline
CART (\texttt{rpart} \autocite{rpart}) & \texttt{cp} & 0.001 & 0.1 & 5\tabularnewline
C5.0 (\texttt{C50} \autocite{c50}) & \texttt{CF} & 0 & 0.35 & 7\tabularnewline
& \texttt{winnow} & \texttt{FALSE} & \texttt{TRUE} & 2\tabularnewline
& \texttt{rules} & \texttt{FALSE} & \texttt{TRUE} & 2\tabularnewline
RF (\texttt{ranger} \autocite{ranger}) & \texttt{mtry} & 4 & 100 & 7\tabularnewline
& \texttt{min.node.size} & 1 & 25 & 6\tabularnewline
GBT (\texttt{xgboost} \autocite{xgboost}) & \texttt{eta} & 0.01 & 0.4 & 4\tabularnewline
& \texttt{max\_depth} & 1 & 3 & 3\tabularnewline
& \texttt{colsample\_bytree} & 0.2 & 1 & 5\tabularnewline
& \texttt{min\_child\_weight} & 0.5 & 2 & 3\tabularnewline
& \texttt{subsample} & 0.2 & 1 & 3\tabularnewline
& \texttt{nrounds} & 50 & 250 & 3\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{iterative-feature-elimination}{%
\subsection{Iterative Feature Elimination}\label{iterative-feature-elimination}}

We developed a feature selection wrapper that successively eliminates a subset of predictors that do not \emph{positively} contribute to the performance of a model.
The contribution of a predictor is computed using \emph{model reliance}~\autocite{Fisher:ModelReliance2018}, which is a generalization of random forest permutation feature importance~\autocite{Breiman:RandomForests2001}.
Model reliance estimates the merit of a predictor \(f\) toward a model \(\zeta\) by comparing the classification error of \(\zeta\) on the original training set \(\mathbf{X}_{orig}\) with the classification error of \(\zeta\) on a modified version of the training set \(\mathbf{X}_{perm}\) where the values of \(f\) are randomly permuted.
In particular, the model reliance \(MR\) of a model \(\zeta\) on a predictor \(f\in F\) is calculated as

\begin{equation}
MR(f,\zeta) = \frac{CE(y,\zeta(\mathbf{X}_{perm}))}{CE(y,\zeta(\mathbf{X}_{orig}))}
\label{eq:model-reliance}
\end{equation}

where \(CE\) is the classification error function that takes the true class labels \(y\) and a vector of predicted class labels, and returns the fraction of incorrectly classified observations.
A high MR score represents a high dependence of the model on \(f\), since shuffling the values of \(f\) increases the classification error.
Conversely, a \(MR\) score smaller than 1 suggests that \(f\) is potentially adversarial to model performance, and its removal could increase model performance. Thus, our feature elimination wrapper starts by training a model on the full set of predictors, followed by an iterative step where the subset of adversarial predictors according to model reliance is removed, and a new model on the remaining predictors is trained.
In the first iteration \(i=1\), an initial model \(\zeta_1\) is calculated on the full set of predictors \(F_1 = F\).
For each predictor \(f \in F_i\), the model reliance \(MR(f,\zeta_i)\) is calculated.
Predictors with \(f\in F_i:MR(f,\zeta_i)>1\) are kept for iteration \(i+1\) while the remaining predictors are removed.
This procedure is repeated either until all \(MR\) are smaller or equal to 1, i.e., \(\forall f \in F_i: MR(f,\zeta_i) \leq 1\), or \(F_{i+1}=F_i\).
As random feature permutation introduces some statistical variability, we compute mean \(MR\) over 10 runs to obtain a more stable estimate.

\hypertarget{post-hoc-interpretation}{%
\subsection{Post-Hoc Interpretation}\label{post-hoc-interpretation}}

\textbf{SHAP.} To facilitate model interpretation, the model-agnostic post-hoc framework SHAP~\autocite{Lundberg:SHAP2017,Lundberg:TreeSHAP2019} was used to assess feature importance for the CHA data.
Briefly, the SHAP value \(\phi_f(\zeta,x)\) expresses the estimated importance of a feature \(f\) to the prediction of model \(\zeta\) for an instance \(x\) as change in the expected value of the prediction if for \(f\) the feature vector of \(x\) is observed instead of being random.
The SHAP framework composes the model prediction as sum of SHAP values of each feature, i.e., \(\zeta(x)=\phi_0(\zeta,x)+\sum_{i=1}^M \phi_i(\zeta,x)\), where \(\phi_0(\zeta,x)\) is the expected value of the model (bias) and \(M\) is the number of features.

SHAP values were calculated for the best model \(\zeta_{opt}\) according to AUC.
A ranking of T0 feature attribution towards \(\zeta_{opt}\) was determined by calculating the average SHAP value magnitude over all instances, i.e., \(A(j)=\sum_{i=1}^N |\phi_j(\zeta_{opt},x)|\),
where \(A(j)\) is the attribution of the \(j\)-th feature.
The \(N\times M\) SHAP matrix was clustered with agglomerative hierarchical clustering to identify subgroups of patients with similar SHAP values.

\textbf{PDP feature importance.} We derive a \emph{global} feature importance from the PD of a predictor.
Our assumption is that predictors with high PD variability are more important.
For example, consider the two PD curves in Figure \ref{fig:09-pd-intro}~(c): the predicted response changes considerably with different predictor values for blue PD curve whereas the green PD curve is basically a flat line.
Therefore, the predictor with the blue PD curve should have a higher importance score than the predictor with the green PD curve.
We define partial dependence importance \(I\) of a predictor \(f\) as average of the magnitude of differences between consecutive values along the distribution of \(f\), i.e.,

\begin{equation}
I_f = \frac{1}{k-1}\sum_{i}^{k-1} |PD(Q=s_i) - PD(Q=s_{i+1})|
\label{eq:pdp-imp}
\end{equation}

where \(k\) is the number of (sampled) values from the distribution of \(f\).



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/09-pd-intro-2} 

}

\caption{\textbf{Illustration of partial dependence importance.} Partial dependence importance \(I_f\) for 2 toy PD curves.}\label{fig:09-pd-intro-2}
\end{figure}

\hypertarget{iml-results}{%
\section{Results}\label{iml-results}}

In this section, we report our results on all three classification tasks, i.e., regarding CHA-Tinnitus (Section~\ref{iml-results-tinnitus}),
CHA-Depression (Section~\ref{iml-results-depression}) and
AneurD (Section~\ref{iml-results-aneur}).

\hypertarget{iml-results-tinnitus}{%
\subsection{Results for CHA-Tinnitus}\label{iml-results-tinnitus}}

\textbf{Correlational analysis.} Figure \ref{fig:09-cor}~(a) shows all pairwise correlations among the predictors in T0.
We identified two major subgroups with moderate to high intra-group correlations and low or negative inter-group correlations.
The larger group (cf.~upper black square in Figure \ref{fig:09-cor}~(a)) comprises 114 predictors (ca. 55.6\%) representing negatively worded items and scores where higher values represent a higher disease burden, e.g.~the ADSL\_depression and BI\_overallcomplaints.
Consequently, the smaller group (cf.~lower black square in Figure \ref{fig:09-cor}~(a)) contains 47 predictors (ca. 22.9\%) with positive wording, e.g.~the SF8 mental health score (SF8\_mental) and the BSF elevated mood score (BSF\_mood).
Predictors of one of the two subgroups exhibit a moderate to high negative correlation with the predictors of the other subgroup.
Figure \ref{fig:09-cor}~(b) compares the correlation of the predictors with TQ\_distress before (x-axis) and and after treatment (y-axis).
Overall, only low to moderate bivariate correlations were observed, as all values are between -0.6 and +0.6.
The average change in absolute correlation between T0 and T1 is 0.031.
The change in absolute correlation is smaller than 0.067 for ca. 95\% of the predictors (compare distance of the points to the diagonal line in Figure \ref{fig:09-cor}~(b)).
For 137 out of 205 predictors (66.8\%), absolute correlation decreased from T0 to T1.
Median target-correlation of the questionnaires ADSL, BSF and BI (SF8) are greater (smaller) than +0.3 (-0.3) at both moments, respectively, and thus greater than for the remaining questionnaires.
Figures \ref{fig:09-cor}~(c) and (d) reveal that predictors from ADSL, BSF, BI, SF8, TINSKAL and PSQ are among the top-20 predictors ranked by absolute correlation with TQ\_distress in T0 and T1.
The general depression score ADSL\_depression shows largest correlation magnitude before (\(\rho\) = 0.630) and after treatment (\(\rho\) = 0.564).
Figure \ref{fig:09-cor}~(e) shows the 10 predictors with the largest differences in correlation magnitudes between T0 and T1.
Correlation before treatment is larger for each of these predictors.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-cor} 

}

\caption{\textbf{Spearman correlation among predictors and correlation of predictors with TQ\_distress in T0 and T1.} (a) The heatmap depicts the correlation coefficients for all pairs of predictors in T0. Predictors are arranged by the result of agglomerative hierarchical clustering with complete linkage. The two black squares depict two major subgroups of correlated predictors. (b) The relationship between each predictor with TQ\_distress in T0 (x-axis) and in T1 (y-axis). The diamond symbol represents the median correlation of the predictors from the same questionnaire. (c) Top-20 predictors which exhibit highest absolute correlation with TQ\_distress in T0. (d) Top-20 predictors which exhibit highest absolute correlation with TQ\_distress in T1. (e) Top-10 predictors with the highest change in absolute correlation with TQ\_distress from T0 to T1.}\label{fig:09-cor}
\end{figure}

\textbf{Predictive performance of classification models.} The performances of all 11 classifiers across each feature elimination iterations are shown in Figure~\ref{fig:09-results-pone}.
The gradient boosted trees model (GBT) yields highest AUC (iteration i = 7, AUC = 0.890 \(\pm\) 0.04; mean\(\pm\)SD), using only 26 predictors (ca. 13\%).
The RIDGE classifier achievs second-best performance (i=2, AUC: 0.876 \(\pm\) 0.05), relying on 127 features, followed by the random forest model (i=3, AUC: 0.872 \(\pm\) 0.05) using 77 features.
Classification using the best model (GBT, i=7) based on a probability threshold of 0.5 resulted in an accuracy of 0.86, a true positive rate (sensitivity) of 0.72, a true negative rate (specificity) of 0.88, a precision of 0.48 and a negative predictive value of 0.95.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-results-pone} 

}

\caption{\textbf{Classification results for CHA-Tinnitus}. Average cross-validation AUC and relative number of retained predictors for each classifier with optimal hyperparameter configuration and for each feature selection iteration. Yellow ribbons depict standard deviation. Points highlight each classifier's run with maximum AUC. Classifiers are ordered by their maximum AUC from left to right.}\label{fig:09-results-pone}
\end{figure}

When trained using a smaller feature space, each classifier produce at least one model with similar or even improved performance compared to the respective model learned on the whole set of predictors.
In fact, with the exception of WKNN, all classification methods benefit from feature elimination as they produce their best model on a predictor subset (cf.~Figure~\ref{fig:09-results-pone}).
For GBT, the gain in AUC from 185 features to 26 features (i = 11) is 0.01.
This model achieves both maximum AUC and a good trade-off between high predictive performance and low model complexity, and we thus decided to further investigate this model.

\textbf{Feature importance.} For the best model, the attributions of the 26 selected features are shown in Figure \ref{fig:09-tq-xgboost}~(a).
Among the 26 features are 6 scores, 12 single items, 4 demographic features (number of visited doctors, university-level education, lower secondary education, tinnitus duration) and 4 features measuring the average time spent completing an item.
The TINSKAL tinnitus impairment score (TINSKAL\_impairment) represents the predictor with highest model attribution as it exhibits the highest average absolute SHAP value (change in log odds) of 0.448.
The ADSL depressivity score (ADSL\_depression) and a single question from ADSL (ADSL\_adsl11: \emph{``During the past week my sleep was restless.''}) are ranked second and third most important, respectively.
Remarkably, from each of the 9 questionnaires at least 1 feature was selected.
Figure \ref{fig:09-tq-xgboost}~(b) shows the patient-individual SHAP values for each predictor where point color depicts predictor value magnitude.
The high attribution of TINSKAL\_impairment is highlighted by the wide spread in the SHAP value distribution.
For this predictor, high values generally correspond to an increased predicted probability of tinnitus decompensation.
However, this trend is non-linear, since small values (light green to yellow) are associated with a SHAP value just slightly smaller than or equal to 0.
Moreover, there is a large spread in SHAP value between ca. 0.7 and 1.2 for patients with high TINSKAL\_impairment values opposed to the somewhat more dense bulk points representing patients with SHAP values between ca. -0.7 and -0.4.
This could indicate that patients who report high tinnitus impairment are more difficult to classify.
Further, it may suggest that visual analog scales are not robust enough to quantify tinnitus-related distress.
This inference is supported by the SHAP feature dependence plot in Figure \ref{fig:09-tq-xgboost-shap-per-feature}~(1) which juxtaposes the actual values of the predictor with the corresponding SHAP values for all patients and reveals a J-shaped relationship between them.
More specifically, the predicted tinnitus-related distress is decreasing from 0 to 2.5, remains at a plateau from 2.5 to 4 and is increasing from 4 to its maximum value 10.
Besides TINSKAL\_impairment, the features ADSL\_depression, TINSKAL\_loudness, BI\_overallcomplaints, BSF\_timestamp and SWOP\_pessimism also showed a non-linear relationship with respect to their SHAP values.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-tq-xgboost} 

}

\caption{\textbf{SHAP analysis results for the best model (GBT, feature elimination iteration i = 7)}. (a) Global feature importance based on the mean absolute SHAP magnitude over all observations. Values depict absolute change in log odds where higher values indicate higher feature attribution towards the model. (b) Patient-individual SHAP values. Points represents the SHAP value of the predictor (y-axis) for an individual patient. The further afar a point from the vertical 0-baseline, the larger the attribution of the corresponding predictor value to the model prediction. Vertically offset points depict regions of high density (similar to a violin plot), i.e., there is a greater number of patients with similar SHAP values. Actual predictor values are mapped to point color. (c) Stacked patient-individual SHAP values for the 6 predictors with highest mean absolute SHAP values. Patients are ordered according to hierarchical clustering with Ward linkage. Black horizontal lines depict the average sum of SHAP values of the cluster members for k = 5 clusters. The inset plot shows that Bayesian information criterion is minimal for this number of clusters.}\label{fig:09-tq-xgboost}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-tq-xgboost-shap-per-feature} 

}

\caption{\textbf{SHAP feature dependence.} The relationship between the actual values of a predictor (x-axis) and corresponding SHAP values (y-axis) is shown as points representing a patient and as locally weighted scatterplot smoothing (LOWESS) curves indicating the overall trend. Predictors are ordered by mean absolute SHAP value (see Figure\ref{fig:09-tq-xgboost}~(a)). Gray histograms and bar charts depict the distributions of the predictors.}\label{fig:09-tq-xgboost-shap-per-feature}
\end{figure}

Even though several predictors exhibit only a low or moderate global importance, some of them have a high attribution towards model prediction for specific subgroups.
For example, considering SOZK\_lowersec, patients with lower secondary education have an average SHAP value of +0.5, whereas patient with different education levels have an average SHAP value of -0.1 and hence are more close to the population average (cf.~Figure \ref{fig:09-tq-xgboost}~(b), Figure \ref{fig:09-tq-xgboost-shap-per-feature}~(13)).
Most features show a monotonic relationship between actual values and SHAP values.
For example, increasing values of the SF8 physical component score (SF8\_physicalcomp) exhibit decreasing likelihood of predicted decompensated tinnitus with increasing physical health (cf.~Figure \ref{fig:09-tq-xgboost}~(b), Figure \ref{fig:09-tq-xgboost-shap-per-feature}~(14)).

To investigate whether there are subgroups of patients which are similar with respect to how the model prediction can be explained by them, we clustered the patients based on the SHAP values.
Figure \ref{fig:09-tq-xgboost}~(c) shows stacked patient-individual SHAP values for the six predictors with highest average absolute SHAP values and the remaining predictors combined.
According to Bayesian information criterion (cf.~insetted plot in Figure \ref{fig:09-tq-xgboost}~(c)), the optimal number of patients clusters with similar SHAP value patterns is 5.
Clusters 1 and 5 comprise subgroups where the sum of SHAP values over all predictors is positive, see the horizontal lines in Figure \ref{fig:09-tq-xgboost}~(c).
Hence, these patients are more likely to be predicted with decompensated tinnitus.
In comparison with the other subgroups, patients of clusters 1 and 5 reported higher degrees of tinnitus impairment, depressivity, anxiety, tinnitus loudness, sleeplessness, pessimism, psychosomatic complaints as well as perceived levels of stress and social isolation.
In general, patients of cluster 1 have slightly higher values across all predictors than patients of cluster 2.
In addition, cluster 1 contains a higher fraction of patients with lower secondary education (``Hauptschule''), report more frequently occuring headaches, higher levels of fears for the future and a longer tinnitus duration.
Cluster 3 is the largest subgroup comprising 39.6\% of all patients.
Together with cluster 2, these subgroups have the lowest predicted probability of tinnitus decomposition.
Patients of cluster 2 and 3 report highest physical health and levels of determination.
Cluster 4 is somewhat close to the prediction average where positive and negative SHAP values nearly even out.
With respect to average patient-sum of SHAP values, cluster 3 lies in between cluster 2 and cluster 4.

\hypertarget{iml-results-depression}{%
\subsection{Results for CHA-Depression}\label{iml-results-depression}}

\textbf{Predictive performance of classification models.}
Figure~\ref{fig:09-results-srep} depicts the performance of all classification methods across iterations.
The LASSO classifier achieved maximum AUC over all classification algorithms (iteration i = 1, AUC = 0.867 \(\pm\) 0.037; mean \(\pm\) SD), followed by Ridge (i = 1, AUC = 0.864 \(\pm\) 0.040) and GBT (i = 1, AUC = 0.862 \(\pm\) 0.038).
When considering only the best model per classifier, the models are similar in performance, ranging in AUC from 0.809 (C5.0) to 0.867 (LASSO).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-results-srep} 

}

\caption{\textbf{Classification results for CHA-ADSL\_depression.} Average cross-validation AUC and relative number of retained predictors for each classifier with optimal hyperparameter configuration and for each feature selection iteration. Yellow ribbons depict standard deviation. Points highlight each classifier's run with maximum AUC. Classifiers are ordered by their maximum AUC from left to right.}\label{fig:09-results-srep}
\end{figure}

The best model (Lasso, i = 1) achieves an accuracy of 79\%, a true positive rate (sensitivity) of 61\%, a true negative rate (specificity) of 88\%, a precision of 72\% and a negative predictive value of 82\% based on a probability threshold of 0.5.
This final model includes 40 predictors with non-zero coefficients.
Figure~\ref{fig:09-lasso-depression} shows the median model coefficient for these features across 10 cross-validation folds.
From the ADSL questionnaire alone, 16 single items were included in the final model, including indicators of depressivity (ADSL\_adsl09, ADSL\_adsl18, ADSL\_adsl12) perceived antipathy received from other people (ADSL\_adsl19), sleeplessness (ADSL\_adsl11), dejectedness (ADSL\_adsl03), lack of appetite (ADSL\_adsl02), confusion (ADSL\_adsl05), anxiety (ADSL\_adsl10, ADSL\_adsl08), absence of self-respect (ADSL\_adsl04, ADSL\_adsl09), lack of vitality (ADSL\_adsl09, ADSL\_adsl09), taciturnity (ADSL\_adsl13) and irritability (ADSL\_adsl01).
Thus, this questionnaire contributed the highest number of predictors to the model. From the tinnitus-distress-oriented TQ, 5 predictors were selected.
Further, the model used 5 predictors from the socio-demographics questionnaire (SOZK), including German nationality (SOZK\_nationality) which has the highest absolute model coefficient, university level graduation (SOZK\_graduate), tinnitus duration (SOZK\_tinnitusdur), employment status (SOZK\_job), marital status (SOZK\_unmarried) and partnership status (SOZK\_partnership).



\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{figures/09-lasso-depression} 

}

\caption{\textbf{Coefficients of LASSO model.} Cross-validation (CV) median (points) \(\pm\) median absolute deviation (line ranges) of coefficients for the best LASSO model (i = 1). The frequency of non-zero coefficients in 10-fold CV is given in parentheses right to the predictor name. From 185 features in total, 40 features exhibit a non-zero model coefficient for at least one CV fold.}\label{fig:09-lasso-depression}
\end{figure}

Table~\ref{tab:09-lasso-depression-tab} provides a description for each of the predictors from Figure~\ref{fig:09-lasso-depression}.

\textbf{Effect of feature elimination of classification performance.} All classifiers but SVM show high stability in performance on smaller feature subsets.
From Figure~\ref{fig:09-results-srep}, we see that for LASSO the difference in AUC when trained on 185 features (i = 1) vs.~when trained on 6 features (i = 7) is only -0.017.
Several classifiers benefited from feature selection in terms of predictive performance.
For GPLS, NNET, CART, C5.0 and RF, max. AUC is achieved on a feature subset.
Both decision tree variants CART and C5.0 gain most in performance from feature removal, since their respective max. AUC is obtained on the smallest predictor subset, with a cardinality of 22 and 10, respectively.



\begin{table}

\caption{\label{tab:09-lasso-depression-tab}\textbf{Most important features of LASSO model.} Predictors with highest absolute coefficient in the final LASSO model (iteration i = 1). From 185 predictors in total, these 40 predictors exhibit a non-zero model coefficient in at least one out of ten cross-validation folds.}
\centering
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10cm}r}
\toprule
\textbf{Feature} & \textbf{Description} & \textbf{Coef.}\\
\midrule
SOZK\_nationality & German nationality & -0.370\\
ADSL\_adsl06 & "During the past week I felt depressed." & 0.309\\
ADSL\_adsl19 & "During the past week I felt that people disliked me." & 0.288\\
PSQ\_stress21 & "You enjoy yourself." & -0.284\\
SOZK\_graduate & Education level: university & -0.210\\
ADSL\_adsl11 & "During the past week my sleep was restless." & 0.196\\
ADSL\_adsl03 & "During the past week I felt that I could not shake off the blues even with help from my family or friends." & 0.175\\
TQ\_tin50 & "Because of the noises I am unable to enjoy the radio or television." & 0.151\\
TQ\_tin47 & "I am a victim of my noises." & 0.137\\
ADSL\_adsl02 & "During the past week I did not feel like eating; my appetite was poor." & 0.132\\
ADSL\_adsl05 & "During the past week I had trouble keeping my mind on what I was doing." & 0.132\\
SF8\_sf07 & "During the past 4 weeks, how much have you been bothered by emotional problems (...) ?" & 0.125\\
ADSL\_adsl10 & "During the past week I felt fearful." & 0.107\\
ADSL\_adsl04 & "During the past week I felt I was just as good as other people." & -0.107\\
TQ\_tin40 & "I am able to forget about the noises when I am doing something interesting." & -0.104\\
ADSL\_adsl16 & "During the past week I enjoyed life." & -0.085\\
PSQ\_stress15 & "Your problems seem to be piling up." & 0.081\\
TQ\_tin07 & "Most of the time the noises are fairly quiet." & -0.069\\
ADSL\_adsl08 & "During the past week I felt hopeful about the future." & -0.064\\
SF8\_sf02 & "During the past 4 weeks, how much did physical health problems limit your physical activities (such as walking or climbing stairs)?" & 0.059\\
SOZK\_tinnitusdur & "How long have you been suffering from tinnitus (in years)?" & 0.058\\
PSQ\_stress28 & "You feel loaded down with responsibility." & 0.055\\
ADSL\_adsl18 & "During the past week I felt sad." & 0.053\\
SOZK\_job & Job status: currently employed & -0.050\\
ADSL\_adsl13 & "During the past week I talked less than usual." & 0.049\\
TQ\_tin49 & "The noises are one of those problems in life you have to live with." & -0.048\\
ADSL\_adsl12 & "During the past week I was happy." & -0.046\\
SF8\_sf05 & "During the past 4 weeks, how much energy did you have?" & 0.040\\
SOZK\_unmarried & Unmarried & 0.033\\
SOZK\_partnership & In partnership & -0.032\\
TQ\_tin41 & "Because of the noises life seems to be getting on top of me." & 0.031\\
PSQ\_stress05 & "You feel lonely or isolated." & 0.025\\
ADSL\_adsl01 & "During the past week I was bothered by things that usually don't bother me." & 0.017\\
TQ\_tin51 & "The noises sometimes produce a bad headache." & 0.015\\
TLQ\_02\_whistling & Tinnitus noise: whistling & 0.010\\
ADSL\_adsl07 & "During the past week I felt that everything I did was an effort." & 0.010\\
ADSL\_adsl09 & "During the past week I thought my life had been a failure." & 0.005\\
SF8\_overallhealth & Overall health score & -0.003\\
PSQ\_stress18 & "You have many worries." & 0.001\\
TQ\_distress & Total tinnitus distress score & 0.001\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{iml-results-aneur}{%
\subsection{Results for AneurD}\label{iml-results-aneur}}

Figure~\ref{fig:09-results-cbms} shows the classification results on each data subset.
GBT achieves maximum AUC on \emph{ALL} (cross-validation average 67.2\% \(\pm\) 1.8\% standard deviation), followed by C5.0 (AUC 64.6\% \(\pm\) 1.9\%) and GPLS (AUC 63.3\% \(\pm\) 1.2\%).
On the subset \emph{SW}, SVM comes up best with 75.2\% \(\pm\) 5.7\% AUC, slightly superior to GPLS (AUC 73.6\% \(\pm\) 4.4\%) and NNET (AUC 71.6\% \(\pm\) 5.5\%).
For \emph{BF}, WKNN yields the best (AUC 64.0\% \(\pm\) 1.1\%) model, while GPLS (AUC 62.9\% \(\pm\) 2.6\%) and RF (AUC 62.7\% \(\pm\) 2.3\%) have similar yet slightly inferior generalization performances.
Our results indicate that all classifiers yield better performance on the subset of sidewall aneurysms.
Overall, none of the classification algorithms outperforms all others across all three subsets.



\begin{figure}

{\centering \includegraphics[width=0.67\linewidth]{figures/09-results-cbms} 

}

\caption{\textbf{Classification results for AneurD.} For each combination of data subset and classification algorithm, the performance of the run with the preprocessing transformation that achieves highest AUC is shown. SW = sidewall; BF = bifurcation.}\label{fig:09-results-cbms}
\end{figure}

With respect to PD importance, Figure~\ref{fig:09-pd-global-aneur} illustrates the high attribution of the angle parameter \(\gamma\) towards rupture status classification, as this feature is ranked first and third for the best models of \emph{ALL} and \emph{BF}.
On the SVM model trained on the \emph{SW} subset, ellipticity index (EI) is most important.



\begin{figure}

{\centering \includegraphics[width=0.67\linewidth]{figures/09-pd-global-aneur} 

}

\caption{\textbf{Relative PD importance (AneurD).} PD importance for the best model of each data subset. Values are relative to the maximum PD importance. SW = sidewall; BF = bifurcation.}\label{fig:09-pd-global-aneur}
\end{figure}

Figure~\ref{fig:09-pd-local-aneur} shows PDP and ICE curves for the most important predictors according to \(I_f\) for the best models on each data subset.
All ICE curves of the GBT model and the SVM model (Figure \ref{fig:09-pd-local-aneur}~(a) and~(b)) exhibit nearly identical trend but different intercept.
In contrast, the ICE curves of WKNN (Figure\ref{fig:09-pd-local-aneur}~(c)) appear more jittery.
Apparently, the plots summarize major characteristics of the different model families.
More specifically, the GBT model (Figure \ref{fig:09-pd-local-aneur}~(a)) produces jagged curves with distinct vertical cuts, representing the splits in the base decision trees of this tree ensemble.
For example, the plot for \(\gamma\) shows 4 of such splits at \{16.54, 49.26, 54.42, 64.35\}.
The SVM classifier (Figure \ref{fig:09-pd-local-aneur}~(b)) is a linear model, hence the ICE and PD curves are just lines with a fixed slope.
The marginal model posterior of aneurysm rupture increases with higher values of ellipticity index, max. width of the aneurysm body, aspect ratio \(H_{ortho}/N_{avg}\) and max. aneurysm diameter, whereas for the area of the ostium (variant 2), lower values are more indicative of a high rupture likelihood.
For WKNN, the PDP is better able to clearly show the marginal posteriors of individual predictors than the ICE curves.
This could be due to the property of WKNN being a ``lazy'' learner which does not produce an actual model but makes its predictions based on observation-individual similarity.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-pd-local-aneur} 

}

\caption{\textbf{Relative PD importance (AneurD).} PD importance for the best model of each data subset. Values are relative to the maximum PD importance. SW = sidewall; BF = bifurcation.}\label{fig:09-pd-local-aneur}
\end{figure}

\hypertarget{iml-discussion}{%
\section{Discussion}\label{iml-discussion}}

In this section, we discuss our findings with respect to all three classification tasks, i.e., regarding
CHA-Depression (Section~\ref{iml-discussion-depression}), CHA-Tinnitus (Section~\ref{iml-discussion-tinnitus}) and
AneurD (Section~\ref{iml-discussion-aneur}).

\hypertarget{iml-discussion-depression}{%
\subsection{CHA-Depression}\label{iml-discussion-depression}}

Machine learning has been used to build predictive models of depression severity based on structured patient interviews~\autocite{VanLoo:depression_pred2014,Kessler:MLDepression2016}.
We refrain from quantitative comparison with these studies due to differences in population characteristics and measurements.
But how good is the best of our models actually?
A reasonable baseline is a classifier that simply predicts depression status at time T0, which yields 79\% accuracy.
Our models outperform this baseline, although it is likely that they provide a good fit only for our sample, with patient subgroups from other centers yet to be studied.
However, our models are a promising first step in supporting timely prediction of depression severity and selection of appropriate treatment with only a few questionnaire items.

Consistent with previous studies~\autocite{Langguth:TinAndDepression2011}, we found a strong association between tinnitus distress and depression severity.
Furthermore, predictors measuring perceived stress and demands were found to be a significant contributor to depression in tinnitus patients~\autocite{Trevis:TinnitusReview2018}.
The fact that predictors were selected from different questionnaires confirms the multifactoriality of depression, whose assessment requires the inclusion of different measurements.
Therefore, concomitant emotional symptoms and other comorbidities must be taken into account to meet patient-specific needs.
In a previous study~\autocite{Whooley:2q_depression1997}, high sensitivity in detecting depression was achieved by using only a two-item questionnaire.
One of the two items was \emph{``During the past month, have you often been bothered by feeling down, depressed, or hopeless.''};\autocite{Whooley:2q_depression1997} which is similar to the ADSL\_adsl06 (\emph{``In the past week, I have felt depressed.''}), which has the second largest absolute coefficient in our best LASSO model.

Generally, care must be taken when interpreting the model coefficients:
for example, we identified a strong relationship between non-German citizenship and depression severity (cf.~Figure~\ref{fig:09-lasso-depression} and Table~\ref{tab:09-lasso-depression-tab}).
Although some studies have reported ethnic differences in depression~\autocite{Riolo:DepressionEthnicity2005,Weinberger:DepressionEthnicity2018}, the occurrence of this item tends to suggest higher perceived social stress in patients of predominantly Turkish origin, due to higher unemployment rates, larger families, poorer housing conditions, etc. in this demographic group.
Because only 5.0\% of the cohort population were non-German citizens, these results could also be an effect of overfitting.
Since the associated predictor in the first iteration of feature elimination has a model reliance score of less than 1.0, it is consequently omitted from the sparser models.

Regarding the stability of the models on smaller feature sets, our results show that simpler models are only slightly inferior to the most predictive model.
More specifically, most classification methods show an improvement in AUC as the number of predictors decreases.
In fact, 5 of 11 classifiers improved even by feature selection, i.e., the AUC in the second or later iteration was superior to the AUC in the first iteration (where all 205 predictors are used).
For example, the two decision tree variants achieved the highest performance on the smallest feature subset in each case.
Regarding the LASSO classifier, which showed the best performance, it is encouraging that only 6 predictors from 4 questionnaires showed similar performance (AUC = 0.850) compared to the best overall model (AUC = 0.867).
It is noteworthy that neither predictors of tinnitus localization and quality nor sociodemographic predictors were included in this model.
This finding could be used to reduce the number of questions or entire questionnaires that patients must answer before and after treatment.
psychological or physical stress for a subject undergoing an examination (e.g., a painful biopsy vs.~a blood test).
For example, Yu et al.~\autocite{yu2020controlling} perform feature selection under a budget, where the cost of feature acquisition is derived from suggestions by medical experts based on total financial burden, patient privacy, and patient inconvenience.
Kachuee et al.~\autocite{kachuee2019costsensitive} derive feature costs based on the convenience of answering questions, performing medical exams, and blood and urine tests.

In terms of clinical relevance, our results should be a first step to guide clinicians in making treatment decisions regarding clinical depression in patients with chronic tinnitus.
The models could be used to design an appropriate treatment pathway.
However, before using the models in practice, one must be aware that they are trained on cross-sectional data, i.e., the models separate subclinical and clinical depression based on questionnaire responses and sociodemographic data before treatment.
Also, one must keep in mind that the treatment was a 7-day treatment and the response is depression status \emph{after treatment}.

There are also some limitations to our approach.
First, our models might be subject to selection bias because patients who did not complete all seven questionnaires both at admission \emph{and} after treatment were excluded from our analyses.
However, we do not consider these data as ``missing values'' because this could lead to the problematic suggestion of using imputation methods.
We cannot use imputation because (i) a proportion of patients did not complete the entire questionnaire (rather than individual items) and (ii) we do not know whether the data are missing at random.
However, because the number of patients is large, we believe our results are sufficiently robust.
In future work, we will investigate possible systematic differences between included and excluded patients.
The exclusion of patients who dropped out of completing the questionnaires prematurely, partly because of a gradual loss of motivation, technical unfamiliarity with the computer, or possible interruptions by staff to complete other baseline assessments, could lead to selection bias.
Because the patient population was from only one hospital, future work involves external validation of the models on data from different populations and hospitals.
Because the use of cross-sectional data limits the interpretation of the prediction of depression severity beyond the end of therapy, future work will need to validate the models with longitudinal data.

Another potential limitation is the greedy process of our iterative feature selection wrapper, which can miss global optima as a result.
At each iteration, predictors that prevent the model from classifying correctly are removed from the feature set.
Once a predictor is eliminated, it cannot be included in any subsequent iteration.
However, it is possible that including a predictor that was removed in an early iteration could lead to a better model in a later iteration.
A possible solution would be a mechanism to backtrack or revisit earlier iterations if it turns out that some of the removed predictors actually contributed positively to model performance.
Alternatively, the \(MR\) cutoff value for discarding features (which was set to 1 in our experiments) could be subjected to hyperparameter tuning.
Future work therefore includes comparison with other feature selection algorithms.

\hypertarget{iml-discussion-tinnitus}{%
\subsection{CHA-Tinnitus}\label{iml-discussion-tinnitus}}

We trained classification models to predict tinnitus-related distress after multimodal treatment (T1) in patients with chronic tinnitus based on self-report questionnaires data acquired before treatment (T0).
The gradient boosted trees model which uses 26 (12.7\%) from a total of 205 predictors separates patients with ``compensated'' vs.~``decompensated'' tinnitus with best AUC.

Among these features are measurements that describe a variety of psychological and psychosomatic patient characteristics as well as socio-demographics and therefore confirming the multi-factorial nature of tinnitus-related distress.
These characteristics were used for the phenotyping in Chapter~\ref{phenotypes}.
Additionally, the predictors can be investigated in a followup studies of how such characteristics influence treatment success.
As expected, predictors that are directly linked to tinnitus quality show high model attribution, such as the degree of perceived tinnitus impairment and loudness.
At the same time, depression, attitudinal factors (self-efficacy, pessimism, complaint tendency), sleep problems, educational level, tinnitus location and duration emerged as highly important for the model prediction as well.

Quantitative predictors, such as tinnitus impairment and loudness, show non-monotonic relationships with respect to the predicted outcome.
Notably, very low self-reported impairment or loudness measured by visual analogue scales do not generally indicate low tinnitus-related distress measured by the TQ.
One explanation is that simple measurements like TINSKAL\_impairment and TINSKAL\_loudness are less robust and show higher variability than a compound scale that combines multiple single questionnaire items.
These findings could be investigated further, e.g., whether there is a relationship towards a subgroup of patients that were more fatigued and thus not less thoroughly filling a large number of questionnaires.

Our results confirm the intricate interplay between depression and tinnitus-related distress that was elucidated by numerous previous studies~\autocite{Dobie:DepressionTinnitus2003,Folmer:Tinnitus1999,Halford:AnxietyDepressionTinnitus1991,Langguth:TinnitusDepression2011,Salazar:Depression2019}.
For our best model, an ADSL score of more than 20 is associated with an increased predicted risk of tinnitus decompensation (cf.~Figure \ref{fig:09-tq-xgboost-shap-per-feature}~(2)) which is close to the cutoff of clinical relevance of depression~\autocite{Hautzinger:ADSL2003}.

In the context of parsimonious learning, a general strategy is to determine the set of predictors which is as small as possible and where the inclusion of any other predictor does not yield in a considerable improvement in performance.
So how many predictors are really necessary for an accurate tinnitus distress prediction?
Figure \ref{fig:09-tq-xgboost-parsimonious}~(a) illustrates the change in performance for a GBT classifier when predictors are iteratively added to the feature space in the order of their SHAP values with respect to our best model.
A model that uses only the TINSKAL\_impairment achieves AUC = 0.79 \(\pm\) 0.06.
Adding ADSL\_depression leads to an improvement in AUC of 0.06.
However, none of the remaining 24 predictors results in an improvement of more than 0.01, respectively.
Moreover, only 3 predictors are necessary for a model with AUC = 0.85, 8 predictors for a model with AUC = 0.87 and 15 predictors for a model with AUC = 0.89 (cf.~Figure \ref{fig:09-tq-xgboost-parsimonious}~(a)).



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-tq-xgboost-parsimonious} 

}

\caption{\textbf{Cumulative feature contribution \& correlation network.} (a) Cross-validation AUC (average \(\pm\) standard deviation) of a GBT model trained on the feature subset comprising the predictors denoted on the y-axis up to that iteration. The ordering of features is according to mean absolute SHAP value (cf.~Figure \ref{fig:09-tq-xgboost}\textasciitilde(a)). (b) Network illustrating 3 groups of features among the 26 selected predictors of the best model with high intra-group correlation (\(|\rho| \geq\) 0.5). 8 predictors (predominantly from SOZK) without any moderate to high pairwise correlation are not shown.}\label{fig:09-tq-xgboost-parsimonious}
\end{figure}

One potential explanation could be multicollinearity among groups of predictors.
Figure \ref{fig:09-tq-xgboost-parsimonious}~(b) shows a network of 3 predictor groups among the 26 features of the best model.
For example, the features TINSKAL\_impairment and TINSKAL\_loudness are moderately correlated (Spearman correlation \(\rho\) = 0.69), which raises the question of whether one of the two predictors could be removed without a considerable loss in AUC.
The largest subgroup spanning 14 features involves descriptors of depression, perceived stress and reported physical health.
In future work, an investigation of possible interaction effects among these moderately to strongly correlated features could be investigated, to better understand why all of them were selected and to determine whether some of them could be removed to achieve a better trade-off between model accuracy and complexity.

Our workflow leverages the potential of machine learning for identifying key predictors from a variety of features collected before treatment for post-treatment tinnitus compensation, by ensuring that every potential predictor is included in the analysis, and by the internal validation of the classification models using cross-validation and hyperparameter tuning.
Furthermore, by selecting a variety of classification algorithm families, both linear and nonlinear relationships between a feature and outcome could be identified.
A limitation of this hypothesis-free approach is that the learned models could contain features that quantify the same or similar patient characteristics.
For example, the best model in this study included the two highly correlated features ADSL\_depression and BSF\_anx\_depression (anxious depressiveness score).
While the inclusion of both features contributed to model performance, from a medical perspective, a predictive model with only certain features might be more beneficial. Preselecting features to avoid multicollinearity could be a direction for future work.

Finally, the exclusion of 2,701 out of 4,117 patients (65.6\%) who did not complete \emph{all} 10 questionnaires could have resulted in selection bias.
Many patients spent more than one hour completing the questionnaire on a dedicated minicomputer and were therefore more likely to drop out of the completion process.
Completers were slightly younger than non-completers (mean age 49.8 \(\pm\) 12.2 vs.~51.7 \(\pm\) 13.8), were more likely to have the highest German school degree ``Abitur'' (48.2\% vs.~42.0\%) and had been suffering from tinnitus longer (\(>\) 5 years: 33.3\% vs.~25.1\%).
In future work we intend to investigate to what extent insights from completers can be used on subsamples of non-completers.
Therefore, we can use the DIVA framework of Hielscher et al.~\autocite{Hielscher:DIVA2018}.
However, psychological treatment approaches are likely to benefit only those who report psychological problems prior to tinnitus perception or in association with tinnitus perception.

\hypertarget{iml-discussion-aneur}{%
\subsection{AneurD}\label{iml-discussion-aneur}}

Our classification results are promising, as morphological parameters alone can provide models with moderate power.
Because previous studies have found that hemodynamic parameters are also predictive~\autocite{CebralEtAl:Neuroradiology2011,BergBeuing:CARS2018}, future work includes exploring the potential of combining morphologic and hemodynamic features for classification of rupture status.
In addition, because our focus for now has been on quantifying the merit of morphologic parameters, we have ignored demographic characteristics, such as age and sex, which also correlate strongly with aneurysm rupture~\autocite{detmer2018development}.
We expect that adding these patient characteristics will further improve classification performance.

PDP analysis showed that for the best model (gradient boosted trees), the parameters angle at the dome point \(\gamma\), ellipticity index \(EI\), maximum aneurysm width \(W_{max}\), nonsphericity index \(NSI\), and aneurysm area \(A_{O2}\) had the highest attribution (see Figure \ref{fig:09-pd-global-aneur}~(a)).
These differed from those found for two subsets of sidewall and bifurcation aneurysms, respectively.
Figure~\ref{fig:09-pd-local-aneur} shows that none of the features appear among the top 5 predictors for sidewall aneurysms, bifurcation aneurysms, and the overall data set.
This is also partly due to the fact that the family of the best model is different for each of the subsets.
Consequently, Figure~\ref{fig:09-pd-local-aneur-discussion} shows that the PDP curves for the top 5 features on ALL differ substantially.
Therefore, we argue that PDPs are more appropriate for \emph{intra}-model comparisons of feature attributions.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/09-pd-local-aneur-discussion} 

}

\caption{\textbf{PDP curves for top-5 predictors on ALL - GBT for the best models on each data subset.}}\label{fig:09-pd-local-aneur-discussion}
\end{figure}

We observed that classification performance is consistently higher for the subset of sidewall aneurysms vs.~bifurcation aneurysms, and that different parameters were found to have high model attribution.
This could be partially due to the rather small sample size or the already mentioned differences in model families.
However, Baharoglu et al.~\autocite{BaharogluEtAl:Neurosurgery2012} identified significant differences between sidewall and bifurcation aneurysms with respect to morphological parameters, and how these parameters can predict rupture status.

Some of our findings also suggest some form of higher-level interactions between groups of features.
For example, the ellipticity index (\(EI\))was found second most important for ALL - GBT and most important for SW - SVM, although differences in \(EI\) between unruptured and ruptured aneurysms are not significant (p = 0.323, Wilcoxon rank sum test, \(\alpha\) = 0.01).

There are some limitations to our analysis.
The small sample size, especially for the subset of sidewall aneurysms (N=24), could lead to overfitting.
In future work, we would like to retrain our models on a larger number of datasets, and incorporate a wider variety of predictors, such as hemodynamic and demographic features, as mentioned above.
A further limitation concerns the validity of the class label.
Samples that were labeled as unruptured could have ruptured at a later moment.
Further, we would like to investigate samples with high classification error in more detail.
Here, our goal is to derive descriptions of aneurysms subgroups which are hard to classify, in order to better understand reasons for misclassification, and to signalize to the medical expert that a manual diagnosis is necessary.

\hypertarget{iml-conclusion}{%
\section{Conclusion}\label{iml-conclusion}}

In medical applications, black-box models are becoming increasingly popular due to their high predictive power.
However, due to their opacity, a post-modeling step is required to extract actionable insights from them.\\
We presented a machine learning workflow for classification and post-hoc interpretation alongside dataset-specific steps for three medical applications.
While the variety of classification algorithms to be created is identical for all datasets, we chose the interpretation method based on the opacity of the model family that achieves maximum performance, and on dimensionality.
For CHA-tinnitus, gradient-boosted trees performed best, and we used Shapely value explanations to obtain feature importance values at model, subpopulation, and observation levels.
For CHA depression, LASSO was found to achieve the best generalization performance, so we used the intrinsically interpretable model coefficients.
For AneurD, we used PD importance instead of SHAP values, although gradient-boosted trees provide the best model because the number of observations is too small to produce reliable importance scores at the subpopulation or observation level.

\textbf{TODO: some discussion on the robustness}

\hypertarget{gender}{%
\chapter{Subpopulation-Specific Learning and Post-Hoc Model Interpretation}\label{gender}}

\begin{infobox}{tasks.pdf}

\hypertarget{brief-chapter-summary-6}{%
\subsubsection*{Brief Chapter Summary}\label{brief-chapter-summary-6}}
\addcontentsline{toc}{subsubsection}{Brief Chapter Summary}

We present a workflow to examine how subpopulations differ with respect to their most predictive characteristics in temporal data. To do so, we derive a post-hoc interpretation measure to assess the difference in association of predictors between two subpopulations. We report results for CHA on gender differences (subpopulations of female and male patients) for the two outcomes of tinnitus-related distress and depression, and the effect of treatment for both outcomes.

\end{infobox}

\begin{infobox}

This chapter is partly based on:

Uli Niemann, Benjamin Boecking, Petra Brueggemann, Birgit Mazurek, and Myra Spiliopoulou. ``Gender-Specific Differences in Patients With Chronic Tinnitus -- Baseline Characteristics and Treatment Effects''. In: \emph{Frontiers in Neuroscience} 14 (2020), p.~487. DOI: \href{https://doi.org/10.3389\%2Ffnins.2020.00487}{10.3389/fnins.2020.00487}.

\end{infobox}

\hypertarget{gender-intro}{%
\section{Motivation and Comparison to Related Work}\label{gender-intro}}

\textbf{to be written}

\hypertarget{gender-measure}{%
\section{Comparing Differences in Feature Importance between Two Subpopulations}\label{gender-measure}}

\textbf{to be written}

\hypertarget{gender-workflow}{%
\section{Workflow}\label{gender-workflow}}

\hypertarget{learning-tasks}{%
\subsection{Learning Tasks}\label{learning-tasks}}

\textbf{to be written}

\hypertarget{model-evaluation-and-hyperparameter-tuning}{%
\subsection{Model Evaluation and Hyperparameter Tuning}\label{model-evaluation-and-hyperparameter-tuning}}

\hypertarget{gender-results}{%
\section{Results}\label{gender-results}}

\textbf{to be written}

\hypertarget{gender-conclusions}{%
\section{Conclusions on Subpopulation-Specific Differences in Feature Importance}\label{gender-conclusions}}

\textbf{to be written}

\hypertarget{part-summary}{%
\part{SUMMARY}\label{part-summary}}

\hypertarget{summary}{%
\chapter{Conclusion and Future Work}\label{summary}}

\textbf{to be written}

\hypertarget{summary-results}{%
\section{Research Results for Medical Expert-Guided Knowledge Discovery}\label{summary-results}}

\textbf{to be written}

\hypertarget{summary-future-work}{%
\section{Future Work}\label{summary-future-work}}

\textbf{to be written}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}

\hypertarget{abbreviations}{%
\chapter*{Abbreviations}\label{abbreviations}}
\addcontentsline{toc}{chapter}{Abbreviations}

\begin{longtable}[]{@{}ll@{}}
\toprule
~ & ~\tabularnewline
\midrule
\endhead
AI & Artificial Intelligence\tabularnewline
DM & Data Mining\tabularnewline
ML & Machine Learning\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{appx-pheno}{%
\chapter{Variables selected for phenotyping}\label{appx-pheno}}

\begin{itemize}
\tightlist
\item
  ACSA\_qualityoflife*: Quality of life during the last 2 weeks
\item
  ADSL\_depression: Depressive disorder sum score
\item
  BI\_abdominalsymptoms: Abdominal symptoms score
\item
  BI\_fatigue: Fatigue score
\item
  BI\_heartsymptoms: Heart symptoms score
\item
  BI\_limbpain: Limb pain score
\item
  BI\_overallcomplaints: Overall complaints sum score
\item
  BSF\_anger: Anger score
\item
  BSF\_anxdepression: Anxious depression score
\item
  BSF\_apathy: Apathy score
\item
  BSF\_elevatedmood*: Elevated mood score
\item
  BSF\_fatigue: Fatigue score
\item
  BSF\_mindset*: Positive mindset score
\item
  ISR\_additionalitems: Additional items score
\item
  ISR\_anxiety: Anxiety score
\item
  ISR\_compulsivesyn: Obsessive-compulsive syndrome score
\item
  ISR\_depression: Depression score
\item
  ISR\_eatingdisorder: Eating disorder score
\item
  ISR\_somatosyn: Somatoform syndrome score
\item
  ISR\_totalpsychiatricsyn: Total psychiatric syndrome score
\item
  PHQK\_depression: Presence of depression
\item
  PHQK\_panicsyn: Presence of panic syndrome
\item
  PSQ\_demand: Demand score
\item
  PSQ\_joy*: Joy score
\item
  PSQ\_stress: Total perceived stress sum score
\item
  PSQ\_tension: Tension score
\item
  PSQ\_worries: Worries score
\item
  SES\_affectivepain: Affective pain
\item
  SES\_sensoricpain: Sensoric pain
\item
  SF8\_bodilyhealth*: Bodily health score
\item
  SF8\_mentalcomp*: Mental component summary score
\item
  SF8\_mentalhealth*: Mental health score
\item
  SF8\_overallhealth*: Overall health score
\item
  SF8\_physicalcomp*: Physical component summary score
\item
  SF8\_physicalfunct*: Physical functioning score
\item
  SF8\_roleemotional*: Role emotional score
\item
  SF8\_rolephysical*: Role physical score
\item
  SF8\_socialfunct*: Social functioning score
\item
  SF8\_vitality*: Vitality score
\item
  SSKAL\_painfrequency: Visual analog scale pain frequency
\item
  SSKAL\_painimpairment: Visual analog scale pain impairment
\item
  SSKAL\_painseverity: Visual analog scale pain severity
\item
  SWOP\_optimism*: Optimism score
\item
  SWOP\_pessimism: Pessimism score
\item
  SWOP\_selfefficacy*: Self-efficacy score
\item
  TINSKAL\_frequency: Tinnitus frequency
\item
  TINSKAL\_impairment: Tinnitus impairment
\item
  TINSKAL\_loudness: Tinnitus loudness
\item
  TLQ\_01\_bothears: Tinnitus location: both ears
\item
  TLQ\_01\_entirehead: Tinnitus location: entire head
\item
  TLQ\_01\_leftear: Tinnitus location: left ear
\item
  TLQ\_01\_rightear: Tinnitus location: right ear
\item
  TLQ\_02\_hissing: Tinnitus noise: hissing
\item
  TLQ\_02\_ringing: Tinnitus noise: ringing
\item
  TLQ\_02\_rustling: Tinnitus noise: rustling
\item
  TLQ\_02\_whistling: Tinnitus noise: whistling
\item
  TQ\_auditoryperceptdiff: Auditory perceptual difficulties score
\item
  TQ\_cognitivedistress: Cognitive distress score
\item
  TQ\_distress: Total tinnitus distress score
\item
  TQ\_emodistress: Emotional distress score
\item
  TQ\_intrusiveness: Intrusiveness score
\item
  TQ\_psychodistress: Psychological distress score
\item
  TQ\_sleepdisturbances: Sleep disturbances score
\item
  TQ\_somacomplaints: Somatic complaints score
\end{itemize}

\printbibliography

\end{document}
